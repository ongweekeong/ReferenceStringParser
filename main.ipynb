{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install torch\n",
    "!pip install fasttext\n",
    "!pip install sklearn-hierarchical-classification\n",
    "!pip install spacy\n",
    "!pip install prettytable\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ongwe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Preprocess Data\n",
    "'''\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import fasttext\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk import word_tokenize\n",
    "\n",
    "OTHER_TAG = \"other\"\n",
    "PUNCT_TAG = \"punct\"\n",
    "\n",
    "with open('./utils/tags.txt', encoding=\"utf-8\", errors='ignore') as f:\n",
    "    tags = set([str.rstrip(tag) for tag in f.readlines()])\n",
    "\n",
    "with open('./utils/tags_hierarchy.txt', encoding=\"utf-8\", errors='ignore') as f:\n",
    "    hierarchy_tags = set([str.rstrip(tag) for tag in f.readlines()])\n",
    "\n",
    "def remove_labels(text):\n",
    "    return re.sub(r'\\<\\/?[\\w-]*\\>\\s*', \"\", text).strip()\n",
    "\n",
    "def tag_token(token, tag):\n",
    "    if token in string.punctuation:\n",
    "        return (token, PUNCT_TAG)\n",
    "    return (token, tag)\n",
    "\n",
    "def get_tagged_tokens(ref, groups):\n",
    "    tagged_tokens = []\n",
    "    relevant_groups = list(filter(lambda group: group[1] in tags, groups))\n",
    "    tag_dict = dict()\n",
    "    for group in relevant_groups:\n",
    "        text = remove_labels(group[0])\n",
    "        tokens = word_tokenize(text)\n",
    "        for token in tokens:\n",
    "            if token not in tag_dict:\n",
    "                tag_dict[token] = [group[1]]\n",
    "            else:\n",
    "                tag_dict[token].append(group[1])\n",
    "    tokenized_ref = word_tokenize(remove_labels(ref))\n",
    "    tagged_tokens = []\n",
    "    for token in tokenized_ref:\n",
    "        if token in tag_dict and tag_dict[token]: # still has a tag\n",
    "            tag = tag_dict[token][0]\n",
    "            tag_dict[token].pop(0)\n",
    "        else:\n",
    "            tag = OTHER_TAG\n",
    "        tagged_tokens.append(tag_token(token, tag))\n",
    "    return tagged_tokens\n",
    "\n",
    "def find_groups(text):\n",
    "    groups = re.findall(r'(\\<(.*)\\>.*\\<\\/\\2\\>)', text) # group: (<tag> ... <tag>, tag)\n",
    "    if not groups:\n",
    "        return []\n",
    "    combined = []\n",
    "    for group in groups:\n",
    "        new_group = re.sub(r'\\<\\/?'+ group[1] + '\\>\\s*', \"\", group[0]).strip()\n",
    "        combined.append(group)\n",
    "        combined.extend(find_groups(new_group))\n",
    "    return combined\n",
    "\n",
    "''' Attach tags to each token '''\n",
    "def attach_tags(dataset_path):\n",
    "    dataset = []\n",
    "    with open(dataset_path, encoding=\"utf-8\", errors='ignore') as f:\n",
    "        refs = f.readlines()\n",
    "        for ref in refs:\n",
    "            groups = find_groups(ref)\n",
    "            # groups = re.findall(r'(\\<(.*)\\>.*\\<\\/\\2\\>)', ref) # format (<tag>...</tag>, tag)\n",
    "            tagged_tokens = get_tagged_tokens(ref, groups)\n",
    "            dataset.append(tagged_tokens)\n",
    "    return dataset\n",
    "\n",
    "''' Removes labels and tokenizes '''\n",
    "def tokenize_dataset(dataset_path, sep=\" \"):\n",
    "    dataset = []\n",
    "    with open(dataset_path, encoding=\"utf-8\", errors='ignore') as f:\n",
    "        refs = f.readlines()\n",
    "        for ref in refs:\n",
    "            ref = remove_labels(ref) \n",
    "            tokenized = sep.join(word_tokenize(ref))\n",
    "            dataset.append(tokenized)\n",
    "    return dataset\n",
    "\n",
    "def map_to_index(keys, idx_start=0):\n",
    "    key_to_idx, keys_arr, idx = {}, [], idx_start\n",
    "    for key in keys:\n",
    "        key_to_idx[key] = idx\n",
    "        keys_arr.append(key)\n",
    "        idx += 1\n",
    "    return key_to_idx, keys_arr\n",
    "\n",
    "all_tags = tags \n",
    "all_tags.add(OTHER_TAG)\n",
    "all_tags.add(PUNCT_TAG)\n",
    "tag_to_idx, tag_arr = map_to_index(all_tags)\n",
    "\n",
    "dataset_path = './dataset/standardized_dataset.txt'\n",
    "dataset = attach_tags(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "def train_word_embedding_model(dataset_paths, embedding_dim, use_subwords=False, use_hierarchy=False):\n",
    "    embedding_dataset_path = './dataset/word_embedding_dataset.txt'\n",
    "    hierarchy_dataset_path = './dataset/umass-citation/training'\n",
    "\n",
    "    word_embedding_dataset = []\n",
    "    for dataset_path in dataset_paths:\n",
    "      word_embedding_dataset.extend(tokenize_dataset(dataset_path, sep=\" \"))\n",
    "    with open(embedding_dataset_path, 'w', errors='ignore') as f:\n",
    "        # fasttext tokenizes by whitespaces\n",
    "        f.write(\"\\n\".join(word_embedding_dataset))\n",
    "    if use_subwords:\n",
    "      model_path = './models/subword_embedding.bin'\n",
    "      model = fasttext.train_unsupervised(embedding_dataset_path, dim = embedding_dim, minn = 3, maxn = 6, wordNgrams=6) \n",
    "    elif use_hierarchy:\n",
    "        model_path = './models/hierarchy_word_embedding.bin'\n",
    "        model = fasttext.train_unsupervised(embedding_dataset_path, dim=embedding_dim, maxn=0)\n",
    "    else:\n",
    "      model_path = './models/word_embedding.bin'\n",
    "      model = fasttext.train_unsupervised(embedding_dataset_path, dim=embedding_dim, maxn=0)\n",
    "    model.save_model(model_path)\n",
    "    return model\n",
    "\n",
    "we_dataset_dir_path = \"./dataset/cstyle_dataset\"\n",
    "we_dataset_paths = [join(we_dataset_dir_path, f) for f in listdir(we_dataset_dir_path) if isfile(join(we_dataset_dir_path, f))]\n",
    "\n",
    "''' 1. Word Embeddings: Without Pretrained Word Embeddings '''\n",
    "# WE_model = train_word_embedding_model(we_dataset_paths, embedding_dim = EMBEDDING_DIM)\n",
    "\n",
    "''' 2. Subword Embeddings: Without Pretrained Subword Embeddings '''\n",
    "SWE_model = train_word_embedding_model(we_dataset_paths, embedding_dim = EMBEDDING_DIM, use_subwords = True)\n",
    "\n",
    "curr_WE_model = SWE_model # Change accordingly\n",
    "\n",
    "def get_word_vector(token):\n",
    "    return curr_WE_model.get_word_vector(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Named Entity Recognition'''\n",
    "import spacy\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "ner_dict = {\n",
    "        'ORG': 0,\n",
    "        \"NORP\": 1,\n",
    "        \"GPE\": 2,\n",
    "        \"PERSON\": 3,\n",
    "        \"LANGUAGE\": 4,\n",
    "        \"DATE\": 5,\n",
    "        \"TIME\": 6,\n",
    "        \"PRODUCT\": 7,\n",
    "        \"EVENT\": 8,\n",
    "        \"ORDINAL\": 9\n",
    "}\n",
    "\n",
    "# text should be tokenized and joined together by whitespaces\n",
    "def generate_ner_features(text):\n",
    "    doc = spacy_nlp(text)\n",
    "    entities = doc.ents\n",
    "    default_feature = [0 for _ in range(len(ner_dict.keys()) + 1)]\n",
    "    default_feature[-1] = 1 \n",
    "    entity_to_label = defaultdict(lambda: default_feature)\n",
    "    for entity in entities:\n",
    "        entity_tokens = entity.text.split(\" \")\n",
    "        label = entity.label_\n",
    "        if label in ner_dict:\n",
    "            features = [0 for _ in range(len(ner_dict.keys()) + 1)]\n",
    "            features[ner_dict[label]] = 1\n",
    "            for token in entity_tokens:\n",
    "                entity_to_label[token] = features\n",
    "    return entity_to_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "model_version = 'allenai/scibert_scivocab_cased'\n",
    "do_lower_case = False\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
    "\n",
    "def get_tokens_and_segments_tensors(text):\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segment_ids = [1] * len(indexed_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = torch.tensor([segment_ids])\n",
    "    return tokens_tensor, segments_tensor\n",
    "\n",
    "def average(embeddings):\n",
    "    if len(embeddings.size()) == 1: # if only one embedding, just return\n",
    "        return embeddings\n",
    "    averaged_embedding = np.array([0 for _ in range(len(embeddings[0]))])\n",
    "    for embedding in embeddings:\n",
    "        averaged_embedding = np.add(averaged_embedding, embedding)\n",
    "    return np.true_divide(averaged_embedding, len(embeddings))\n",
    "\n",
    "from transformers import BertForPreTraining, BertConfig \n",
    "\n",
    "config = BertConfig.from_json_file('./models/bert/fine_tuned_bert/config.json')\n",
    "bert_model = BertModel(config)\n",
    "bert_model.eval()\n",
    "\n",
    "def get_scibert_vector(text):\n",
    "    tokens_tensor, segments_tensor =get_tokens_and_segments_tensors(text)\n",
    "    outputs = bert_model(tokens_tensor, segments_tensor)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    token_embeddings = torch.squeeze(hidden_states[-1], dim=0).detach()\n",
    "    embeddings = dict()\n",
    "    tokens = text.split(\" \")\n",
    "    curr = 0\n",
    "    for token in tokens:\n",
    "        end = curr + len(tokenizer.tokenize(token))\n",
    "        start, end = curr, end\n",
    "        embeddings[token] = average(token_embeddings[start: end]).tolist()\n",
    "        curr = end\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(tokenize_dataset(dataset_path), columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "\n",
    "def token_transformer(sentence, add_noise=False):\n",
    "    sentence = re.sub(r\"\\b[A-Z]\\b\", 'L', sentence) #uppercase letters\n",
    "    sentence = re.sub(r\"\\b[a-z]\\b\", 'l', sentence) #lowercase letters\n",
    "    if add_noise:\n",
    "        sentence = re.sub(r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\b', random.choices(['M',''], weights=[3,1])[0], sentence)\n",
    "        sentence = re.sub(r'\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\b', random.choices(['m',''], weights=[3,1])[0], sentence)\n",
    "    else:\n",
    "        sentence = re.sub(r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\b', 'M', sentence) #random.choices(['M',''], weights=[3,1])[0], sentence)\n",
    "        sentence = re.sub(r'\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\b', 'm', sentence) #random.choices(['m',''], weights=[3,1])[0], sentence)\n",
    "    sentence = re.sub(r\"\\b[a-z][a-z]+\\b\", 'w', sentence) #lowercase words\n",
    "    sentence = re.sub(r\"\\b[A-Z][a-z]+\\b\", 'W', sentence) #uppercase words\n",
    "    sentence = re.sub(r'\\b(1\\d{3}|20[012]\\d)\\b', 'y', sentence)\n",
    "    sentence = re.sub(r\"\\b[0-9]+\\b\", 'n', sentence) #numbers\n",
    "    return sentence\n",
    "\n",
    "def token_punctuation(sentence, add_noise=False):\n",
    "    if add_noise:\n",
    "        words = sentence.split()\n",
    "        newSentence = []\n",
    "        for word in words: # for every word with punctuation, there is a 10% chance of omitting one type of punctuation.\n",
    "            newSentence.append(word.translate({ord(i):\" {} \".format(random.choices([i, ''], weights=[9,1])[0]) for i in ',.()[]:;\\'\\\"-'}))\n",
    "        sentence = \" \".join(newSentence)\n",
    "        return sentence\n",
    "    \n",
    "    else:\n",
    "        return sentence.translate({ord(i):\" {} \".format(i) for i in ',.()[]:;\\'\\\"-'})\n",
    "    \n",
    "\n",
    "''' Preprocess text column -- separate symbols with spaces to preserve punctuation as tokens'''\n",
    "def preprocess(df, add_noise=False): # text is a dataframe column\n",
    "    df.text = df.text.apply(token_punctuation, add_noise)\n",
    "    df.text = df.text.apply(token_transformer, add_noise)\n",
    "    return df\n",
    "df = preprocess(df, False) # boolean flag to add noise. For training purposes to be robust against noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, plot_confusion_matrix, confusion_matrix as cm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', TfidfVectorizer(lowercase=False, token_pattern=r\"\\S+\")), #r\"(?u)(\\b\\w+\\b|[\\.\\\"(),\\'\\[\\]:;])\")),\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('classifier', LogisticRegression(class_weight='balanced'))\n",
    "])\n",
    "params = {'features__ngram_range': [(1,3)],\n",
    "    'classifier__C': [0.001],\n",
    "         'classifier__max_iter': [500],}\n",
    "gs = GridSearchCV(pipeline, params, refit=True, cv=2, scoring='f1_macro', verbose=10)\n",
    "from joblib import load\n",
    "\n",
    "gs = load('models\\\\cstyle_LR_augmentedfeatures_3grams_noisyinput_noUnknown.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mla', 'mla', 'mla', ..., 'acm-sig-proceedings',\n",
       "       'acm-sig-proceedings', 'harvard3'], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.predict(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "categories = [['acm-sig-proceedings'], ['american-chemical-society'], ['apa'],\n",
    "       ['chicago-author-date'], ['harvard3'], ['ieee'], ['mla']]#, ['unknown']]\n",
    "cat = ['acm-sig-proceedings', 'american-chemical-society', 'apa',\n",
    "       'chicago-author-date', 'harvard3', 'ieee', 'mla']\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# enc.fit(categories)\n",
    "enc.fit(gs.predict(df.text).reshape(-1, 1))\n",
    "\n",
    "def style_to_feature(style, train=False):\n",
    "#     local_df = pd.DataFrame()\n",
    "#     fstyle = list(map(lambda s: [s], style))\n",
    "    if train:\n",
    "        enc.fit(style.reshape(-1,1))\n",
    "    y_transformed = enc.transform(style.reshape(-1,1)).toarray()\n",
    "\n",
    "    return y_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_encoder(styles):\n",
    "    cat2vec = {}\n",
    "    vec = []\n",
    "    for i in range(len(categories)):\n",
    "        cat2vec[categories[i][0]] = [0 for j in range(len(categories))]\n",
    "        cat2vec[categories[i][0]][i] = 1\n",
    "    for style in styles:\n",
    "        vec.append(cat2vec[style])\n",
    "    return np.array(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "''' Get inputs and outputs for model '''\n",
    "ref_train, ref_test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "def get_x_y(refs, train=False):\n",
    "    X, y = [], []\n",
    "    X_style, y_style = [], []\n",
    "    for ref in refs:\n",
    "        X_ref, y_ref = [], []\n",
    "        joined_ref = \" \".join(list(map(lambda x: x[0], ref))) # concatenate tokens using whitespace\n",
    "        X_style.append(joined_ref)\n",
    "        ner_features = generate_ner_features(joined_ref)\n",
    "        scibert_features = get_scibert_vector(joined_ref)\n",
    "#         style = gs.predict([joined_ref])\n",
    "        for token, tag in ref:\n",
    "            features = get_word_vector(token)\n",
    "            features = np.hstack([features, np.array(ner_features[token]), np.array(scibert_features[token])])#, style])\n",
    "            X_ref.append(features)\n",
    "            y_ref.append(tag_to_idx[tag])\n",
    "\n",
    "        X.append(X_ref)\n",
    "        y.append(y_ref)\n",
    "\n",
    "    style = gs.predict(X_style)\n",
    "    feature_style = style_to_feature(style, train)#style_encoder(style)#\n",
    "#     X = style_to_feature(X, train)\n",
    "    y_style = []\n",
    "    idx = 0\n",
    "    for ref in refs:\n",
    "        sentence_style = []\n",
    "        feat = feature_style[idx]\n",
    "        for token, tag in ref:\n",
    "            sentence_style.append(feat)\n",
    "            features = np.concatenate((features, feature_style[idx]), axis=None)\n",
    "        y_style.append(sentence_style)\n",
    "        idx += 1\n",
    "    return X, y, X_style, y_style\n",
    "\n",
    "def add_padding(matrix, padding_value, max_length):\n",
    "    return pad_sequences(matrix, maxlen=max_length, padding='post', truncating='pre', value=padding_value, dtype='float32')\n",
    "\n",
    "X_train, y_train, X_sentence, X_style = get_x_y(ref_train, True)\n",
    "X_test, y_test, X_test_sentence, X_test_style = get_x_y(ref_test)\n",
    "\n",
    "padding_value = float(len(all_tags))\n",
    "max_length = max(map(lambda ref: len(ref), X_train + X_test))\n",
    "\n",
    "X_train = add_padding(X_train, padding_value, max_length)\n",
    "X_test = add_padding(X_test, padding_value, max_length)\n",
    "y_train = add_padding(y_train, padding_value, max_length)\n",
    "y_test = add_padding(y_test, padding_value, max_length)\n",
    "X_style = add_padding(X_style, padding_value, max_length)\n",
    "X_test_style = add_padding(X_test_style, padding_value, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5],\n",
       " [Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, '')])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "testingstyle = gs.predict(X_sentence)\n",
    "testingstyle1 = gs.predict(X_test_sentence)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(testingstyle)\n",
    "plt.xticks(rotation=45)\n",
    "plt.hist(testingstyle1)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = self.input_size,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialise hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "\n",
    "        # Initialise internal state\n",
    "        c0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size)\n",
    "\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "\n",
    "        return output, (hn, cn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = LSTM(input_size, hidden_size, output_size, num_layers)\n",
    "        # self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.fc = nn.Linear(hidden_size + X_style.shape[2], output_size)\n",
    "        # self.fc1 = nn.Linear(output_size, output_size)\n",
    "\n",
    "    def forward(self, x, style):\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        output = torch.cat((output, torch.tensor(style)), 2)\n",
    "        output = self.fc(output)\n",
    "        # output = self.fc1(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hyperparameters\n",
    "'''\n",
    "num_epochs = 800\n",
    "learning_rate = 0.001\n",
    "\n",
    "we_feature_size = EMBEDDING_DIM\n",
    "ner_feature_size = len(ner_dict.keys()) + 1\n",
    "scibert_feature_size = 768\n",
    "\n",
    "input_size = we_feature_size + ner_feature_size + scibert_feature_size #+ len(categories) # Number of features (change accordingly)\n",
    "hidden_size = 25 #+ X_style.shape[2] # Number of features in the hidden state\n",
    "num_layers = 1 # Number of stacked LSTM layers\n",
    "\n",
    "output_size = len(all_tags) # Number of output classes\n",
    "model = Net(input_size, hidden_size, output_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loss Function and Optimiser\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=len(all_tags))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss after minibatch     1: 2.95473\n",
      "Epoch: 0, loss after minibatch     2: 2.84798\n",
      "Epoch: 0, loss after minibatch     3: 2.78803\n",
      "Epoch: 0, loss after minibatch     4: 2.73177\n",
      "Epoch: 0, loss after minibatch     5: 2.67026\n",
      "Epoch: 0, loss after minibatch     6: 2.62725\n",
      "Epoch: 0, loss after minibatch     7: 2.58319\n",
      "Epoch: 0, loss after minibatch     8: 2.53701\n",
      "Epoch: 0, loss after minibatch     9: 2.49358\n",
      "Epoch: 0, loss after minibatch    10: 2.46433\n",
      "+-------------+-----------------------+---------------------+------------------------+\n",
      "|     Tag     |       Precision       |        Recall       |         FBeta          |\n",
      "+-------------+-----------------------+---------------------+------------------------+\n",
      "|  publisher  | 7.616146230007616e-05 |         0.04        | 0.00015203344735841882 |\n",
      "|    punct    |   0.5176767676767676  |  0.7382953181272509 |   0.6086095992083127   |\n",
      "|    editor   |          0.0          |         0.0         |          0.0           |\n",
      "|    pages    |          0.0          |         0.0         |          0.0           |\n",
      "|     note    |          0.0          |         0.0         |          0.0           |\n",
      "|   journal   |  0.09497206703910614  | 0.24285714285714285 |  0.13654618473895583   |\n",
      "|   location  |          0.0          |         0.0         |          0.0           |\n",
      "|    other    |   0.0989010989010989  |        0.075        |  0.08530805687203792   |\n",
      "|    author   |  0.45161290322580644  | 0.06140350877192982 |   0.1081081081081081   |\n",
      "|     date    |        0.09375        |        0.025        |  0.03947368421052632   |\n",
      "|    volume   |          0.0          |         0.0         |          0.0           |\n",
      "|  booktitle  |          0.0          |         0.0         |          0.0           |\n",
      "| institution |          0.0          |         0.0         |          0.0           |\n",
      "|     tech    |          0.0          |         0.0         |          0.0           |\n",
      "|    title    |   0.2696629213483146  | 0.43537414965986393 |  0.33304423243712056   |\n",
      "+-------------+-----------------------+---------------------+------------------------+\n",
      "9.951688349246979 10.0 current epoch 16: saving best model...\n",
      "9.530028223991394 9.951688349246979 current epoch 17: saving best model...\n",
      "9.126105725765228 9.530028223991394 current epoch 18: saving best model...\n",
      "8.767439544200897 9.126105725765228 current epoch 19: saving best model...\n",
      "8.453522503376007 8.767439544200897 current epoch 20: saving best model...\n",
      "8.13777869939804 8.453522503376007 current epoch 21: saving best model...\n",
      "7.816926002502441 8.13777869939804 current epoch 22: saving best model...\n",
      "7.529548823833466 7.816926002502441 current epoch 23: saving best model...\n",
      "7.252161264419556 7.529548823833466 current epoch 24: saving best model...\n",
      "6.980733692646027 7.252161264419556 current epoch 25: saving best model...\n",
      "6.718612492084503 6.980733692646027 current epoch 26: saving best model...\n",
      "6.4705153703689575 6.718612492084503 current epoch 27: saving best model...\n",
      "6.2219502329826355 6.4705153703689575 current epoch 28: saving best model...\n",
      "5.988633155822754 6.2219502329826355 current epoch 29: saving best model...\n",
      "5.786630392074585 5.988633155822754 current epoch 30: saving best model...\n",
      "5.597278833389282 5.786630392074585 current epoch 31: saving best model...\n",
      "5.428508758544922 5.597278833389282 current epoch 32: saving best model...\n",
      "5.284677475690842 5.428508758544922 current epoch 33: saving best model...\n",
      "5.146025478839874 5.284677475690842 current epoch 34: saving best model...\n",
      "5.008420616388321 5.146025478839874 current epoch 35: saving best model...\n",
      "4.875495910644531 5.008420616388321 current epoch 36: saving best model...\n",
      "4.76638188958168 4.875495910644531 current epoch 37: saving best model...\n",
      "4.6661898493766785 4.76638188958168 current epoch 38: saving best model...\n",
      "4.566528260707855 4.6661898493766785 current epoch 39: saving best model...\n",
      "4.481945365667343 4.566528260707855 current epoch 40: saving best model...\n",
      "4.380734026432037 4.481945365667343 current epoch 41: saving best model...\n",
      "4.26222088932991 4.380734026432037 current epoch 42: saving best model...\n",
      "4.1632843017578125 4.26222088932991 current epoch 43: saving best model...\n",
      "4.079328715801239 4.1632843017578125 current epoch 44: saving best model...\n",
      "4.003222078084946 4.079328715801239 current epoch 45: saving best model...\n",
      "3.9266440868377686 4.003222078084946 current epoch 46: saving best model...\n",
      "3.8661275804042816 3.9266440868377686 current epoch 47: saving best model...\n",
      "3.8148361146450043 3.8661275804042816 current epoch 48: saving best model...\n",
      "3.785400003194809 3.8148361146450043 current epoch 49: saving best model...\n",
      "3.712718188762665 3.785400003194809 current epoch 50: saving best model...\n",
      "3.647826999425888 3.712718188762665 current epoch 51: saving best model...\n",
      "3.571096360683441 3.647826999425888 current epoch 52: saving best model...\n",
      "3.5005590319633484 3.571096360683441 current epoch 53: saving best model...\n",
      "3.444763332605362 3.5005590319633484 current epoch 54: saving best model...\n",
      "3.347709983587265 3.444763332605362 current epoch 56: saving best model...\n",
      "3.2934577465057373 3.347709983587265 current epoch 57: saving best model...\n",
      "3.249793589115143 3.2934577465057373 current epoch 58: saving best model...\n",
      "3.209724932909012 3.249793589115143 current epoch 59: saving best model...\n",
      "3.1744527220726013 3.209724932909012 current epoch 60: saving best model...\n",
      "3.121602565050125 3.1744527220726013 current epoch 61: saving best model...\n",
      "3.0889212489128113 3.121602565050125 current epoch 62: saving best model...\n",
      "3.0782129168510437 3.0889212489128113 current epoch 63: saving best model...\n",
      "2.952051967382431 3.0782129168510437 current epoch 68: saving best model...\n",
      "2.9096899330615997 2.952051967382431 current epoch 69: saving best model...\n",
      "2.7361685633659363 2.9096899330615997 current epoch 70: saving best model...\n",
      "2.6698258072137833 2.7361685633659363 current epoch 71: saving best model...\n",
      "2.6215828359127045 2.6698258072137833 current epoch 72: saving best model...\n",
      "2.578765779733658 2.6215828359127045 current epoch 73: saving best model...\n",
      "2.536421239376068 2.578765779733658 current epoch 74: saving best model...\n",
      "2.498531624674797 2.536421239376068 current epoch 75: saving best model...\n",
      "2.463028386235237 2.498531624674797 current epoch 76: saving best model...\n",
      "2.4279095381498337 2.463028386235237 current epoch 77: saving best model...\n",
      "2.394523337483406 2.4279095381498337 current epoch 78: saving best model...\n",
      "2.3612198680639267 2.394523337483406 current epoch 79: saving best model...\n",
      "2.3271548449993134 2.3612198680639267 current epoch 80: saving best model...\n",
      "2.292926087975502 2.3271548449993134 current epoch 81: saving best model...\n",
      "2.2601571828126907 2.292926087975502 current epoch 82: saving best model...\n",
      "2.229415938258171 2.2601571828126907 current epoch 83: saving best model...\n",
      "2.2000954002141953 2.229415938258171 current epoch 84: saving best model...\n",
      "2.1717516481876373 2.2000954002141953 current epoch 85: saving best model...\n",
      "2.144417777657509 2.1717516481876373 current epoch 86: saving best model...\n",
      "2.1182804107666016 2.144417777657509 current epoch 87: saving best model...\n",
      "2.0942321866750717 2.1182804107666016 current epoch 88: saving best model...\n",
      "2.0682456344366074 2.0942321866750717 current epoch 89: saving best model...\n",
      "2.042259857058525 2.0682456344366074 current epoch 90: saving best model...\n",
      "2.016790747642517 2.042259857058525 current epoch 91: saving best model...\n",
      "1.9913498014211655 2.016790747642517 current epoch 92: saving best model...\n",
      "1.967658370733261 1.9913498014211655 current epoch 93: saving best model...\n",
      "1.9448962956666946 1.967658370733261 current epoch 94: saving best model...\n",
      "1.9227517694234848 1.9448962956666946 current epoch 95: saving best model...\n",
      "1.9043941050767899 1.9227517694234848 current epoch 96: saving best model...\n",
      "1.8833115994930267 1.9043941050767899 current epoch 97: saving best model...\n",
      "1.8606848865747452 1.8833115994930267 current epoch 98: saving best model...\n",
      "1.8380322754383087 1.8606848865747452 current epoch 99: saving best model...\n",
      "Epoch: 100, loss after minibatch     1: 0.17006\n",
      "Epoch: 100, loss after minibatch     2: 0.18949\n",
      "Epoch: 100, loss after minibatch     3: 0.18264\n",
      "Epoch: 100, loss after minibatch     4: 0.18737\n",
      "Epoch: 100, loss after minibatch     5: 0.18743\n",
      "Epoch: 100, loss after minibatch     6: 0.17441\n",
      "Epoch: 100, loss after minibatch     7: 0.18813\n",
      "Epoch: 100, loss after minibatch     8: 0.17748\n",
      "Epoch: 100, loss after minibatch     9: 0.21309\n",
      "Epoch: 100, loss after minibatch    10: 0.14965\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "|     Tag     |      Precision      |       Recall       |        FBeta        |\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "|  publisher  |  0.7931034482758621 |        0.92        |  0.851851851851852  |\n",
      "|    punct    | 0.05958363244795405 | 0.9963985594237695 | 0.11244327033800718 |\n",
      "|    editor   |         0.9         | 0.9473684210526315 |  0.9230769230769231 |\n",
      "|    pages    |  0.9827586206896551 |        1.0         |  0.9913043478260869 |\n",
      "|     note    |         0.0         |        0.0         |         0.0         |\n",
      "|   journal   |  0.9655172413793104 |        1.0         |  0.9824561403508771 |\n",
      "|   location  |  0.8947368421052632 | 0.9444444444444444 |  0.918918918918919  |\n",
      "|    other    |  0.9528301886792453 | 0.8416666666666667 |  0.8938053097345132 |\n",
      "|    author   |  0.9695652173913043 | 0.9780701754385965 |  0.9737991266375545 |\n",
      "|     date    |  0.9908256880733946 |        0.9         |  0.9432314410480349 |\n",
      "|    volume   |  0.967741935483871  | 0.967741935483871  |  0.967741935483871  |\n",
      "|  booktitle  |  0.9541284403669725 | 0.9904761904761905 |  0.9719626168224299 |\n",
      "| institution |  0.4838709677419355 | 0.7142857142857143 |  0.5769230769230769 |\n",
      "|     tech    |         1.0         |        0.3         |  0.4615384615384615 |\n",
      "|    title    |  0.954248366013072  | 0.9931972789115646 |  0.9733333333333335 |\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "1.8197576254606247 1.8380322754383087 current epoch 100: saving best model...\n",
      "1.8068292438983917 1.8197576254606247 current epoch 101: saving best model...\n",
      "1.7931268066167831 1.8068292438983917 current epoch 102: saving best model...\n",
      "1.791303664445877 1.7931268066167831 current epoch 104: saving best model...\n",
      "1.7826698571443558 1.791303664445877 current epoch 105: saving best model...\n",
      "1.766580194234848 1.7826698571443558 current epoch 106: saving best model...\n",
      "1.755677044391632 1.766580194234848 current epoch 107: saving best model...\n",
      "1.7553328573703766 1.755677044391632 current epoch 108: saving best model...\n",
      "1.711078479886055 1.7553328573703766 current epoch 117: saving best model...\n",
      "1.613875336945057 1.711078479886055 current epoch 118: saving best model...\n",
      "1.5774386152625084 1.613875336945057 current epoch 119: saving best model...\n",
      "1.5618273094296455 1.5774386152625084 current epoch 120: saving best model...\n",
      "1.5417006090283394 1.5618273094296455 current epoch 121: saving best model...\n",
      "1.5141872987151146 1.5417006090283394 current epoch 122: saving best model...\n",
      "1.4928541779518127 1.5141872987151146 current epoch 123: saving best model...\n",
      "1.476172387599945 1.4928541779518127 current epoch 124: saving best model...\n",
      "1.4589065834879875 1.476172387599945 current epoch 125: saving best model...\n",
      "1.4404042065143585 1.4589065834879875 current epoch 126: saving best model...\n",
      "1.4221929460763931 1.4404042065143585 current epoch 127: saving best model...\n",
      "1.407356396317482 1.4221929460763931 current epoch 128: saving best model...\n",
      "1.389017902314663 1.407356396317482 current epoch 129: saving best model...\n",
      "1.371934436261654 1.389017902314663 current epoch 130: saving best model...\n",
      "1.3568635508418083 1.371934436261654 current epoch 131: saving best model...\n",
      "1.342162385582924 1.3568635508418083 current epoch 132: saving best model...\n",
      "1.3295738473534584 1.342162385582924 current epoch 133: saving best model...\n",
      "1.3182369694113731 1.3295738473534584 current epoch 134: saving best model...\n",
      "1.3072757050395012 1.3182369694113731 current epoch 135: saving best model...\n",
      "1.296264074742794 1.3072757050395012 current epoch 136: saving best model...\n",
      "1.284834936261177 1.296264074742794 current epoch 137: saving best model...\n",
      "1.2751337140798569 1.284834936261177 current epoch 138: saving best model...\n",
      "1.263091191649437 1.2751337140798569 current epoch 139: saving best model...\n",
      "1.252818800508976 1.263091191649437 current epoch 140: saving best model...\n",
      "1.243935838341713 1.252818800508976 current epoch 141: saving best model...\n",
      "1.237382709980011 1.243935838341713 current epoch 142: saving best model...\n",
      "1.2244132906198502 1.237382709980011 current epoch 143: saving best model...\n",
      "1.2147811278700829 1.2244132906198502 current epoch 144: saving best model...\n",
      "1.2014357969164848 1.2147811278700829 current epoch 145: saving best model...\n",
      "1.1904787197709084 1.2014357969164848 current epoch 146: saving best model...\n",
      "1.1795633509755135 1.1904787197709084 current epoch 147: saving best model...\n",
      "1.167827844619751 1.1795633509755135 current epoch 148: saving best model...\n",
      "1.1615170910954475 1.167827844619751 current epoch 149: saving best model...\n",
      "1.1551136299967766 1.1615170910954475 current epoch 150: saving best model...\n",
      "1.1448235884308815 1.1551136299967766 current epoch 151: saving best model...\n",
      "1.1340141221880913 1.1448235884308815 current epoch 152: saving best model...\n",
      "1.1236234158277512 1.1340141221880913 current epoch 153: saving best model...\n",
      "1.1150297075510025 1.1236234158277512 current epoch 154: saving best model...\n",
      "1.107548013329506 1.1150297075510025 current epoch 155: saving best model...\n",
      "1.102977953851223 1.107548013329506 current epoch 156: saving best model...\n",
      "1.1018293499946594 1.102977953851223 current epoch 157: saving best model...\n",
      "1.1008497402071953 1.1018293499946594 current epoch 158: saving best model...\n",
      "1.0987172573804855 1.1008497402071953 current epoch 159: saving best model...\n",
      "1.0946301594376564 1.0987172573804855 current epoch 160: saving best model...\n",
      "1.090862326323986 1.0946301594376564 current epoch 161: saving best model...\n",
      "1.0773401036858559 1.090862326323986 current epoch 162: saving best model...\n",
      "1.0692522525787354 1.0773401036858559 current epoch 163: saving best model...\n",
      "1.0522452220320702 1.0692522525787354 current epoch 172: saving best model...\n",
      "0.968839056789875 1.0522452220320702 current epoch 183: saving best model...\n",
      "0.9245957992970943 0.968839056789875 current epoch 184: saving best model...\n",
      "0.899034395813942 0.9245957992970943 current epoch 185: saving best model...\n",
      "0.8822120018303394 0.899034395813942 current epoch 186: saving best model...\n",
      "0.8721222653985023 0.8822120018303394 current epoch 187: saving best model...\n",
      "0.8609792403876781 0.8721222653985023 current epoch 188: saving best model...\n",
      "0.8513937778770924 0.8609792403876781 current epoch 189: saving best model...\n",
      "0.842683233320713 0.8513937778770924 current epoch 190: saving best model...\n",
      "0.8344915844500065 0.842683233320713 current epoch 191: saving best model...\n",
      "0.8268502950668335 0.8344915844500065 current epoch 192: saving best model...\n",
      "0.8194127529859543 0.8268502950668335 current epoch 193: saving best model...\n",
      "0.8116427809000015 0.8194127529859543 current epoch 194: saving best model...\n",
      "0.8043265230953693 0.8116427809000015 current epoch 195: saving best model...\n",
      "0.7985373213887215 0.8043265230953693 current epoch 196: saving best model...\n",
      "0.7914016991853714 0.7985373213887215 current epoch 197: saving best model...\n",
      "0.7849919609725475 0.7914016991853714 current epoch 198: saving best model...\n",
      "0.7780018225312233 0.7849919609725475 current epoch 199: saving best model...\n",
      "Epoch: 200, loss after minibatch     1: 0.07756\n",
      "Epoch: 200, loss after minibatch     2: 0.08425\n",
      "Epoch: 200, loss after minibatch     3: 0.07964\n",
      "Epoch: 200, loss after minibatch     4: 0.08235\n",
      "Epoch: 200, loss after minibatch     5: 0.07956\n",
      "Epoch: 200, loss after minibatch     6: 0.07183\n",
      "Epoch: 200, loss after minibatch     7: 0.08500\n",
      "Epoch: 200, loss after minibatch     8: 0.07707\n",
      "Epoch: 200, loss after minibatch     9: 0.09136\n",
      "Epoch: 200, loss after minibatch    10: 0.04316\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "|     Tag     |      Precision      |       Recall       |        FBeta        |\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "|  publisher  |  0.9615384615384616 |        1.0         |  0.9803921568627451 |\n",
      "|    punct    | 0.05979470246213481 |        1.0         | 0.11284204822541317 |\n",
      "|    editor   |  0.9473684210526315 | 0.9473684210526315 |  0.9473684210526315 |\n",
      "|    pages    |         1.0         |        1.0         |         1.0         |\n",
      "|     note    |         1.0         |        1.0         |         1.0         |\n",
      "|   journal   |  0.9929078014184397 |        1.0         |  0.9964412811387899 |\n",
      "|   location  |         1.0         | 0.9444444444444444 |  0.9714285714285714 |\n",
      "|    other    |         1.0         | 0.9833333333333333 |  0.9915966386554621 |\n",
      "|    author   |  0.9912854030501089 | 0.9978070175438597 |  0.9945355191256832 |\n",
      "|     date    |         1.0         |       0.975        |  0.9873417721518987 |\n",
      "|    volume   |         1.0         |        1.0         |         1.0         |\n",
      "|  booktitle  |  0.9905660377358491 |        1.0         |  0.9952606635071091 |\n",
      "| institution |  0.9523809523809523 | 0.9523809523809523 |  0.9523809523809523 |\n",
      "|     tech    |         1.0         |        1.0         |         1.0         |\n",
      "|    title    |  0.9931972789115646 | 0.9931972789115646 |  0.9931972789115646 |\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "0.7717854417860508 0.7780018225312233 current epoch 200: saving best model...\n",
      "0.7654648013412952 0.7717854417860508 current epoch 201: saving best model...\n",
      "0.759329080581665 0.7654648013412952 current epoch 202: saving best model...\n",
      "0.7533681690692902 0.759329080581665 current epoch 203: saving best model...\n",
      "0.7472414299845695 0.7533681690692902 current epoch 204: saving best model...\n",
      "0.741394404321909 0.7472414299845695 current epoch 205: saving best model...\n",
      "0.7355309911072254 0.741394404321909 current epoch 206: saving best model...\n",
      "0.7296555526554585 0.7355309911072254 current epoch 207: saving best model...\n",
      "0.7239333242177963 0.7296555526554585 current epoch 208: saving best model...\n",
      "0.7183081246912479 0.7239333242177963 current epoch 209: saving best model...\n",
      "0.7128536999225616 0.7183081246912479 current epoch 210: saving best model...\n",
      "0.7074573077261448 0.7128536999225616 current epoch 211: saving best model...\n",
      "0.7022479772567749 0.7074573077261448 current epoch 212: saving best model...\n",
      "0.6971598863601685 0.7022479772567749 current epoch 213: saving best model...\n",
      "0.6920887604355812 0.6971598863601685 current epoch 214: saving best model...\n",
      "0.6870332658290863 0.6920887604355812 current epoch 215: saving best model...\n",
      "0.6819397136569023 0.6870332658290863 current epoch 216: saving best model...\n",
      "0.676689125597477 0.6819397136569023 current epoch 217: saving best model...\n",
      "0.6716162599623203 0.676689125597477 current epoch 218: saving best model...\n",
      "0.6667353808879852 0.6716162599623203 current epoch 219: saving best model...\n",
      "0.661872711032629 0.6667353808879852 current epoch 220: saving best model...\n",
      "0.6572703868150711 0.661872711032629 current epoch 221: saving best model...\n",
      "0.6529380790889263 0.6572703868150711 current epoch 222: saving best model...\n",
      "0.648650549352169 0.6529380790889263 current epoch 223: saving best model...\n",
      "0.6449744254350662 0.648650549352169 current epoch 224: saving best model...\n",
      "0.6425048038363457 0.6449744254350662 current epoch 225: saving best model...\n",
      "0.6404917649924755 0.6425048038363457 current epoch 226: saving best model...\n",
      "0.6385348103940487 0.6404917649924755 current epoch 227: saving best model...\n",
      "0.6351817026734352 0.6385348103940487 current epoch 228: saving best model...\n",
      "0.6328898072242737 0.6351817026734352 current epoch 229: saving best model...\n",
      "0.6313340440392494 0.6328898072242737 current epoch 230: saving best model...\n",
      "0.6158840395510197 0.6313340440392494 current epoch 249: saving best model...\n",
      "0.596334882080555 0.6158840395510197 current epoch 250: saving best model...\n",
      "0.5810096152126789 0.596334882080555 current epoch 251: saving best model...\n",
      "0.5700385812669992 0.5810096152126789 current epoch 252: saving best model...\n",
      "0.5613434053957462 0.5700385812669992 current epoch 253: saving best model...\n",
      "0.5556469950824976 0.5613434053957462 current epoch 254: saving best model...\n",
      "0.5502668283879757 0.5556469950824976 current epoch 255: saving best model...\n",
      "0.5457549653947353 0.5502668283879757 current epoch 256: saving best model...\n",
      "0.541729461401701 0.5457549653947353 current epoch 257: saving best model...\n",
      "0.538096945732832 0.541729461401701 current epoch 258: saving best model...\n",
      "0.534733509644866 0.538096945732832 current epoch 259: saving best model...\n",
      "0.5316101666539907 0.534733509644866 current epoch 260: saving best model...\n",
      "0.5288877375423908 0.5316101666539907 current epoch 261: saving best model...\n",
      "0.5267045386135578 0.5288877375423908 current epoch 262: saving best model...\n",
      "0.5250394828617573 0.5267045386135578 current epoch 263: saving best model...\n",
      "0.5238441321998835 0.5250394828617573 current epoch 264: saving best model...\n",
      "0.5232698861509562 0.5238441321998835 current epoch 265: saving best model...\n",
      "0.49128939397633076 0.5232698861509562 current epoch 285: saving best model...\n",
      "0.4640635885298252 0.49128939397633076 current epoch 286: saving best model...\n",
      "0.45392800495028496 0.4640635885298252 current epoch 287: saving best model...\n",
      "0.450116153806448 0.45392800495028496 current epoch 288: saving best model...\n",
      "0.44296415336430073 0.450116153806448 current epoch 289: saving best model...\n",
      "0.43728155456483364 0.44296415336430073 current epoch 290: saving best model...\n",
      "0.4323598425835371 0.43728155456483364 current epoch 291: saving best model...\n",
      "0.42835409566760063 0.4323598425835371 current epoch 292: saving best model...\n",
      "0.42467377707362175 0.42835409566760063 current epoch 293: saving best model...\n",
      "0.42123071663081646 0.42467377707362175 current epoch 294: saving best model...\n",
      "0.41802236437797546 0.42123071663081646 current epoch 295: saving best model...\n",
      "0.41483178921043873 0.41802236437797546 current epoch 296: saving best model...\n",
      "0.4117092303931713 0.41483178921043873 current epoch 297: saving best model...\n",
      "0.4086179919540882 0.4117092303931713 current epoch 298: saving best model...\n",
      "0.4055136516690254 0.4086179919540882 current epoch 299: saving best model...\n",
      "Epoch: 300, loss after minibatch     1: 0.04016\n",
      "Epoch: 300, loss after minibatch     2: 0.04427\n",
      "Epoch: 300, loss after minibatch     3: 0.04287\n",
      "Epoch: 300, loss after minibatch     4: 0.04189\n",
      "Epoch: 300, loss after minibatch     5: 0.04114\n",
      "Epoch: 300, loss after minibatch     6: 0.03760\n",
      "Epoch: 300, loss after minibatch     7: 0.04572\n",
      "Epoch: 300, loss after minibatch     8: 0.04156\n",
      "Epoch: 300, loss after minibatch     9: 0.04860\n",
      "Epoch: 300, loss after minibatch    10: 0.01870\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "|     Tag     |      Precision      |       Recall       |        FBeta        |\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "|  publisher  |         1.0         |        1.0         |         1.0         |\n",
      "|    punct    | 0.05979470246213481 |        1.0         | 0.11284204822541317 |\n",
      "|    editor   |         1.0         |        1.0         |         1.0         |\n",
      "|    pages    |         1.0         |        1.0         |         1.0         |\n",
      "|     note    |         1.0         |        1.0         |         1.0         |\n",
      "|   journal   |         1.0         |        1.0         |         1.0         |\n",
      "|   location  |         1.0         |        1.0         |         1.0         |\n",
      "|    other    |         1.0         |        1.0         |         1.0         |\n",
      "|    author   |  0.9978118161925602 |        1.0         |  0.9989047097480832 |\n",
      "|     date    |         1.0         |        1.0         |         1.0         |\n",
      "|    volume   |         1.0         |        1.0         |         1.0         |\n",
      "|  booktitle  |         1.0         |        1.0         |         1.0         |\n",
      "| institution |  0.9523809523809523 | 0.9523809523809523 |  0.9523809523809523 |\n",
      "|     tech    |         1.0         |        1.0         |         1.0         |\n",
      "|    title    |  0.9977272727272727 | 0.9954648526077098 |  0.996594778660613  |\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "0.4025065004825592 0.4055136516690254 current epoch 300: saving best model...\n",
      "0.3995644636452198 0.4025065004825592 current epoch 301: saving best model...\n",
      "0.3966597728431225 0.3995644636452198 current epoch 302: saving best model...\n",
      "0.3938108757138252 0.3966597728431225 current epoch 303: saving best model...\n",
      "0.3910207077860832 0.3938108757138252 current epoch 304: saving best model...\n",
      "0.3882675915956497 0.3910207077860832 current epoch 305: saving best model...\n",
      "0.3855487424880266 0.3882675915956497 current epoch 306: saving best model...\n",
      "0.38285782746970654 0.3855487424880266 current epoch 307: saving best model...\n",
      "0.38018069230020046 0.38285782746970654 current epoch 308: saving best model...\n",
      "0.37752117961645126 0.38018069230020046 current epoch 309: saving best model...\n",
      "0.3748888783156872 0.37752117961645126 current epoch 310: saving best model...\n",
      "0.3722826596349478 0.3748888783156872 current epoch 311: saving best model...\n",
      "0.3697030786424875 0.3722826596349478 current epoch 312: saving best model...\n",
      "0.3671573456376791 0.3697030786424875 current epoch 313: saving best model...\n",
      "0.36464570462703705 0.3671573456376791 current epoch 314: saving best model...\n",
      "0.3621644265949726 0.36464570462703705 current epoch 315: saving best model...\n",
      "0.35971156507730484 0.3621644265949726 current epoch 316: saving best model...\n",
      "0.35728191398084164 0.35971156507730484 current epoch 317: saving best model...\n",
      "0.3548677768558264 0.35728191398084164 current epoch 318: saving best model...\n",
      "0.3524697385728359 0.3548677768558264 current epoch 319: saving best model...\n",
      "0.3500860258936882 0.3524697385728359 current epoch 320: saving best model...\n",
      "0.3476904984563589 0.3500860258936882 current epoch 321: saving best model...\n",
      "0.3453229106962681 0.3476904984563589 current epoch 322: saving best model...\n",
      "0.3429814837872982 0.3453229106962681 current epoch 323: saving best model...\n",
      "0.3406657539308071 0.3429814837872982 current epoch 324: saving best model...\n",
      "0.33835876174271107 0.3406657539308071 current epoch 325: saving best model...\n",
      "0.3360491544008255 0.33835876174271107 current epoch 326: saving best model...\n",
      "0.33376528415828943 0.3360491544008255 current epoch 327: saving best model...\n",
      "0.3314985251054168 0.33376528415828943 current epoch 328: saving best model...\n",
      "0.3292434262111783 0.3314985251054168 current epoch 329: saving best model...\n",
      "0.32700440753251314 0.3292434262111783 current epoch 330: saving best model...\n",
      "0.3247718531638384 0.32700440753251314 current epoch 331: saving best model...\n",
      "0.3225491913035512 0.3247718531638384 current epoch 332: saving best model...\n",
      "0.32036673929542303 0.3225491913035512 current epoch 333: saving best model...\n",
      "0.31823166087269783 0.32036673929542303 current epoch 334: saving best model...\n",
      "0.3161156037822366 0.31823166087269783 current epoch 335: saving best model...\n",
      "0.3139842748641968 0.3161156037822366 current epoch 336: saving best model...\n",
      "0.311822310090065 0.3139842748641968 current epoch 337: saving best model...\n",
      "0.3096359483897686 0.311822310090065 current epoch 338: saving best model...\n",
      "0.30746332090348005 0.3096359483897686 current epoch 339: saving best model...\n",
      "0.3053303901106119 0.30746332090348005 current epoch 340: saving best model...\n",
      "0.3032219475135207 0.3053303901106119 current epoch 341: saving best model...\n",
      "0.3011495042592287 0.3032219475135207 current epoch 342: saving best model...\n",
      "0.2991387452930212 0.3011495042592287 current epoch 343: saving best model...\n",
      "0.2971728229895234 0.2991387452930212 current epoch 344: saving best model...\n",
      "0.2951859775930643 0.2971728229895234 current epoch 345: saving best model...\n",
      "0.2931147161871195 0.2951859775930643 current epoch 346: saving best model...\n",
      "0.291003217920661 0.2931147161871195 current epoch 347: saving best model...\n",
      "0.2889275876805186 0.291003217920661 current epoch 348: saving best model...\n",
      "0.2869422696530819 0.2889275876805186 current epoch 349: saving best model...\n",
      "0.2849917309358716 0.2869422696530819 current epoch 350: saving best model...\n",
      "0.2830685069784522 0.2849917309358716 current epoch 351: saving best model...\n",
      "0.28141557332128286 0.2830685069784522 current epoch 352: saving best model...\n",
      "0.27995127253234386 0.28141557332128286 current epoch 353: saving best model...\n",
      "0.2784237004816532 0.27995127253234386 current epoch 354: saving best model...\n",
      "0.2770214891061187 0.2784237004816532 current epoch 355: saving best model...\n",
      "0.2765402141958475 0.2770214891061187 current epoch 356: saving best model...\n",
      "0.2750476188957691 0.2765402141958475 current epoch 357: saving best model...\n",
      "0.2718233745545149 0.2750476188957691 current epoch 382: saving best model...\n",
      "0.26042409986257553 0.2718233745545149 current epoch 383: saving best model...\n",
      "0.2514946274459362 0.26042409986257553 current epoch 384: saving best model...\n",
      "0.24650486931204796 0.2514946274459362 current epoch 385: saving best model...\n",
      "0.24274091329425573 0.24650486931204796 current epoch 386: saving best model...\n",
      "0.23964598402380943 0.24274091329425573 current epoch 387: saving best model...\n",
      "0.23719223029911518 0.23964598402380943 current epoch 388: saving best model...\n",
      "0.23489613085985184 0.23719223029911518 current epoch 389: saving best model...\n",
      "0.23306521214544773 0.23489613085985184 current epoch 390: saving best model...\n",
      "0.2313222661614418 0.23306521214544773 current epoch 391: saving best model...\n",
      "0.22978970874100924 0.2313222661614418 current epoch 392: saving best model...\n",
      "0.22841624822467566 0.22978970874100924 current epoch 393: saving best model...\n",
      "0.227118868380785 0.22841624822467566 current epoch 394: saving best model...\n",
      "0.22590232081711292 0.227118868380785 current epoch 395: saving best model...\n",
      "0.22477713972330093 0.22590232081711292 current epoch 396: saving best model...\n",
      "0.223717643879354 0.22477713972330093 current epoch 397: saving best model...\n",
      "0.22271685022860765 0.223717643879354 current epoch 398: saving best model...\n",
      "0.22174878232181072 0.22271685022860765 current epoch 399: saving best model...\n",
      "Epoch: 400, loss after minibatch     1: 0.02097\n",
      "Epoch: 400, loss after minibatch     2: 0.02470\n",
      "Epoch: 400, loss after minibatch     3: 0.02450\n",
      "Epoch: 400, loss after minibatch     4: 0.02249\n",
      "Epoch: 400, loss after minibatch     5: 0.02149\n",
      "Epoch: 400, loss after minibatch     6: 0.02011\n",
      "Epoch: 400, loss after minibatch     7: 0.02635\n",
      "Epoch: 400, loss after minibatch     8: 0.02414\n",
      "Epoch: 400, loss after minibatch     9: 0.02677\n",
      "Epoch: 400, loss after minibatch    10: 0.00930\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "|     Tag     |      Precision      |       Recall       |        FBeta        |\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "|  publisher  |         1.0         |        1.0         |         1.0         |\n",
      "|    punct    | 0.05979470246213481 |        1.0         | 0.11284204822541317 |\n",
      "|    editor   |         1.0         |        1.0         |         1.0         |\n",
      "|    pages    |         1.0         |        1.0         |         1.0         |\n",
      "|     note    |         1.0         |        1.0         |         1.0         |\n",
      "|   journal   |         1.0         |        1.0         |         1.0         |\n",
      "|   location  |         1.0         |        1.0         |         1.0         |\n",
      "|    other    |         1.0         |        1.0         |         1.0         |\n",
      "|    author   |  0.9978118161925602 |        1.0         |  0.9989047097480832 |\n",
      "|     date    |         1.0         |        1.0         |         1.0         |\n",
      "|    volume   |         1.0         |        1.0         |         1.0         |\n",
      "|  booktitle  |         1.0         |        1.0         |         1.0         |\n",
      "| institution |         1.0         | 0.9523809523809523 |  0.975609756097561  |\n",
      "|     tech    |         1.0         |        1.0         |         1.0         |\n",
      "|    title    |  0.9977324263038548 | 0.9977324263038548 |  0.9977324263038548 |\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "0.22083323914557695 0.22174878232181072 current epoch 400: saving best model...\n",
      "0.21997454669326544 0.22083323914557695 current epoch 401: saving best model...\n",
      "0.21916887070983648 0.21997454669326544 current epoch 402: saving best model...\n",
      "0.21841641422361135 0.21916887070983648 current epoch 403: saving best model...\n",
      "0.21772472839802504 0.21841641422361135 current epoch 404: saving best model...\n",
      "0.21709641441702843 0.21772472839802504 current epoch 405: saving best model...\n",
      "0.21653737034648657 0.21709641441702843 current epoch 406: saving best model...\n",
      "0.21604393050074577 0.21653737034648657 current epoch 407: saving best model...\n",
      "0.21559677831828594 0.21604393050074577 current epoch 408: saving best model...\n",
      "0.21511956304311752 0.21559677831828594 current epoch 409: saving best model...\n",
      "0.2147618094459176 0.21511956304311752 current epoch 410: saving best model...\n",
      "0.21462130546569824 0.2147618094459176 current epoch 411: saving best model...\n",
      "0.21438499633222818 0.21462130546569824 current epoch 412: saving best model...\n",
      "0.21084747649729252 0.21438499633222818 current epoch 437: saving best model...\n",
      "0.20147778186947107 0.21084747649729252 current epoch 438: saving best model...\n",
      "0.1941762063652277 0.20147778186947107 current epoch 439: saving best model...\n",
      "0.18896419135853648 0.1941762063652277 current epoch 440: saving best model...\n",
      "0.18505655601620674 0.18896419135853648 current epoch 441: saving best model...\n",
      "0.182450863532722 0.18505655601620674 current epoch 442: saving best model...\n",
      "0.17991987708956003 0.182450863532722 current epoch 443: saving best model...\n",
      "0.17759994138032198 0.17991987708956003 current epoch 444: saving best model...\n",
      "0.1757369190454483 0.17759994138032198 current epoch 445: saving best model...\n",
      "0.17386011127382517 0.1757369190454483 current epoch 446: saving best model...\n",
      "0.1721556126140058 0.17386011127382517 current epoch 447: saving best model...\n",
      "0.17054221173748374 0.1721556126140058 current epoch 448: saving best model...\n",
      "0.16900249291211367 0.17054221173748374 current epoch 449: saving best model...\n",
      "0.16751907393336296 0.16900249291211367 current epoch 450: saving best model...\n",
      "0.1660809307359159 0.16751907393336296 current epoch 451: saving best model...\n",
      "0.16467188205569983 0.1660809307359159 current epoch 452: saving best model...\n",
      "0.16329046664759517 0.16467188205569983 current epoch 453: saving best model...\n",
      "0.1619352330453694 0.16329046664759517 current epoch 454: saving best model...\n",
      "0.1605652836151421 0.1619352330453694 current epoch 455: saving best model...\n",
      "0.15924739418551326 0.1605652836151421 current epoch 456: saving best model...\n",
      "0.1579974857158959 0.15924739418551326 current epoch 457: saving best model...\n",
      "0.15680641122162342 0.1579974857158959 current epoch 458: saving best model...\n",
      "0.1556633864529431 0.15680641122162342 current epoch 459: saving best model...\n",
      "0.15455533796921372 0.1556633864529431 current epoch 460: saving best model...\n",
      "0.15347548248246312 0.15455533796921372 current epoch 461: saving best model...\n",
      "0.15241974219679832 0.15347548248246312 current epoch 462: saving best model...\n",
      "0.15138510148972273 0.15241974219679832 current epoch 463: saving best model...\n",
      "0.15036837616935372 0.15138510148972273 current epoch 464: saving best model...\n",
      "0.149368345271796 0.15036837616935372 current epoch 465: saving best model...\n",
      "0.14838476618751884 0.149368345271796 current epoch 466: saving best model...\n",
      "0.14741880586370826 0.14838476618751884 current epoch 467: saving best model...\n",
      "0.1464704442769289 0.14741880586370826 current epoch 468: saving best model...\n",
      "0.14553824812173843 0.1464704442769289 current epoch 469: saving best model...\n",
      "0.1446212031878531 0.14553824812173843 current epoch 470: saving best model...\n",
      "0.14371838746592402 0.1446212031878531 current epoch 471: saving best model...\n",
      "0.14282852597534657 0.14371838746592402 current epoch 472: saving best model...\n",
      "0.14195069298148155 0.14282852597534657 current epoch 473: saving best model...\n",
      "0.14108407450839877 0.14195069298148155 current epoch 474: saving best model...\n",
      "0.1402274314314127 0.14108407450839877 current epoch 475: saving best model...\n",
      "0.13938000006601214 0.1402274314314127 current epoch 476: saving best model...\n",
      "0.13854081463068724 0.13938000006601214 current epoch 477: saving best model...\n",
      "0.13770903460681438 0.13854081463068724 current epoch 478: saving best model...\n",
      "0.13688420737162232 0.13770903460681438 current epoch 479: saving best model...\n",
      "0.13606631103903055 0.13688420737162232 current epoch 480: saving best model...\n",
      "0.13525531021878123 0.13606631103903055 current epoch 481: saving best model...\n",
      "0.13445163471624255 0.13525531021878123 current epoch 482: saving best model...\n",
      "0.13365515926852822 0.13445163471624255 current epoch 483: saving best model...\n",
      "0.1328659043647349 0.13365515926852822 current epoch 484: saving best model...\n",
      "0.13208331493660808 0.1328659043647349 current epoch 485: saving best model...\n",
      "0.13130663754418492 0.13208331493660808 current epoch 486: saving best model...\n",
      "0.13053495390340686 0.13130663754418492 current epoch 487: saving best model...\n",
      "0.12976700067520142 0.13053495390340686 current epoch 488: saving best model...\n",
      "0.12900124955922365 0.12976700067520142 current epoch 489: saving best model...\n",
      "0.12823403580114245 0.12900124955922365 current epoch 490: saving best model...\n",
      "0.12745896819978952 0.12823403580114245 current epoch 491: saving best model...\n",
      "0.12666702875867486 0.12745896819978952 current epoch 492: saving best model...\n",
      "0.1258694063872099 0.12666702875867486 current epoch 493: saving best model...\n",
      "0.125088004861027 0.1258694063872099 current epoch 494: saving best model...\n",
      "0.12430421309545636 0.125088004861027 current epoch 495: saving best model...\n",
      "0.12349121598526835 0.12430421309545636 current epoch 496: saving best model...\n",
      "0.122668681666255 0.12349121598526835 current epoch 497: saving best model...\n",
      "0.12189104873687029 0.122668681666255 current epoch 498: saving best model...\n",
      "0.1211202428676188 0.12189104873687029 current epoch 499: saving best model...\n",
      "Epoch: 500, loss after minibatch     1: 0.01007\n",
      "Epoch: 500, loss after minibatch     2: 0.01280\n",
      "Epoch: 500, loss after minibatch     3: 0.01426\n",
      "Epoch: 500, loss after minibatch     4: 0.01312\n",
      "Epoch: 500, loss after minibatch     5: 0.01151\n",
      "Epoch: 500, loss after minibatch     6: 0.01029\n",
      "Epoch: 500, loss after minibatch     7: 0.01370\n",
      "Epoch: 500, loss after minibatch     8: 0.01455\n",
      "Epoch: 500, loss after minibatch     9: 0.01507\n",
      "Epoch: 500, loss after minibatch    10: 0.00494\n",
      "+-------------+---------------------+--------+---------------------+\n",
      "|     Tag     |      Precision      | Recall |        FBeta        |\n",
      "+-------------+---------------------+--------+---------------------+\n",
      "|  publisher  |         1.0         |  1.0   |         1.0         |\n",
      "|    punct    | 0.05979470246213481 |  1.0   | 0.11284204822541317 |\n",
      "|    editor   |         1.0         |  1.0   |         1.0         |\n",
      "|    pages    |         1.0         |  1.0   |         1.0         |\n",
      "|     note    |         1.0         |  1.0   |         1.0         |\n",
      "|   journal   |         1.0         |  1.0   |         1.0         |\n",
      "|   location  |         1.0         |  1.0   |         1.0         |\n",
      "|    other    |         1.0         |  1.0   |         1.0         |\n",
      "|    author   |         1.0         |  1.0   |         1.0         |\n",
      "|     date    |         1.0         |  1.0   |         1.0         |\n",
      "|    volume   |         1.0         |  1.0   |         1.0         |\n",
      "|  booktitle  |         1.0         |  1.0   |         1.0         |\n",
      "| institution |         1.0         |  1.0   |         1.0         |\n",
      "|     tech    |         1.0         |  1.0   |         1.0         |\n",
      "|    title    |         1.0         |  1.0   |         1.0         |\n",
      "+-------------+---------------------+--------+---------------------+\n",
      "0.12030739523470402 0.1211202428676188 current epoch 500: saving best model...\n",
      "0.11956260399892926 0.12030739523470402 current epoch 501: saving best model...\n",
      "0.11883803270757198 0.11956260399892926 current epoch 502: saving best model...\n",
      "0.11812926130369306 0.11883803270757198 current epoch 503: saving best model...\n",
      "0.11742646479979157 0.11812926130369306 current epoch 504: saving best model...\n",
      "0.1167301437817514 0.11742646479979157 current epoch 505: saving best model...\n",
      "0.11604042910039425 0.1167301437817514 current epoch 506: saving best model...\n",
      "0.11535712704062462 0.11604042910039425 current epoch 507: saving best model...\n",
      "0.1146795628592372 0.11535712704062462 current epoch 508: saving best model...\n",
      "0.11400771792978048 0.1146795628592372 current epoch 509: saving best model...\n",
      "0.1133416797965765 0.11400771792978048 current epoch 510: saving best model...\n",
      "0.11268036579713225 0.1133416797965765 current epoch 511: saving best model...\n",
      "0.11202321900054812 0.11268036579713225 current epoch 512: saving best model...\n",
      "0.1113699390552938 0.11202321900054812 current epoch 513: saving best model...\n",
      "0.11072129616513848 0.1113699390552938 current epoch 514: saving best model...\n",
      "0.11007690941914916 0.11072129616513848 current epoch 515: saving best model...\n",
      "0.10943645471706986 0.11007690941914916 current epoch 516: saving best model...\n",
      "0.10879951389506459 0.10943645471706986 current epoch 517: saving best model...\n",
      "0.10816503409296274 0.10879951389506459 current epoch 518: saving best model...\n",
      "0.10753206303343177 0.10816503409296274 current epoch 519: saving best model...\n",
      "0.10690106824040413 0.10753206303343177 current epoch 520: saving best model...\n",
      "0.10626890882849693 0.10690106824040413 current epoch 521: saving best model...\n",
      "0.10562888393178582 0.10626890882849693 current epoch 522: saving best model...\n",
      "0.10496048489585519 0.10562888393178582 current epoch 523: saving best model...\n",
      "0.10418353835120797 0.10496048489585519 current epoch 524: saving best model...\n",
      "0.10330133279785514 0.10418353835120797 current epoch 525: saving best model...\n",
      "0.10266494145616889 0.10330133279785514 current epoch 526: saving best model...\n",
      "0.10203935066238046 0.10266494145616889 current epoch 527: saving best model...\n",
      "0.10140443779528141 0.10203935066238046 current epoch 528: saving best model...\n",
      "0.10078545240685344 0.10140443779528141 current epoch 529: saving best model...\n",
      "0.1002001822926104 0.10078545240685344 current epoch 530: saving best model...\n",
      "0.09961577970534563 0.1002001822926104 current epoch 531: saving best model...\n",
      "0.0990150417201221 0.09961577970534563 current epoch 532: saving best model...\n",
      "0.09841880667954683 0.0990150417201221 current epoch 533: saving best model...\n",
      "0.09782889857888222 0.09841880667954683 current epoch 534: saving best model...\n",
      "0.09723615041002631 0.09782889857888222 current epoch 535: saving best model...\n",
      "0.09660659544169903 0.09723615041002631 current epoch 536: saving best model...\n",
      "0.09583564940840006 0.09660659544169903 current epoch 537: saving best model...\n",
      "0.09521660581231117 0.09583564940840006 current epoch 538: saving best model...\n",
      "0.09461280796676874 0.09521660581231117 current epoch 539: saving best model...\n",
      "0.09399579372256994 0.09461280796676874 current epoch 540: saving best model...\n",
      "0.09339619986712933 0.09399579372256994 current epoch 541: saving best model...\n",
      "0.09281468158587813 0.09339619986712933 current epoch 542: saving best model...\n",
      "0.09225540258921683 0.09281468158587813 current epoch 543: saving best model...\n",
      "0.09171284036710858 0.09225540258921683 current epoch 544: saving best model...\n",
      "0.0911471489816904 0.09171284036710858 current epoch 545: saving best model...\n",
      "0.09054248128086329 0.0911471489816904 current epoch 546: saving best model...\n",
      "0.08991404273547232 0.09054248128086329 current epoch 547: saving best model...\n",
      "0.08930368744768202 0.08991404273547232 current epoch 548: saving best model...\n",
      "0.08875128929503262 0.08930368744768202 current epoch 549: saving best model...\n",
      "0.08820665744133294 0.08875128929503262 current epoch 550: saving best model...\n",
      "0.08766444004140794 0.08820665744133294 current epoch 551: saving best model...\n",
      "0.08712197933346033 0.08766444004140794 current epoch 552: saving best model...\n",
      "0.08659961400553584 0.08712197933346033 current epoch 553: saving best model...\n",
      "0.08610109821893275 0.08659961400553584 current epoch 554: saving best model...\n",
      "0.08560798200778663 0.08610109821893275 current epoch 555: saving best model...\n",
      "0.08509234129451215 0.08560798200778663 current epoch 556: saving best model...\n",
      "0.08455201727338135 0.08509234129451215 current epoch 557: saving best model...\n",
      "0.08401928469538689 0.08455201727338135 current epoch 558: saving best model...\n",
      "0.08352879504673183 0.08401928469538689 current epoch 559: saving best model...\n",
      "0.08313650940544903 0.08352879504673183 current epoch 560: saving best model...\n",
      "0.08284120494499803 0.08313650940544903 current epoch 561: saving best model...\n",
      "0.08251644694246352 0.08284120494499803 current epoch 562: saving best model...\n",
      "0.0820798974018544 0.08251644694246352 current epoch 563: saving best model...\n",
      "0.0815275115892291 0.0820798974018544 current epoch 564: saving best model...\n",
      "0.08101613819599152 0.0815275115892291 current epoch 565: saving best model...\n",
      "0.08082151412963867 0.08101613819599152 current epoch 566: saving best model...\n",
      "Epoch: 600, loss after minibatch     1: 0.02791\n",
      "Epoch: 600, loss after minibatch     2: 0.04504\n",
      "Epoch: 600, loss after minibatch     3: 0.04811\n",
      "Epoch: 600, loss after minibatch     4: 0.03245\n",
      "Epoch: 600, loss after minibatch     5: 0.04808\n",
      "Epoch: 600, loss after minibatch     6: 0.03503\n",
      "Epoch: 600, loss after minibatch     7: 0.04271\n",
      "Epoch: 600, loss after minibatch     8: 0.03036\n",
      "Epoch: 600, loss after minibatch     9: 0.02496\n",
      "Epoch: 600, loss after minibatch    10: 0.01609\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "|     Tag     |      Precision      |       Recall       |        FBeta        |\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "|  publisher  |         1.0         |        1.0         |         1.0         |\n",
      "|    punct    | 0.05979470246213481 |        1.0         | 0.11284204822541317 |\n",
      "|    editor   |         1.0         |        1.0         |         1.0         |\n",
      "|    pages    |         1.0         | 0.9912280701754386 |  0.9955947136563876 |\n",
      "|     note    |  0.6666666666666666 |        1.0         |         0.8         |\n",
      "|   journal   |         1.0         | 0.9928571428571429 |  0.996415770609319  |\n",
      "|   location  |  0.9473684210526315 |        1.0         |  0.972972972972973  |\n",
      "|    other    |         1.0         |        0.95        |  0.9743589743589743 |\n",
      "|    author   |  0.991304347826087  |        1.0         |  0.9956331877729258 |\n",
      "|     date    |  0.9836065573770492 |        1.0         |  0.9917355371900827 |\n",
      "|    volume   |  0.9841269841269841 |        1.0         |  0.9919999999999999 |\n",
      "|  booktitle  |         1.0         | 0.9904761904761905 |  0.9952153110047847 |\n",
      "| institution |  0.9545454545454546 |        1.0         |  0.9767441860465117 |\n",
      "|     tech    |         1.0         |        1.0         |         1.0         |\n",
      "|    title    |         1.0         | 0.9977324263038548 |  0.9988649262202043 |\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "0.0804627223405987 0.08082151412963867 current epoch 625: saving best model...\n",
      "0.07933527533896267 0.0804627223405987 current epoch 626: saving best model...\n",
      "0.078230204526335 0.07933527533896267 current epoch 627: saving best model...\n",
      "0.07723623095080256 0.078230204526335 current epoch 628: saving best model...\n",
      "0.07630877220071852 0.07723623095080256 current epoch 629: saving best model...\n",
      "0.0754316090606153 0.07630877220071852 current epoch 630: saving best model...\n",
      "0.07460364582948387 0.0754316090606153 current epoch 631: saving best model...\n",
      "0.07381416414864361 0.07460364582948387 current epoch 632: saving best model...\n",
      "0.07306206715293229 0.07381416414864361 current epoch 633: saving best model...\n",
      "0.07234161207452416 0.07306206715293229 current epoch 634: saving best model...\n",
      "0.07165089924819767 0.07234161207452416 current epoch 635: saving best model...\n",
      "0.07098670164123178 0.07165089924819767 current epoch 636: saving best model...\n",
      "0.07034678990021348 0.07098670164123178 current epoch 637: saving best model...\n",
      "0.06972928252071142 0.07034678990021348 current epoch 638: saving best model...\n",
      "0.06913231126964092 0.06972928252071142 current epoch 639: saving best model...\n",
      "0.06855434481985867 0.06913231126964092 current epoch 640: saving best model...\n",
      "0.06799386022612453 0.06855434481985867 current epoch 641: saving best model...\n",
      "0.06744971638545394 0.06799386022612453 current epoch 642: saving best model...\n",
      "0.06692076451145113 0.06744971638545394 current epoch 643: saving best model...\n",
      "0.06640618853271008 0.06692076451145113 current epoch 644: saving best model...\n",
      "0.06590530811809003 0.06640618853271008 current epoch 645: saving best model...\n",
      "0.06541720102541149 0.06590530811809003 current epoch 646: saving best model...\n",
      "0.06494103581644595 0.06541720102541149 current epoch 647: saving best model...\n",
      "0.06447600712999701 0.06494103581644595 current epoch 648: saving best model...\n",
      "0.0640213880687952 0.06447600712999701 current epoch 649: saving best model...\n",
      "0.06357658142223954 0.0640213880687952 current epoch 650: saving best model...\n",
      "0.06314073945395648 0.06357658142223954 current epoch 651: saving best model...\n",
      "0.06271215458400548 0.06314073945395648 current epoch 652: saving best model...\n",
      "0.06228103395551443 0.06271215458400548 current epoch 653: saving best model...\n",
      "0.06186135020107031 0.06228103395551443 current epoch 654: saving best model...\n",
      "0.06144821015186608 0.06186135020107031 current epoch 655: saving best model...\n",
      "0.06105308420956135 0.06144821015186608 current epoch 656: saving best model...\n",
      "0.06066436367109418 0.06105308420956135 current epoch 657: saving best model...\n",
      "0.060282676480710506 0.06066436367109418 current epoch 658: saving best model...\n",
      "0.05990784824825823 0.060282676480710506 current epoch 659: saving best model...\n",
      "0.059538691537454724 0.05990784824825823 current epoch 660: saving best model...\n",
      "0.059175341157242656 0.059538691537454724 current epoch 661: saving best model...\n",
      "0.058817194076254964 0.059175341157242656 current epoch 662: saving best model...\n",
      "0.05846390384249389 0.058817194076254964 current epoch 663: saving best model...\n",
      "0.058115142630413175 0.05846390384249389 current epoch 664: saving best model...\n",
      "0.05777047900483012 0.058115142630413175 current epoch 665: saving best model...\n",
      "0.05742952995933592 0.05777047900483012 current epoch 666: saving best model...\n",
      "0.057091596303507686 0.05742952995933592 current epoch 667: saving best model...\n",
      "0.05675608478486538 0.057091596303507686 current epoch 668: saving best model...\n",
      "0.05642303102649748 0.05675608478486538 current epoch 669: saving best model...\n",
      "0.05609378474764526 0.05642303102649748 current epoch 670: saving best model...\n",
      "0.05576955247670412 0.05609378474764526 current epoch 671: saving best model...\n",
      "0.05545051395893097 0.05576955247670412 current epoch 672: saving best model...\n",
      "0.055136377923190594 0.05545051395893097 current epoch 673: saving best model...\n",
      "0.05482674902305007 0.055136377923190594 current epoch 674: saving best model...\n",
      "0.05452126590535045 0.05482674902305007 current epoch 675: saving best model...\n",
      "0.05421963846310973 0.05452126590535045 current epoch 676: saving best model...\n",
      "0.05392167600803077 0.05421963846310973 current epoch 677: saving best model...\n",
      "0.05362736061215401 0.05392167600803077 current epoch 678: saving best model...\n",
      "0.05333645991049707 0.05362736061215401 current epoch 679: saving best model...\n",
      "0.05304883304052055 0.05333645991049707 current epoch 680: saving best model...\n",
      "0.05276427837088704 0.05304883304052055 current epoch 681: saving best model...\n",
      "0.05248265853151679 0.05276427837088704 current epoch 682: saving best model...\n",
      "0.05220378772355616 0.05248265853151679 current epoch 683: saving best model...\n",
      "0.05192754417657852 0.05220378772355616 current epoch 684: saving best model...\n",
      "0.05165368365123868 0.05192754417657852 current epoch 685: saving best model...\n",
      "0.05138198647182435 0.05165368365123868 current epoch 686: saving best model...\n",
      "0.05111259513068944 0.05138198647182435 current epoch 687: saving best model...\n",
      "0.05084554001223296 0.05111259513068944 current epoch 688: saving best model...\n",
      "0.050580595270730555 0.05084554001223296 current epoch 689: saving best model...\n",
      "0.05031697801314294 0.050580595270730555 current epoch 690: saving best model...\n",
      "0.05005345563404262 0.05031697801314294 current epoch 691: saving best model...\n",
      "0.049789883429184556 0.05005345563404262 current epoch 692: saving best model...\n",
      "0.04953115724492818 0.049789883429184556 current epoch 693: saving best model...\n",
      "0.049275675672106445 0.04953115724492818 current epoch 694: saving best model...\n",
      "0.04902070190291852 0.049275675672106445 current epoch 695: saving best model...\n",
      "0.04876688972581178 0.04902070190291852 current epoch 696: saving best model...\n",
      "0.04851423762738705 0.04876688972581178 current epoch 697: saving best model...\n",
      "0.0482628766912967 0.04851423762738705 current epoch 698: saving best model...\n",
      "0.04801275045610964 0.0482628766912967 current epoch 699: saving best model...\n",
      "Epoch: 700, loss after minibatch     1: 0.00378\n",
      "Epoch: 700, loss after minibatch     2: 0.00508\n",
      "Epoch: 700, loss after minibatch     3: 0.00608\n",
      "Epoch: 700, loss after minibatch     4: 0.00481\n",
      "Epoch: 700, loss after minibatch     5: 0.00422\n",
      "Epoch: 700, loss after minibatch     6: 0.00403\n",
      "Epoch: 700, loss after minibatch     7: 0.00574\n",
      "Epoch: 700, loss after minibatch     8: 0.00649\n",
      "Epoch: 700, loss after minibatch     9: 0.00572\n",
      "Epoch: 700, loss after minibatch    10: 0.00181\n",
      "+-------------+---------------------+--------+---------------------+\n",
      "|     Tag     |      Precision      | Recall |        FBeta        |\n",
      "+-------------+---------------------+--------+---------------------+\n",
      "|  publisher  |         1.0         |  1.0   |         1.0         |\n",
      "|    punct    | 0.05979470246213481 |  1.0   | 0.11284204822541317 |\n",
      "|    editor   |         1.0         |  1.0   |         1.0         |\n",
      "|    pages    |         1.0         |  1.0   |         1.0         |\n",
      "|     note    |         1.0         |  1.0   |         1.0         |\n",
      "|   journal   |         1.0         |  1.0   |         1.0         |\n",
      "|   location  |         1.0         |  1.0   |         1.0         |\n",
      "|    other    |         1.0         |  1.0   |         1.0         |\n",
      "|    author   |         1.0         |  1.0   |         1.0         |\n",
      "|     date    |         1.0         |  1.0   |         1.0         |\n",
      "|    volume   |         1.0         |  1.0   |         1.0         |\n",
      "|  booktitle  |         1.0         |  1.0   |         1.0         |\n",
      "| institution |         1.0         |  1.0   |         1.0         |\n",
      "|     tech    |         1.0         |  1.0   |         1.0         |\n",
      "|    title    |         1.0         |  1.0   |         1.0         |\n",
      "+-------------+---------------------+--------+---------------------+\n",
      "0.04776360956020653 0.04801275045610964 current epoch 700: saving best model...\n",
      "0.04751538473647088 0.04776360956020653 current epoch 701: saving best model...\n",
      "0.04726802697405219 0.04751538473647088 current epoch 702: saving best model...\n",
      "0.047021337086334825 0.04726802697405219 current epoch 703: saving best model...\n",
      "0.04677513358183205 0.047021337086334825 current epoch 704: saving best model...\n",
      "0.04652947373688221 0.04677513358183205 current epoch 705: saving best model...\n",
      "0.046284300507977605 0.04652947373688221 current epoch 706: saving best model...\n",
      "0.04603966337163001 0.046284300507977605 current epoch 707: saving best model...\n",
      "0.04579589166678488 0.04603966337163001 current epoch 708: saving best model...\n",
      "0.04555316467303783 0.04579589166678488 current epoch 709: saving best model...\n",
      "0.04531162034254521 0.04555316467303783 current epoch 710: saving best model...\n",
      "0.04507167334668338 0.04531162034254521 current epoch 711: saving best model...\n",
      "0.044833624619059265 0.04507167334668338 current epoch 712: saving best model...\n",
      "0.04459772142581642 0.044833624619059265 current epoch 713: saving best model...\n",
      "0.044364006258547306 0.04459772142581642 current epoch 714: saving best model...\n",
      "0.04413167433813214 0.044364006258547306 current epoch 715: saving best model...\n",
      "0.043895329465158284 0.04413167433813214 current epoch 716: saving best model...\n",
      "0.04366274725180119 0.043895329465158284 current epoch 717: saving best model...\n",
      "0.04343692061956972 0.04366274725180119 current epoch 718: saving best model...\n",
      "0.04321454605087638 0.04343692061956972 current epoch 719: saving best model...\n",
      "0.04299260932020843 0.04321454605087638 current epoch 720: saving best model...\n",
      "0.04277240938972682 0.04299260932020843 current epoch 721: saving best model...\n",
      "0.04255272203590721 0.04277240938972682 current epoch 722: saving best model...\n",
      "0.042334143654443324 0.04255272203590721 current epoch 723: saving best model...\n",
      "0.042115836520679295 0.042334143654443324 current epoch 724: saving best model...\n",
      "0.041898438590578735 0.042115836520679295 current epoch 725: saving best model...\n",
      "0.04167874553240836 0.041898438590578735 current epoch 726: saving best model...\n",
      "0.04146305553149432 0.04167874553240836 current epoch 727: saving best model...\n",
      "0.04125061817467213 0.04146305553149432 current epoch 728: saving best model...\n",
      "0.04104020004160702 0.04125061817467213 current epoch 729: saving best model...\n",
      "0.04083178588189185 0.04104020004160702 current epoch 730: saving best model...\n",
      "0.04062495206017047 0.04083178588189185 current epoch 731: saving best model...\n",
      "0.04041906539350748 0.04062495206017047 current epoch 732: saving best model...\n",
      "0.040214405162259936 0.04041906539350748 current epoch 733: saving best model...\n",
      "0.04001110827084631 0.040214405162259936 current epoch 734: saving best model...\n",
      "0.03980923641938716 0.04001110827084631 current epoch 735: saving best model...\n",
      "0.03960886166896671 0.03980923641938716 current epoch 736: saving best model...\n",
      "0.039409986580722034 0.03960886166896671 current epoch 737: saving best model...\n",
      "0.0392124637728557 0.039409986580722034 current epoch 738: saving best model...\n",
      "0.03901612549088895 0.0392124637728557 current epoch 739: saving best model...\n",
      "0.03882113203871995 0.03901612549088895 current epoch 740: saving best model...\n",
      "0.03862792591098696 0.03882113203871995 current epoch 741: saving best model...\n",
      "0.038437042268924415 0.03862792591098696 current epoch 742: saving best model...\n",
      "0.03824829205404967 0.038437042268924415 current epoch 743: saving best model...\n",
      "0.03806128888390958 0.03824829205404967 current epoch 744: saving best model...\n",
      "0.037875829031690955 0.03806128888390958 current epoch 745: saving best model...\n",
      "0.03769176744390279 0.037875829031690955 current epoch 746: saving best model...\n",
      "0.03750890539959073 0.03769176744390279 current epoch 747: saving best model...\n",
      "0.037327066413126886 0.03750890539959073 current epoch 748: saving best model...\n",
      "0.03714612766634673 0.037327066413126886 current epoch 749: saving best model...\n",
      "0.03696595737710595 0.03714612766634673 current epoch 750: saving best model...\n",
      "0.036786328884772956 0.03696595737710595 current epoch 751: saving best model...\n",
      "0.03660709969699383 0.036786328884772956 current epoch 752: saving best model...\n",
      "0.036427817423827946 0.03660709969699383 current epoch 753: saving best model...\n",
      "0.036248474614694715 0.036427817423827946 current epoch 754: saving best model...\n",
      "0.0360689798835665 0.036248474614694715 current epoch 755: saving best model...\n",
      "0.035890181199647486 0.0360689798835665 current epoch 756: saving best model...\n",
      "0.03571278927847743 0.035890181199647486 current epoch 757: saving best model...\n",
      "0.0355215115705505 0.03571278927847743 current epoch 758: saving best model...\n",
      "0.03530656616203487 0.0355215115705505 current epoch 759: saving best model...\n",
      "0.03514213627204299 0.03530656616203487 current epoch 760: saving best model...\n",
      "0.03496528952382505 0.03514213627204299 current epoch 761: saving best model...\n",
      "0.03477875026874244 0.03496528952382505 current epoch 762: saving best model...\n",
      "0.03460325067862868 0.03477875026874244 current epoch 763: saving best model...\n",
      "0.034436541609466076 0.03460325067862868 current epoch 764: saving best model...\n",
      "0.03426502016372979 0.034436541609466076 current epoch 765: saving best model...\n",
      "0.034089898806996644 0.03426502016372979 current epoch 766: saving best model...\n",
      "0.033913962193764746 0.034089898806996644 current epoch 767: saving best model...\n",
      "0.033736453391611576 0.033913962193764746 current epoch 768: saving best model...\n",
      "0.033564164536073804 0.033736453391611576 current epoch 769: saving best model...\n",
      "0.03338289889506996 0.033564164536073804 current epoch 770: saving best model...\n",
      "0.03321167186368257 0.03338289889506996 current epoch 771: saving best model...\n",
      "0.0330343134701252 0.03321167186368257 current epoch 772: saving best model...\n",
      "0.03286936157383025 0.0330343134701252 current epoch 773: saving best model...\n",
      "0.03269685490522534 0.03286936157383025 current epoch 774: saving best model...\n",
      "0.03253379394300282 0.03269685490522534 current epoch 775: saving best model...\n",
      "0.032365756342187524 0.03253379394300282 current epoch 776: saving best model...\n",
      "0.032215582905337214 0.032365756342187524 current epoch 777: saving best model...\n",
      "0.03205719811376184 0.032215582905337214 current epoch 778: saving best model...\n",
      "0.03190458402968943 0.03205719811376184 current epoch 779: saving best model...\n",
      "0.03174448374193162 0.03190458402968943 current epoch 780: saving best model...\n",
      "0.03158329601865262 0.03174448374193162 current epoch 781: saving best model...\n",
      "0.03141876892186701 0.03158329601865262 current epoch 782: saving best model...\n",
      "0.031266627833247185 0.03141876892186701 current epoch 783: saving best model...\n",
      "0.031117605976760387 0.031266627833247185 current epoch 784: saving best model...\n",
      "0.030969939893111587 0.031117605976760387 current epoch 785: saving best model...\n",
      "0.030821036896668375 0.030969939893111587 current epoch 786: saving best model...\n",
      "0.03066159028094262 0.030821036896668375 current epoch 787: saving best model...\n",
      "0.030498841195367277 0.03066159028094262 current epoch 788: saving best model...\n",
      "0.030337202828377485 0.030498841195367277 current epoch 789: saving best model...\n",
      "0.0301751111401245 0.030337202828377485 current epoch 790: saving best model...\n",
      "0.0300224645761773 0.0301751111401245 current epoch 791: saving best model...\n",
      "0.029868464451283216 0.0300224645761773 current epoch 792: saving best model...\n",
      "0.02972167287953198 0.029868464451283216 current epoch 793: saving best model...\n",
      "0.029570911661721766 0.02972167287953198 current epoch 794: saving best model...\n",
      "0.029433989548124373 0.029570911661721766 current epoch 795: saving best model...\n",
      "0.029284408665262163 0.029433989548124373 current epoch 796: saving best model...\n",
      "0.029142590472474694 0.029284408665262163 current epoch 797: saving best model...\n",
      "0.028991788160055876 0.029142590472474694 current epoch 798: saving best model...\n",
      "0.028855152893811464 0.028991788160055876 current epoch 799: saving best model...\n",
      "\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|     Tag     |     Precision      |       Recall       |       FBeta        |\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|  publisher  |        0.68        | 0.6107784431137725 | 0.6435331230283912 |\n",
      "|    punct    | 0.9986699916874481 | 0.9983380422137278 | 0.9985039893617023 |\n",
      "|    editor   | 0.7737226277372263 | 0.6162790697674418 | 0.6860841423948221 |\n",
      "|    pages    | 0.8965087281795511 | 0.9374185136897001 | 0.9165073295092415 |\n",
      "|     note    |        0.25        | 0.1891891891891892 | 0.2153846153846154 |\n",
      "|   journal   | 0.863932898415657  | 0.8528058877644894 | 0.8583333333333333 |\n",
      "|   location  | 0.7407407407407407 |        0.8         | 0.7692307692307692 |\n",
      "|    other    | 0.697429906542056  | 0.6909722222222222 | 0.694186046511628  |\n",
      "|    author   | 0.9515738498789347 | 0.9564952844539093 | 0.9540282203004097 |\n",
      "|     date    | 0.9276923076923077 |  0.93343653250774  | 0.9305555555555556 |\n",
      "|    volume   | 0.8652173913043478 | 0.8652173913043478 | 0.8652173913043477 |\n",
      "|  booktitle  | 0.7838983050847458 | 0.7838983050847458 | 0.7838983050847457 |\n",
      "| institution | 0.6419753086419753 | 0.6933333333333334 | 0.6666666666666667 |\n",
      "|     tech    | 0.5789473684210527 | 0.6470588235294118 | 0.6111111111111113 |\n",
      "|    title    | 0.9426203379024546 | 0.9435226547543076 | 0.9430712804975284 |\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "Test F1 macro score: 0.7690874586183245\n",
      "Test F1 micro score: 0.9262429842961619\n",
      "[['0.61078', '0.00000', '0.01198', '0.01796', '0.00599', '0.06587', '0.04192', '0.07784', '0.01198', '0.01796', '0.00000', '0.11377', '0.00599', '0.00000', '0.01796'], ['0.00000', '0.99834', '0.00000', '0.00000', '0.00000', '0.00000', '0.00000', '0.00033', '0.00000', '0.00000', '0.00000', '0.00050', '0.00000', '0.00000', '0.00083'], ['0.00000', '0.00000', '0.61628', '0.00000', '0.00000', '0.00581', '0.01163', '0.10465', '0.18023', '0.01744', '0.00000', '0.04070', '0.00000', '0.00581', '0.01744'], ['0.00391', '0.00000', '0.00000', '0.93742', '0.00130', '0.00000', '0.00261', '0.02477', '0.00000', '0.00652', '0.02086', '0.00000', '0.00000', '0.00000', '0.00261'], ['0.00000', '0.00000', '0.00000', '0.00000', '0.18919', '0.00000', '0.00000', '0.56757', '0.00000', '0.00000', '0.00000', '0.00000', '0.05405', '0.05405', '0.13514'], ['0.00552', '0.00092', '0.00276', '0.00460', '0.00276', '0.85281', '0.00184', '0.03220', '0.01472', '0.00460', '0.00552', '0.03036', '0.00184', '0.00276', '0.03680'], ['0.05333', '0.00000', '0.02667', '0.00667', '0.00000', '0.00667', '0.80000', '0.04000', '0.00000', '0.00667', '0.00000', '0.01333', '0.04000', '0.00667', '0.00000'], ['0.01157', '0.00116', '0.01042', '0.03241', '0.01042', '0.02546', '0.00926', '0.69097', '0.07523', '0.01042', '0.02431', '0.03588', '0.01042', '0.00810', '0.04398'], ['0.00030', '0.00000', '0.00152', '0.00274', '0.00000', '0.00730', '0.00000', '0.01339', '0.95650', '0.00213', '0.00243', '0.00183', '0.00000', '0.00030', '0.01156'], ['0.00310', '0.00155', '0.00000', '0.00929', '0.00000', '0.00310', '0.00929', '0.00929', '0.00155', '0.93344', '0.00310', '0.00155', '0.00000', '0.00155', '0.02322'], ['0.00000', '0.00000', '0.00000', '0.04130', '0.00000', '0.00435', '0.00217', '0.06739', '0.00217', '0.00435', '0.86522', '0.00652', '0.00000', '0.00000', '0.00652'], ['0.01412', '0.00141', '0.01130', '0.01695', '0.00706', '0.03390', '0.01554', '0.05367', '0.00282', '0.00989', '0.00424', '0.78390', '0.00847', '0.00424', '0.03249'], ['0.04000', '0.00000', '0.00000', '0.00000', '0.00000', '0.05333', '0.02667', '0.06667', '0.00000', '0.00000', '0.00000', '0.02667', '0.69333', '0.06667', '0.02667'], ['0.00000', '0.00000', '0.00000', '0.00000', '0.02941', '0.11765', '0.00000', '0.07353', '0.00000', '0.01471', '0.01471', '0.04412', '0.01471', '0.64706', '0.04412'], ['0.00160', '0.00128', '0.00000', '0.00000', '0.00000', '0.01500', '0.00032', '0.00511', '0.01340', '0.00128', '0.00160', '0.01372', '0.00064', '0.00255', '0.94352']]\n",
      "[[, punct, punct] [5, other, other] [], punct, punct] [P., author, author] [J., author, author] [Steinhardt, author, author] [and, author, author] [F., author, author] [S., author, author] [Accetta, author, author] [,, punct, punct] [Phys, journal, journal] [., punct, punct] [Rev, journal, journal] [., punct, punct] [Lett, journal, journal] [., punct, punct] [64, volume, volume] [,, punct, punct] [2740, pages, pages] [(, punct, punct] [1990, date, date] [), punct, punct] [., punct, punct] \n",
      "[[, punct, punct] [29, other, other] [], punct, punct] [S., author, author] [M., author, author] [Salamon, author, author] [,, punct, punct] [Quaternionic, title, title] [structures, title, title] [and, title, title] [twistor, title, title] [spaces, title, title] [,, punct, punct] [in, booktitle, other] [Global, booktitle, booktitle] [Riemannian, booktitle, booktitle] [geometry, booktitle, booktitle] \n",
      "[[, punct, punct] [AxKo, other, other] [], punct, punct] [J, author, author] [., punct, punct] [Ax, author, author] [and, author, author] [S., author, author] [Kochen, author, author] [,, punct, punct] [``, title, title] [Diophantine, title, title] [problems, title, title] [over, title, title] [local, title, title] [rings, title, title] [I, title, title] [,, punct, punct] [``, title, title] [Amer, journal, journal] [., punct, punct] [J, journal, journal] [., punct, punct] [Math, journal, journal] [., punct, punct] [87, volume, volume] [(, punct, punct] [1965, date, date] [), punct, punct] [,, punct, punct] [605-630, pages, pages] [., punct, punct] \n",
      "[Danvy, author, author] [,, punct, punct] [O., author, author] [and, author, author] [A., author, author] [Filinski, author, author] [., punct, punct] [Representing, title, title] [control, title, title] [:, punct, punct] [A, title, title] [study, title, title] [of, title, title] [the, title, title] [CPS, title, title] [transformation, title, title] [., punct, punct] [Tech, tech, tech] [., punct, punct] [Rpt, tech, tech] [., punct, punct] [CIS-91-2, tech, other] [., punct, punct] [Kansas, institution, other] [State, institution, other] [University, institution, other] [,, punct, punct] [1991, date, date] [., punct, punct] \n",
      "[[, punct, punct] [11, other, other] [], punct, punct] [Keller, author, author] [G., other, author] [Stochastic, title, author] [stability, title, author] [in, title, booktitle] [some, title, title] [chaotic, title, title] [dynamical, title, title] [systems, title, title] [,, punct, punct] [Mh, journal, institution] [., punct, punct] [Math, journal, journal] [., punct, punct] [,, punct, punct] [94, volume, volume] [(, punct, punct] [1982, date, date] [), punct, punct] [,, punct, punct] [313-333, pages, pages] [., punct, punct] \n",
      "[[, punct, punct] [6, other, other] [], punct, punct] [Chen, author, author] [,, punct, punct] [J, author, author] [., punct, punct] [(, punct, punct] [1992, date, date] [), punct, punct] [., punct, punct] [Some, title, title] [results, title, title] [on, title, title] [2, title, title] [n-, title, title] [k, title, title] [fractional, title, title] [factorial, title, title] [designs, title, title] [and, title, title] [search, title, title] [for, title, title] [minimum, title, title] [aberration, title, title] [designs, title, title] [., punct, punct] [Ann, journal, journal] [., punct, punct] [Statist, journal, journal] [., punct, punct] [20, volume, volume] [2124, pages, other] [--, pages, pages] [2141, pages, pages] [., punct, punct] [MR1193330, other, other] \n",
      "[Yao, author, author] [,, punct, punct] [A, author, author] [., punct, punct] [(, punct, punct] [1982, date, date] [), punct, punct] [,, punct, punct] [Protocols, title, title] [for, title, title] [secure, title, title] [computations, title, title] [,, punct, punct] [in, other, other] [`, punct, punct] [FOCS, booktitle, booktitle] [1982, booktitle, booktitle] [:, punct, punct] [Proceedings, booktitle, booktitle] [of, booktitle, booktitle] [23rd, booktitle, booktitle] [Annual, booktitle, booktitle] [Symposium, booktitle, booktitle] [on, booktitle, booktitle] [Foundations, booktitle, booktitle] [of, booktitle, booktitle] [Computer, booktitle, booktitle] [Science, booktitle, booktitle] [', punct, punct] [,, punct, punct] [IEEE, other, other] [Computer, other, other] [Society, other, other] [,, punct, punct] [pp, pages, pages] [., punct, punct] [160, pages, pages] [--, pages, pages] [164, pages, pages] [., punct, punct] [doi:10.1109/SFCS.1982.38, other, other] \n",
      "[Barabasi, author, author] [,, punct, punct] [A., author, author] [L., author, author] [,, punct, punct] [&, punct, punct] [Albert, author, author] [,, punct, punct] [R., other, other] [(, punct, punct] [1999, date, date] [), punct, punct] [., punct, punct] [Emergence, title, title] [of, title, title] [scaling, title, title] [in, title, title] [random, title, title] [networks, title, title] [., punct, punct] [Science, journal, journal] [,, punct, punct] [286, volume, volume] [,, punct, punct] [509, pages, pages] [--, pages, pages] [512, pages, pages] [., punct, punct] \n",
      "[32, other, other] [., punct, punct] [S., author, author] [Capozziello, author, author] [,, punct, punct] [G., author, author] [Iovane, author, author] [,, punct, punct] [G., author, author] [Lambiase, author, title] [,, punct, punct] [and, author, author] [C., author, author] [Stornaiolo, author, author] [,, punct, punct] [Europhys, journal, journal] [., punct, punct] [Lett, journal, journal] [., punct, punct] [46, volume, volume] [(, punct, punct] [1999, date, date] [), punct, punct] [710, pages, pages] [., punct, punct] \n",
      "[[, punct, punct] [14, other, other] [], punct, punct] [,, punct, punct] [Formal, title, title] [languages, title, title] [and, title, title] [global, title, title] [cellular, title, title] [automaton, title, title] [behavior, title, title] [,, punct, punct] [In, other, booktitle] [Gutowitz, other, publisher] [[, punct, punct] [6, other, other] [], punct, punct] [., punct, punct] \n",
      "[[, punct, punct] [12, other, other] [], punct, punct] [K., author, author] [Bernlohr, author, author] [et, author, author] [al, author, author] [., punct, punct] [,, punct, punct] [Nucl, journal, journal] [., punct, punct] [Instr, journal, journal] [., punct, punct] [and, journal, journal] [Meth, journal, journal] [., punct, punct] [A, journal, journal] [369, volume, pages] [(, punct, punct] [1996, date, date] [), punct, punct] [293, pages, pages] [., punct, punct] \n",
      "[G., author, author] [Moore, author, author] [and, author, author] [N., author, author] [Seiberg, author, author] [,, punct, punct] [Phys.Lett, journal, journal] [., punct, punct] [B, journal, journal] [220, volume, volume] [(, punct, punct] [1989, date, date] [), punct, punct] [422, pages, pages] [;, punct, punct] \n",
      "[[, punct, punct] [10, other, other] [], punct, punct] [Conlon, author, author] [,, punct, punct] [T., author, author] [,, punct, punct] [Crane, author, author] [,, punct, punct] [M., author, other] [,, punct, punct] [Ruskin, author, author] [,, punct, punct] [H., author, other] [J., author, author] [,, punct, punct] [Physica, journal, journal] [A, journal, journal] [387, volume, volume] [(, punct, punct] [21, other, volume] [), punct, punct] [(, punct, punct] [2008, date, date] [), punct, punct] [5197-5204, pages, pages] [., punct, punct] \n",
      "[[, punct, punct] [G-N-N, other, other] [], punct, punct] [B., author, author] [Gidas, author, author] [,, punct, punct] [W., author, author] [-M, author, other] [., punct, punct] [Ni, author, author] [,, punct, punct] [and, author, author] [L., author, author] [Nirenberg, author, author] [,, punct, punct] [Symmetry, title, title] [of, title, title] [positive, title, title] [solutions, title, title] [of, title, title] [nonlinear, title, title] [elliptic, title, title] [equations, title, title] [in, title, title] [R, title, title] [n, title, title] [,, punct, punct] [Adv, journal, journal] [., punct, punct] [in, journal, other] [Math, journal, other] [., punct, punct] [Supplementary, journal, journal] [Stud, journal, journal] [., punct, punct] [7A, volume, other] [(, punct, punct] [1981, date, date] [), punct, punct] [pp, pages, pages] [369, pages, pages] [--, pages, pages] [402, pages, pages] \n",
      "[L., author, author] [G., author, author] [Valiant, author, author] [., punct, punct] [A, title, title] [Bridging, title, title] [Model, title, title] [for, title, title] [Parallel, title, title] [Computation, title, title] [., punct, punct] [Communications, journal, journal] [of, journal, journal] [the, journal, journal] [ACM, journal, journal] [,, punct, punct] [33, volume, volume] [(, punct, punct] [8, volume, other] [), punct, punct] [,, punct, punct] [103-111, pages, pages] [,, punct, punct] [1990, date, date] [., punct, punct] \n",
      "[Neistein, author, author] [E., author, author] [,, punct, punct] [Khochfar, author, author] [S., author, author] [,, punct, punct] [Dalla, author, author] [Vecchia, author, author] [C., author, author] [,, punct, punct] [Schaye, author, author] [S., author, author] [,, punct, punct] [2011, date, date] [b, date, date] [,, punct, punct] [preprint, tech, tech] [,, punct, punct] [arXiv:1109.4635, other, other] \n",
      "[[, punct, punct] [14, other, other] [], punct, punct] [Anteneodo, author, author] [C., author, author] [,, punct, punct] [Tsallis, author, author] [C., author, author] [,, punct, punct] [J, journal, journal] [., punct, punct] [Math, journal, journal] [., punct, punct] [Phys, journal, journal] [., punct, punct] [,, punct, punct] [44, volume, volume] [(, punct, punct] [2003, date, date] [), punct, punct] [5194, pages, pages] [;, punct, punct] \n",
      "[[, punct, punct] [30, other, other] [], punct, punct] [Siciak, author, author] [J., author, author] [,, punct, punct] [On, title, title] [some, title, title] [extremal, title, title] [functions, title, title] [and, title, title] [their, title, title] [applications, title, title] [in, title, title] [the, title, title] [theory, title, title] [of, title, title] [analytic, title, title] [functions, title, title] [of, title, title] [several, title, title] [complex, title, title] [variables, title, title] [,, punct, punct] [Trans, journal, journal] [., punct, punct] [Amer, journal, journal] [., punct, punct] [Math, journal, journal] [., punct, punct] [Soc, journal, journal] [., punct, punct] [,, punct, punct] [105, volume, volume] [(, punct, punct] [1962, date, date] [), punct, punct] [,, punct, punct] [322-357, pages, pages] [., punct, punct] \n",
      "[[, punct, punct] [7, other, other] [], punct, punct] [S., author, author] [Maslov, author, author] [,, punct, punct] [Y, author, author] [., punct, punct] [-C, author, author] [., punct, punct] [Zhang, author, author] [,, punct, punct] [International, journal, journal] [Journal, journal, journal] [of, journal, journal] [Theoretical, journal, journal] [and, journal, journal] [Applied, journal, journal] [Finance, journal, journal] [1, volume, volume] [,, punct, punct] [1998, date, date] [,, punct, punct] [377, pages, pages] [--, pages, pages] [387, pages, pages] \n",
      "[Vander, author, author] [Linden, author, author] [,, punct, punct] [K., author, author] [;, punct, punct] [Cumming, author, author] [,, punct, punct] [S., author, author] [;, punct, punct] [and, author, author] [Martin, author, author] [,, punct, punct] [J, author, author] [., punct, punct] [1992, date, date] [., punct, punct] [Expressing, title, title] [local, title, title] [rhetorical, title, title] [relations, title, title] [in, title, title] [instructional, title, title] [text, title, title] [., punct, punct] [Technical, tech, tech] [Report, tech, tech] [92-43, tech, tech] [,, punct, punct] [University, institution, institution] [of, institution, institution] [Colorado, institution, institution] [., punct, punct] [To, note, note] [appear, note, note] [in, note, other] [Computational, note, other] [Linguistics, note, tech] [., punct, punct] \n",
      "[Lyne, author, author] [A., author, author] [,, punct, punct] [Ritchings, author, author] [R., author, author] [,, punct, punct] [Smith, author, author] [F., author, author] [,, punct, punct] [1975, date, date] [,, punct, punct] [MNRAS, journal, journal] [,, punct, punct] [171, volume, volume] [,, punct, punct] [579, pages, pages] \n",
      "[[, punct, punct] [KK, other, other] [], punct, punct] [S-J, author, author] [., punct, punct] [Kang, author, author] [and, author, author] [M., author, author] [Kashiwara, author, author] [,, punct, punct] [Quantized, title, title] [affine, title, title] [algebras, title, title] [and, title, title] [crystals, title, title] [with, title, title] [core, title, title] [,, punct, punct] [Commun, journal, journal] [., punct, punct] [Math, journal, journal] [., punct, punct] [Phys, journal, journal] [., punct, punct] [195, volume, volume] [(, punct, punct] [1998, date, date] [), punct, punct] [725-740, pages, pages] [., punct, punct] \n",
      "[Reichman, author, author] [,, punct, punct] [R., other, author] [(, punct, punct] [1985, date, date] [), punct, punct] [., punct, punct] [Getting, title, title] [Computers, title, title] [to, title, title] [Talk, title, title] [Like, title, title] [You, title, title] [and, title, title] [Me, title, title] [:, punct, punct] [Discourse, title, title] [Context, title, title] [,, punct, punct] [Focus, title, title] [,, punct, punct] [and, title, title] [Semantics, title, title] [., punct, punct] [Cambridge, location, publisher] [,, punct, punct] [MA, location, location] [:, punct, punct] [MIT, publisher, publisher] [Press, publisher, publisher] [., punct, punct] \n",
      "[[, punct, punct] [16, other, other] [], punct, punct] [S., author, author] [Browne, author, author] [,, punct, punct] [W., author, author] [Whitt, author, author] [,, punct, punct] [Adv, journal, journal] [., punct, punct] [Appl, journal, journal] [., punct, punct] [Prob, journal, booktitle] [., punct, punct] [28, volume, volume] [,, punct, punct] [1996, date, date] [,, punct, punct] [1145, pages, pages] [--, pages, pages] [1176, pages, pages] \n",
      "[[, punct, punct] [34, other, other] [], punct, punct] [C., author, author] [J., author, author] [Skinner, author, author] [and, author, author] [M., author, author] [J., author, author] [Elliot, author, author] [,, punct, punct] [A, title, title] [measure, title, title] [of, title, title] [disclosure, title, title] [risk, title, title] [for, title, title] [microdata, title, title] [,, punct, punct] [J., journal, author] [Roy, journal, author] [., punct, punct] [Statist, journal, title] [., punct, punct] [Soc, journal, title] [., punct, punct] [B, journal, journal] [64, volume, volume] [:, punct, punct] [855, pages, pages] [--, pages, pages] [867, pages, pages] [,, punct, punct] [2002, date, date] [., punct, punct] [MR1979391, other, journal] \n",
      "[Rivest, author, author] [,, punct, punct] [R., author, author] [L., other, author] [(, punct, punct] [1987, date, date] [), punct, punct] [., punct, punct] [Learning, title, title] [decision, title, title] [lists, title, title] [., punct, punct] [Machine, booktitle, volume] [Learning, booktitle, booktitle] [,, punct, punct] [2, volume, volume] [(, punct, punct] [3, volume, other] [), punct, punct] [,, punct, punct] [229-246, pages, pages] [., punct, punct] \n",
      "[Sekine, author, author] [,, punct, punct] [T., author, other] [,, punct, punct] [(, punct, punct] [2001, date, date] [), punct, punct] [., punct, punct] [Modeling, title, title] [and, title, title] [Forecasting, title, title] [Inflation, title, title] [in, title, title] [Japan, title, title] [,, punct, punct] [IMF, tech, title] [Working, tech, tech] [Paper, tech, tech] [,, punct, punct] [WP/01/82, other, institution] \n",
      "[Kirkpatrick, author, author] [,, punct, punct] [S., author, author] [,, punct, punct] [Gelatt, author, author] [,, punct, punct] [C., author, other] [,, punct, punct] [&, punct, punct] [Vecchi, author, author] [,, punct, punct] [M., other, other] [(, punct, punct] [1983, date, date] [), punct, punct] [., punct, punct] [Optimization, title, title] [by, title, title] [simulated, title, title] [annealing, title, title] [., punct, punct] [Science, journal, journal] [,, punct, punct] [220, volume, volume] [(, punct, punct] [4598, volume, pages] [), punct, punct] [,, punct, punct] [671-680, pages, pages] [., punct, punct] \n",
      "[[, punct, punct] [Vo1, other, other] [], punct, punct] [V., author, author] [E., author, author] [Voskresenskii, author, author] [,, punct, punct] [R-equivalence, title, title] [questions, title, title] [on, title, title] [semisimple, title, title] [groups, title, title] [(, punct, punct] [in, other, title] [Rus-, other, title] [sian, other, title] [), punct, punct] [,, punct, punct] [Zap, booktitle, title] [., punct, punct] [Nauc, booktitle, journal] [., punct, punct] [Sem, booktitle, title] [., punct, punct] [Leningrad, publisher, author] [Otdel, publisher, journal] [Mat, publisher, journal] [., punct, punct] [Inst, publisher, journal] [., punct, punct] [Steklov, publisher, journal] [(, punct, punct] [LOMI, publisher, journal] [), punct, punct] [86, volume, volume] [(, punct, punct] [1979, date, date] [), punct, punct] [,, punct, punct] [49, pages, pages] [--, pages, pages] [65, pages, pages] [and, pages, pages] [189, pages, title] [--, pages, pages] [190, pages, date] [., punct, punct] \n",
      "[[, punct, punct] [14, other, other] [], punct, punct] [O'Brien, author, author] [W., author, author] [A., author, author] [,, punct, punct] [Grovit-Ferbas, author, author] [K., author, author] [,, punct, punct] [Namazi, author, author] [A., author, author] [,, punct, punct] [et, author, author] [al, author, author] [., punct, punct] [:, punct, punct] [Human, title, title] [immunodefi-, title, title] [ciency, title, title] [virus, title, title] [type, title, title] [1, title, title] [replication, title, title] [can, title, title] [be, title, title] [increased, title, title] [in, title, other] [peripheral, title, title] [blood, title, title] [of, title, title] [seropositive, title, title] [patients, title, title] [after, title, title] [influenza, title, title] [vaccination, title, title] [., punct, punct] [Blood, journal, journal] [1995, date, date] [,, punct, punct] [86, volume, volume] [:, punct, punct] [1082, pages, pages] [--, pages, pages] [1089, pages, pages] [., punct, punct] \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from sklearn_hierarchical_classification.classifier import HierarchicalClassifier\n",
    "from sklearn_hierarchical_classification.constants import ROOT\n",
    "from sklearn_hierarchical_classification.metrics import h_fbeta_score, multi_labeled\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix as cm\n",
    "from sklearn.model_selection import KFold\n",
    "from prettytable import PrettyTable\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "\n",
    "X_train, y_train = torch.tensor(X_train), torch.tensor(y_train)\n",
    "X_test, y_test = torch.tensor(X_test), torch.tensor(y_test)\n",
    "\n",
    "\n",
    "def categorical_accuracy(outputs, y, pad_index):\n",
    "    max_outputs = outputs.argmax(dim = 1, keepdim=True)\n",
    "    non_padded_elements = (y != pad_index).nonzero()\n",
    "    correct = max_outputs[non_padded_elements].squeeze(1).eq(y[non_padded_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_padded_elements].shape[0]])\n",
    "\n",
    "def get_max_outputs(outputs):\n",
    "    max_outputs = outputs.argmax(dim = -1)\n",
    "    return max_outputs\n",
    "\n",
    "def print_report(report):\n",
    "    table = PrettyTable(float_format=\"1.5f\")\n",
    "    table.field_names = [\"Tag\", \"Precision\", \"Recall\", \"FBeta\"]\n",
    "    for i in range(len(tag_arr)):\n",
    "      tag, scores = [tag_arr[i]], list(map(lambda metric: metric[i], report))[:-1] # exclude support metric\n",
    "      tag.extend(scores)\n",
    "      table.add_row(tag)\n",
    "    print(table)\n",
    "\n",
    "def print_statistics(X_test, y_test, y_pred, model):\n",
    "    macro_score = f1_score(y_test, y_pred, average='macro')\n",
    "    micro_score = f1_score(y_test, y_pred, average='micro')\n",
    "    cMtx = cm(y_test, y_pred)\n",
    "    normalized_cMtx = []\n",
    "    for row in cMtx:\n",
    "        total = sum(row)\n",
    "        if total != 0:\n",
    "            row = list(map(lambda value: \"{:.5f}\".format(value / total), row))\n",
    "        normalized_cMtx.append(row)\n",
    "    print('Test F1 macro score: {}'.format(macro_score))\n",
    "    print('Test F1 micro score: {}'.format(micro_score))\n",
    "    print(normalized_cMtx)\n",
    "\n",
    "def print_tags(y_pred, limit = 30):\n",
    "    for i, ref_pred in enumerate(y_pred):\n",
    "        if i == limit:\n",
    "            return\n",
    "        ref = ref_test[i]\n",
    "        output = \"\"\n",
    "        for index, token_tag in enumerate(ref):\n",
    "            token, tag = token_tag[0], token_tag[1]\n",
    "            output +=  (\"[\" + \", \".join([token, tag, tag_arr[ref_pred[index]]]) + \"] \")\n",
    "        print(output)\n",
    "\n",
    "\n",
    "# model_filename is used to save the model\n",
    "def train(train_dataset, model_filename=None):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256)\n",
    "    min_loss = 10.0\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            X_train, y_train = data\n",
    "            if i == len(train_loader)-1:\n",
    "                X_train_style = X_style[i*256:]\n",
    "            else:\n",
    "                X_train_style = X_style[i*256:(i+1)*256]\n",
    "            \n",
    "            outputs = model.forward(X_train, X_train_style)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = outputs.view(-1, outputs.shape[-1]) # [batch_size * seq_len, output_dim]\n",
    "            y_train = y_train.view(-1) # [batch_size * seq_len]\n",
    "            \n",
    "            # Get the loss function\n",
    "            loss = criterion(outputs, y_train.long())\n",
    "\n",
    "            # Calculate loss\n",
    "            loss.backward()\n",
    "            total_train_loss += loss.item()\n",
    "            # Backpropagation\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print loss at every 100th epoch\n",
    "            if epoch % 100 == 0:\n",
    "                print(\"Epoch: %d, loss after minibatch %5d: %1.5f\" % (epoch, i+1, loss.item()))\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            report = precision_recall_fscore_support(y_train.long(), \\\n",
    "                                                    get_max_outputs(outputs.detach()), \\\n",
    "                                                    average=None, \\\n",
    "                                                    zero_division=0, \\\n",
    "                                                    labels = [i for i in range(len(all_tags))])\n",
    "            print_report(report)\n",
    "        if total_train_loss < min_loss:\n",
    "            print(total_train_loss, min_loss, 'current epoch {}: saving best model...'.format(epoch))\n",
    "            min_loss = total_train_loss\n",
    "            torch.save(model.state_dict(), './models/checkpoint.pt')\n",
    "\n",
    "        \n",
    "    model.load_state_dict(torch.load('./models/checkpoint.pt'))\n",
    "\n",
    "def test(model, X_test, y_test):\n",
    "    with torch.no_grad():\n",
    "        print()\n",
    "        outputs = model.forward(X_test, X_test_style)\n",
    "\n",
    "        outputs_squeezed = outputs.view(-1, outputs.shape[-1]) # [batch_size * seq_len, output_dim]\n",
    "        y_test_squeezed = y_test.view(-1) # [batch_size * seq_len]\n",
    "\n",
    "        # Get the loss function\n",
    "        loss = criterion(outputs_squeezed, y_test_squeezed.long())\n",
    "        \n",
    "        int_y_test = y_test.int().tolist()\n",
    "        output_probs = outputs\n",
    "        y_true, y_pred = [], []\n",
    "        for idx, row in enumerate(int_y_test):\n",
    "            padding_idx = row.index(len(all_tags))\n",
    "            y_true.extend(row[:padding_idx])\n",
    "            test_row = output_probs[idx][:padding_idx]\n",
    "            y_pred.extend(get_max_outputs(test_row))\n",
    "\n",
    "        report = precision_recall_fscore_support(y_true, \\\n",
    "                                                y_pred, \\\n",
    "                                                average=None, \\\n",
    "                                                zero_division=0, \\\n",
    "                                                labels = [i for i in range(len(all_tags))])\n",
    "        print_report(report)  \n",
    "        print_statistics(X_test, y_true, y_pred, model)\n",
    "        print_tags(get_max_outputs(outputs.detach()))\n",
    "training_set = TensorDataset(X_train, y_train)\n",
    "train(training_set)\n",
    "model.load_state_dict(torch.load('./models/checkpoint.pt'))\n",
    "model_filename = \"./models/swe_we_cstyle_ner_6gramwords.pt\" # change accordingly based on features being run\n",
    "\n",
    "torch.save(model.state_dict(), str(model_filename))\n",
    "test(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "6e61ca267e818e5d9a430b77a02997073c0a797ef0ae9dc5b9fbd2a4d1cc9a76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}