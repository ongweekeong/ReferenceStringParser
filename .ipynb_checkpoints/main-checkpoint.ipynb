{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install torch\n",
    "!pip install fasttext\n",
    "!pip install sklearn-hierarchical-classification\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ongwe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Preprocess Data\n",
    "'''\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import fasttext\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk import word_tokenize\n",
    "\n",
    "OTHER_TAG = \"other\"\n",
    "PUNCT_TAG = \"punct\"\n",
    "\n",
    "with open('./utils/tags.txt', encoding=\"utf-8\", errors='ignore') as f:\n",
    "    tags = set([str.rstrip(tag) for tag in f.readlines()])\n",
    "\n",
    "with open('./utils/tags_hierarchy.txt', encoding=\"utf-8\", errors='ignore') as f:\n",
    "    hierarchy_tags = set([str.rstrip(tag) for tag in f.readlines()])\n",
    "\n",
    "def remove_labels(text):\n",
    "    return re.sub(r'\\<\\/?[\\w-]*\\>\\s*', \"\", text).strip()\n",
    "\n",
    "def tag_token(token, tag):\n",
    "    if token in string.punctuation:\n",
    "        return (token, PUNCT_TAG)\n",
    "    return (token, tag)\n",
    "\n",
    "def get_tagged_tokens(ref, groups):\n",
    "    tagged_tokens = []\n",
    "    relevant_groups = list(filter(lambda group: group[1] in tags, groups))\n",
    "    tag_dict = dict()\n",
    "    for group in relevant_groups:\n",
    "        text = remove_labels(group[0])\n",
    "        tokens = word_tokenize(text)\n",
    "        for token in tokens:\n",
    "            if token not in tag_dict:\n",
    "                tag_dict[token] = [group[1]]\n",
    "            else:\n",
    "                tag_dict[token].append(group[1])\n",
    "    tokenized_ref = word_tokenize(remove_labels(ref))\n",
    "    tagged_tokens = []\n",
    "    for token in tokenized_ref:\n",
    "        if token in tag_dict and tag_dict[token]: # still has a tag\n",
    "            tag = tag_dict[token][0]\n",
    "            tag_dict[token].pop(0)\n",
    "        else:\n",
    "            tag = OTHER_TAG\n",
    "        tagged_tokens.append(tag_token(token, tag))\n",
    "    return tagged_tokens\n",
    "\n",
    "def find_groups(text):\n",
    "    groups = re.findall(r'(\\<(.*)\\>.*\\<\\/\\2\\>)', text) # group: (<tag> ... <tag>, tag)\n",
    "    if not groups:\n",
    "        return []\n",
    "    combined = []\n",
    "    for group in groups:\n",
    "        new_group = re.sub(r'\\<\\/?'+ group[1] + '\\>\\s*', \"\", group[0]).strip()\n",
    "        combined.append(group)\n",
    "        combined.extend(find_groups(new_group))\n",
    "    return combined\n",
    "\n",
    "''' Attach tags to each token '''\n",
    "def attach_tags(dataset_path):\n",
    "    dataset = []\n",
    "    with open(dataset_path, encoding=\"utf-8\", errors='ignore') as f:\n",
    "        refs = f.readlines()\n",
    "        for ref in refs:\n",
    "            groups = find_groups(ref)\n",
    "            # groups = re.findall(r'(\\<(.*)\\>.*\\<\\/\\2\\>)', ref) # format (<tag>...</tag>, tag)\n",
    "            tagged_tokens = get_tagged_tokens(ref, groups)\n",
    "            dataset.append(tagged_tokens)\n",
    "    return dataset\n",
    "\n",
    "''' Removes labels and tokenizes '''\n",
    "def tokenize_dataset(dataset_path, sep=\" \"):\n",
    "    dataset = []\n",
    "    with open(dataset_path, encoding=\"utf-8\", errors='ignore') as f:\n",
    "        refs = f.readlines()\n",
    "        for ref in refs:\n",
    "            ref = remove_labels(ref) \n",
    "            tokenized = sep.join(word_tokenize(ref))\n",
    "            dataset.append(tokenized)\n",
    "    return dataset\n",
    "\n",
    "def map_to_index(keys, idx_start=0):\n",
    "    key_to_idx, keys_arr, idx = {}, [], idx_start\n",
    "    for key in keys:\n",
    "        key_to_idx[key] = idx\n",
    "        keys_arr.append(key)\n",
    "        idx += 1\n",
    "    return key_to_idx, keys_arr\n",
    "\n",
    "all_tags = tags \n",
    "all_tags.add(OTHER_TAG)\n",
    "all_tags.add(PUNCT_TAG)\n",
    "tag_to_idx, tag_arr = map_to_index(all_tags)\n",
    "\n",
    "dataset_path = './dataset/standardized_dataset.txt'\n",
    "dataset = attach_tags(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "def train_word_embedding_model(dataset_paths, embedding_dim, use_subwords=False, use_hierarchy=False):\n",
    "    embedding_dataset_path = './dataset/word_embedding_dataset.txt'\n",
    "    hierarchy_dataset_path = './dataset/umass-citation/training'\n",
    "\n",
    "    word_embedding_dataset = []\n",
    "    for dataset_path in dataset_paths:\n",
    "      word_embedding_dataset.extend(tokenize_dataset(dataset_path, sep=\" \"))\n",
    "    with open(embedding_dataset_path, 'w', errors='ignore') as f:\n",
    "        # fasttext tokenizes by whitespaces\n",
    "        f.write(\"\\n\".join(word_embedding_dataset))\n",
    "    if use_subwords:\n",
    "      model_path = './models/subword_embedding.bin'\n",
    "      model = fasttext.train_unsupervised(embedding_dataset_path, dim = embedding_dim, minn = 3, maxn = 6, wordNgrams=4) \n",
    "    elif use_hierarchy:\n",
    "        model_path = './models/hierarchy_word_embedding.bin'\n",
    "        model = fasttext.train_unsupervised(embedding_dataset_path, dim=embedding_dim, maxn=0)\n",
    "    else:\n",
    "      model_path = './models/word_embedding.bin'\n",
    "      model = fasttext.train_unsupervised(embedding_dataset_path, dim=embedding_dim, maxn=0)\n",
    "    model.save_model(model_path)\n",
    "    return model\n",
    "\n",
    "we_dataset_dir_path = \"./dataset/cstyle_dataset\"\n",
    "we_dataset_paths = [join(we_dataset_dir_path, f) for f in listdir(we_dataset_dir_path) if isfile(join(we_dataset_dir_path, f))]\n",
    "\n",
    "''' 1. Word Embeddings: Without Pretrained Word Embeddings '''\n",
    "# WE_model = train_word_embedding_model(we_dataset_paths, embedding_dim = EMBEDDING_DIM)\n",
    "\n",
    "''' 2. Subword Embeddings: Without Pretrained Subword Embeddings '''\n",
    "SWE_model = train_word_embedding_model(we_dataset_paths, embedding_dim = EMBEDDING_DIM, use_subwords = True)\n",
    "\n",
    "curr_WE_model = SWE_model # Change accordingly\n",
    "\n",
    "def get_word_vector(token):\n",
    "    return curr_WE_model.get_word_vector(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Named Entity Recognition'''\n",
    "import spacy\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "ner_dict = {\n",
    "        'ORG': 0,\n",
    "        \"NORP\": 1,\n",
    "        \"GPE\": 2,\n",
    "        \"PERSON\": 3,\n",
    "        \"LANGUAGE\": 4,\n",
    "        \"DATE\": 5,\n",
    "        \"TIME\": 6,\n",
    "        \"PRODUCT\": 7,\n",
    "        \"EVENT\": 8,\n",
    "        \"ORDINAL\": 9\n",
    "}\n",
    "\n",
    "# text should be tokenized and joined together by whitespaces\n",
    "def generate_ner_features(text):\n",
    "    doc = spacy_nlp(text)\n",
    "    entities = doc.ents\n",
    "    default_feature = [0 for _ in range(len(ner_dict.keys()) + 1)]\n",
    "    default_feature[-1] = 1 \n",
    "    entity_to_label = defaultdict(lambda: default_feature)\n",
    "    for entity in entities:\n",
    "        entity_tokens = entity.text.split(\" \")\n",
    "        label = entity.label_\n",
    "        if label in ner_dict:\n",
    "            features = [0 for _ in range(len(ner_dict.keys()) + 1)]\n",
    "            features[ner_dict[label]] = 1\n",
    "            for token in entity_tokens:\n",
    "                entity_to_label[token] = features\n",
    "    return entity_to_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "model_version = 'allenai/scibert_scivocab_cased'\n",
    "do_lower_case = False\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
    "\n",
    "def get_tokens_and_segments_tensors(text):\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segment_ids = [1] * len(indexed_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = torch.tensor([segment_ids])\n",
    "    return tokens_tensor, segments_tensor\n",
    "\n",
    "def average(embeddings):\n",
    "    if len(embeddings.size()) == 1: # if only one embedding, just return\n",
    "        return embeddings\n",
    "    averaged_embedding = np.array([0 for _ in range(len(embeddings[0]))])\n",
    "    for embedding in embeddings:\n",
    "        averaged_embedding = np.add(averaged_embedding, embedding)\n",
    "    return np.true_divide(averaged_embedding, len(embeddings))\n",
    "\n",
    "from transformers import BertForPreTraining, BertConfig \n",
    "\n",
    "config = BertConfig.from_json_file('./models/bert/fine_tuned_bert/config.json')\n",
    "bert_model = BertModel(config)\n",
    "bert_model.eval()\n",
    "\n",
    "def get_scibert_vector(text):\n",
    "    tokens_tensor, segments_tensor =get_tokens_and_segments_tensors(text)\n",
    "    outputs = bert_model(tokens_tensor, segments_tensor)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    token_embeddings = torch.squeeze(hidden_states[-1], dim=0).detach()\n",
    "    embeddings = dict()\n",
    "    tokens = text.split(\" \")\n",
    "    curr = 0\n",
    "    for token in tokens:\n",
    "        end = curr + len(tokenizer.tokenize(token))\n",
    "        start, end = curr, end\n",
    "        embeddings[token] = average(token_embeddings[start: end]).tolist()\n",
    "        curr = end\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(tokenize_dataset(dataset_path), columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "\n",
    "def token_transformer(sentence, add_noise=False):\n",
    "    sentence = re.sub(r\"\\b[A-Z]\\b\", 'L', sentence) #uppercase letters\n",
    "    sentence = re.sub(r\"\\b[a-z]\\b\", 'l', sentence) #lowercase letters\n",
    "    if add_noise:\n",
    "        sentence = re.sub(r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\b', random.choices(['M',''], weights=[3,1])[0], sentence)\n",
    "        sentence = re.sub(r'\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\b', random.choices(['m',''], weights=[3,1])[0], sentence)\n",
    "    else:\n",
    "        sentence = re.sub(r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\b', 'M', sentence) #random.choices(['M',''], weights=[3,1])[0], sentence)\n",
    "        sentence = re.sub(r'\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\b', 'm', sentence) #random.choices(['m',''], weights=[3,1])[0], sentence)\n",
    "    sentence = re.sub(r\"\\b[a-z][a-z]+\\b\", 'w', sentence) #lowercase words\n",
    "    sentence = re.sub(r\"\\b[A-Z][a-z]+\\b\", 'W', sentence) #uppercase words\n",
    "    sentence = re.sub(r'\\b(1\\d{3}|20[012]\\d)\\b', 'y', sentence)\n",
    "    sentence = re.sub(r\"\\b[0-9]+\\b\", 'n', sentence) #numbers\n",
    "    return sentence\n",
    "\n",
    "def token_punctuation(sentence, add_noise=False):\n",
    "    if add_noise:\n",
    "        words = sentence.split()\n",
    "        newSentence = []\n",
    "        for word in words: # for every word with punctuation, there is a 10% chance of omitting one type of punctuation.\n",
    "            newSentence.append(word.translate({ord(i):\" {} \".format(random.choices([i, ''], weights=[9,1])[0]) for i in ',.()[]:;\\'\\\"-'}))\n",
    "        sentence = \" \".join(newSentence)\n",
    "        return sentence\n",
    "    \n",
    "    else:\n",
    "        return sentence.translate({ord(i):\" {} \".format(i) for i in ',.()[]:;\\'\\\"-'})\n",
    "    \n",
    "\n",
    "''' Preprocess text column -- separate symbols with spaces to preserve punctuation as tokens'''\n",
    "def preprocess(df, add_noise=False): # text is a dataframe column\n",
    "    df.text = df.text.apply(token_punctuation, add_noise)\n",
    "    df.text = df.text.apply(token_transformer, add_noise)\n",
    "    return df\n",
    "df = preprocess(df, False) # boolean flag to add noise. For training purposes to be robust against noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, plot_confusion_matrix, confusion_matrix as cm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', TfidfVectorizer(lowercase=False, token_pattern=r\"\\S+\")), #r\"(?u)(\\b\\w+\\b|[\\.\\\"(),\\'\\[\\]:;])\")),\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('classifier', LogisticRegression(class_weight='balanced'))\n",
    "])\n",
    "params = {'features__ngram_range': [(1,3)],\n",
    "    'classifier__C': [0.001],\n",
    "         'classifier__max_iter': [500],}\n",
    "gs = GridSearchCV(pipeline, params, refit=True, cv=2, scoring='f1_macro', verbose=10)\n",
    "from joblib import load\n",
    "\n",
    "gs = load('models\\\\cstyle_LR_augmentedfeatures_3grams_noisyinput_noUnknown.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "categories = [['acm-sig-proceedings'], ['american-chemical-society'], ['apa'],\n",
    "       ['chicago-author-date'], ['harvard3'], ['ieee'], ['mla']]#, ['unknown']]\n",
    "enc.fit(categories)\n",
    "\n",
    "def style_to_feature(style, train=False):\n",
    "    local_df = pd.DataFrame()\n",
    "    fstyle = list(map(lambda s: [s], style))\n",
    "    if train:\n",
    "        enc.fit(fstyle)\n",
    "    y_transformed = enc.transform(fstyle).toarray()\n",
    "\n",
    "    return y_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_encoder(styles):\n",
    "    cat2vec = {}\n",
    "    vec = []\n",
    "    for i in range(len(categories)):\n",
    "        cat2vec[categories[i][0]] = [0 for j in range(len(categories))]\n",
    "        cat2vec[categories[i][0]][i] = 1\n",
    "    for style in styles:\n",
    "        vec.append(cat2vec[style])\n",
    "    return np.array(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "''' Get inputs and outputs for model '''\n",
    "ref_train, ref_test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "def get_x_y(refs, train=False):\n",
    "    X, y = [], []\n",
    "    X_style, y_style = [], []\n",
    "    idx = 0\n",
    "    for ref in refs:\n",
    "        X_ref, y_ref = [], []\n",
    "        joined_ref = \" \".join(list(map(lambda x: x[0], ref))) # concatenate tokens using whitespace\n",
    "        X_style.append(joined_ref)\n",
    "        ner_features = generate_ner_features(joined_ref)\n",
    "        scibert_features = get_scibert_vector(joined_ref)\n",
    "        style = style_encoder(gs.predict([joined_ref]))\n",
    "        for token, tag in ref:\n",
    "            features = get_word_vector(token)\n",
    "            features = np.hstack([features, np.array(ner_features[token]), np.array(scibert_features[token])])#, style[0]])\n",
    "            X_ref.append(features)\n",
    "            y_ref.append(tag_to_idx[tag])\n",
    "            \n",
    "        X.append(X_ref)\n",
    "        y.append(y_ref)\n",
    "        idx += 1\n",
    "    style = gs.predict(X_style)\n",
    "    feature_style = style_encoder(style)#style_to_feature(style, train)\n",
    "    y_style = []\n",
    "    idx = 0\n",
    "    for ref in refs:\n",
    "        sentence_style = []\n",
    "        feat = feature_style[idx]\n",
    "        for token, tag in ref:\n",
    "            sentence_style.append(feat)\n",
    "            features = np.concatenate((features, feature_style[idx]), axis=None)\n",
    "        y_style.append(sentence_style)\n",
    "        idx += 1\n",
    "    return X, y, X_style, y_style\n",
    "\n",
    "def add_padding(matrix, padding_value, max_length):\n",
    "    return pad_sequences(matrix, maxlen=max_length, padding='post', truncating='pre', value=padding_value, dtype='float32')\n",
    "\n",
    "X_train, y_train, X_sentence, X_style = get_x_y(ref_train, True)\n",
    "X_test, y_test, X_test_sentence, X_test_style = get_x_y(ref_test)\n",
    "\n",
    "padding_value = float(len(all_tags))\n",
    "max_length = max(map(lambda ref: len(ref), X_train + X_test))\n",
    "\n",
    "X_train = add_padding(X_train, padding_value, max_length)\n",
    "X_test = add_padding(X_test, padding_value, max_length)\n",
    "y_train = add_padding(y_train, padding_value, max_length)\n",
    "y_test = add_padding(y_test, padding_value, max_length)\n",
    "X_style = add_padding(X_style, padding_value, max_length)\n",
    "X_test_style = add_padding(X_test_style, padding_value, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5],\n",
       " [Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, '')])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n",
       "<svg height=\"373.44pt\" version=\"1.1\" viewBox=\"0 0 375.2875 373.44\" width=\"375.2875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       " <metadata>\r\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n",
       "   <cc:Work>\r\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n",
       "    <dc:date>2021-04-14T17:41:59.865479</dc:date>\r\n",
       "    <dc:format>image/svg+xml</dc:format>\r\n",
       "    <dc:creator>\r\n",
       "     <cc:Agent>\r\n",
       "      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n",
       "     </cc:Agent>\r\n",
       "    </dc:creator>\r\n",
       "   </cc:Work>\r\n",
       "  </rdf:RDF>\r\n",
       " </metadata>\r\n",
       " <defs>\r\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n",
       " </defs>\r\n",
       " <g id=\"figure_1\">\r\n",
       "  <g id=\"patch_1\">\r\n",
       "   <path d=\"M 0 373.44 \r\n",
       "L 375.2875 373.44 \r\n",
       "L 375.2875 0 \r\n",
       "L 0 0 \r\n",
       "z\r\n",
       "\" style=\"fill:none;\"/>\r\n",
       "  </g>\r\n",
       "  <g id=\"axes_1\">\r\n",
       "   <g id=\"patch_2\">\r\n",
       "    <path d=\"M 33.2875 224.64 \r\n",
       "L 368.0875 224.64 \r\n",
       "L 368.0875 7.2 \r\n",
       "L 33.2875 7.2 \r\n",
       "z\r\n",
       "\" style=\"fill:#ffffff;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_3\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 48.505682 224.64 \r\n",
       "L 72.854773 224.64 \r\n",
       "L 72.854773 66.058585 \r\n",
       "L 48.505682 66.058585 \r\n",
       "z\r\n",
       "\" style=\"fill:#1f77b4;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_4\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 72.854773 224.64 \r\n",
       "L 97.203864 224.64 \r\n",
       "L 97.203864 224.64 \r\n",
       "L 72.854773 224.64 \r\n",
       "z\r\n",
       "\" style=\"fill:#1f77b4;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_5\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 97.203864 224.64 \r\n",
       "L 121.552955 224.64 \r\n",
       "L 121.552955 17.554286 \r\n",
       "L 97.203864 17.554286 \r\n",
       "z\r\n",
       "\" style=\"fill:#1f77b4;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_6\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 121.552955 224.64 \r\n",
       "L 145.902045 224.64 \r\n",
       "L 145.902045 224.64 \r\n",
       "L 121.552955 224.64 \r\n",
       "z\r\n",
       "\" style=\"fill:#1f77b4;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_7\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 145.902045 224.64 \r\n",
       "L 170.251136 224.64 \r\n",
       "L 170.251136 224.64 \r\n",
       "L 145.902045 224.64 \r\n",
       "z\r\n",
       "\" style=\"fill:#1f77b4;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_8\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 170.251136 224.64 \r\n",
       "L 194.600227 224.64 \r\n",
       "L 194.600227 42.937365 \r\n",
       "L 170.251136 42.937365 \r\n",
       "z\r\n",
       "\" style=\"fill:#1f77b4;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_9\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 194.600227 224.64 \r\n",
       "L 218.949318 224.64 \r\n",
       "L 218.949318 224.64 \r\n",
       "L 194.600227 224.64 \r\n",
       "z\r\n",
       "\" style=\"fill:#1f77b4;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_10\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 218.949318 224.64 \r\n",
       "L 243.298409 224.64 \r\n",
       "L 243.298409 177.392288 \r\n",
       "L 218.949318 177.392288 \r\n",
       "z\r\n",
       "\" style=\"fill:#1f77b4;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_11\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 243.298409 224.64 \r\n",
       "L 267.6475 224.64 \r\n",
       "L 267.6475 224.64 \r\n",
       "L 243.298409 224.64 \r\n",
       "z\r\n",
       "\" style=\"fill:#1f77b4;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_12\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 267.6475 224.64 \r\n",
       "L 291.996591 224.64 \r\n",
       "L 291.996591 219.36233 \r\n",
       "L 267.6475 219.36233 \r\n",
       "z\r\n",
       "\" style=\"fill:#1f77b4;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_13\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 48.505682 224.64 \r\n",
       "L 78.942045 224.64 \r\n",
       "L 78.942045 181.162053 \r\n",
       "L 48.505682 181.162053 \r\n",
       "z\r\n",
       "\" style=\"fill:#ff7f0e;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_14\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 78.942045 224.64 \r\n",
       "L 109.378409 224.64 \r\n",
       "L 109.378409 224.64 \r\n",
       "L 78.942045 224.64 \r\n",
       "z\r\n",
       "\" style=\"fill:#ff7f0e;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_15\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 109.378409 224.64 \r\n",
       "L 139.814773 224.64 \r\n",
       "L 139.814773 178.900194 \r\n",
       "L 109.378409 178.900194 \r\n",
       "z\r\n",
       "\" style=\"fill:#ff7f0e;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_16\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 139.814773 224.64 \r\n",
       "L 170.251136 224.64 \r\n",
       "L 170.251136 224.64 \r\n",
       "L 139.814773 224.64 \r\n",
       "z\r\n",
       "\" style=\"fill:#ff7f0e;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_17\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 170.251136 224.64 \r\n",
       "L 200.6875 224.64 \r\n",
       "L 200.6875 178.397559 \r\n",
       "L 170.251136 178.397559 \r\n",
       "z\r\n",
       "\" style=\"fill:#ff7f0e;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_18\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 200.6875 224.64 \r\n",
       "L 231.123864 224.64 \r\n",
       "L 231.123864 224.64 \r\n",
       "L 200.6875 224.64 \r\n",
       "z\r\n",
       "\" style=\"fill:#ff7f0e;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_19\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 231.123864 224.64 \r\n",
       "L 261.560227 224.64 \r\n",
       "L 261.560227 211.068849 \r\n",
       "L 231.123864 211.068849 \r\n",
       "z\r\n",
       "\" style=\"fill:#ff7f0e;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_20\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 261.560227 224.64 \r\n",
       "L 291.996591 224.64 \r\n",
       "L 291.996591 224.64 \r\n",
       "L 261.560227 224.64 \r\n",
       "z\r\n",
       "\" style=\"fill:#ff7f0e;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_21\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 291.996591 224.64 \r\n",
       "L 322.432955 224.64 \r\n",
       "L 322.432955 223.886047 \r\n",
       "L 291.996591 223.886047 \r\n",
       "z\r\n",
       "\" style=\"fill:#ff7f0e;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_22\">\r\n",
       "    <path clip-path=\"url(#pb49a6a26ec)\" d=\"M 322.432955 224.64 \r\n",
       "L 352.869318 224.64 \r\n",
       "L 352.869318 224.388682 \r\n",
       "L 322.432955 224.388682 \r\n",
       "z\r\n",
       "\" style=\"fill:#ff7f0e;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_1\">\r\n",
       "    <g id=\"xtick_1\">\r\n",
       "     <g id=\"line2d_1\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 0 0 \r\n",
       "L 0 3.5 \r\n",
       "\" id=\"mf3ec877f29\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n",
       "      </defs>\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.505682\" xlink:href=\"#mf3ec877f29\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_1\">\r\n",
       "      <!-- american-chemical-society -->\r\n",
       "      <g transform=\"translate(51.265057 366.24)rotate(-90)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 34.28125 27.484375 \r\n",
       "Q 23.390625 27.484375 19.1875 25 \r\n",
       "Q 14.984375 22.515625 14.984375 16.5 \r\n",
       "Q 14.984375 11.71875 18.140625 8.90625 \r\n",
       "Q 21.296875 6.109375 26.703125 6.109375 \r\n",
       "Q 34.1875 6.109375 38.703125 11.40625 \r\n",
       "Q 43.21875 16.703125 43.21875 25.484375 \r\n",
       "L 43.21875 27.484375 \r\n",
       "z\r\n",
       "M 52.203125 31.203125 \r\n",
       "L 52.203125 0 \r\n",
       "L 43.21875 0 \r\n",
       "L 43.21875 8.296875 \r\n",
       "Q 40.140625 3.328125 35.546875 0.953125 \r\n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \r\n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \r\n",
       "Q 6 8.015625 6 15.921875 \r\n",
       "Q 6 25.140625 12.171875 29.828125 \r\n",
       "Q 18.359375 34.515625 30.609375 34.515625 \r\n",
       "L 43.21875 34.515625 \r\n",
       "L 43.21875 35.40625 \r\n",
       "Q 43.21875 41.609375 39.140625 45 \r\n",
       "Q 35.0625 48.390625 27.6875 48.390625 \r\n",
       "Q 23 48.390625 18.546875 47.265625 \r\n",
       "Q 14.109375 46.140625 10.015625 43.890625 \r\n",
       "L 10.015625 52.203125 \r\n",
       "Q 14.9375 54.109375 19.578125 55.046875 \r\n",
       "Q 24.21875 56 28.609375 56 \r\n",
       "Q 40.484375 56 46.34375 49.84375 \r\n",
       "Q 52.203125 43.703125 52.203125 31.203125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-97\"/>\r\n",
       "        <path d=\"M 52 44.1875 \r\n",
       "Q 55.375 50.25 60.0625 53.125 \r\n",
       "Q 64.75 56 71.09375 56 \r\n",
       "Q 79.640625 56 84.28125 50.015625 \r\n",
       "Q 88.921875 44.046875 88.921875 33.015625 \r\n",
       "L 88.921875 0 \r\n",
       "L 79.890625 0 \r\n",
       "L 79.890625 32.71875 \r\n",
       "Q 79.890625 40.578125 77.09375 44.375 \r\n",
       "Q 74.3125 48.1875 68.609375 48.1875 \r\n",
       "Q 61.625 48.1875 57.5625 43.546875 \r\n",
       "Q 53.515625 38.921875 53.515625 30.90625 \r\n",
       "L 53.515625 0 \r\n",
       "L 44.484375 0 \r\n",
       "L 44.484375 32.71875 \r\n",
       "Q 44.484375 40.625 41.703125 44.40625 \r\n",
       "Q 38.921875 48.1875 33.109375 48.1875 \r\n",
       "Q 26.21875 48.1875 22.15625 43.53125 \r\n",
       "Q 18.109375 38.875 18.109375 30.90625 \r\n",
       "L 18.109375 0 \r\n",
       "L 9.078125 0 \r\n",
       "L 9.078125 54.6875 \r\n",
       "L 18.109375 54.6875 \r\n",
       "L 18.109375 46.1875 \r\n",
       "Q 21.1875 51.21875 25.484375 53.609375 \r\n",
       "Q 29.78125 56 35.6875 56 \r\n",
       "Q 41.65625 56 45.828125 52.96875 \r\n",
       "Q 50 49.953125 52 44.1875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-109\"/>\r\n",
       "        <path d=\"M 56.203125 29.59375 \r\n",
       "L 56.203125 25.203125 \r\n",
       "L 14.890625 25.203125 \r\n",
       "Q 15.484375 15.921875 20.484375 11.0625 \r\n",
       "Q 25.484375 6.203125 34.421875 6.203125 \r\n",
       "Q 39.59375 6.203125 44.453125 7.46875 \r\n",
       "Q 49.3125 8.734375 54.109375 11.28125 \r\n",
       "L 54.109375 2.78125 \r\n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \r\n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \r\n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \r\n",
       "Q 5.515625 13.8125 5.515625 26.8125 \r\n",
       "Q 5.515625 40.234375 12.765625 48.109375 \r\n",
       "Q 20.015625 56 32.328125 56 \r\n",
       "Q 43.359375 56 49.78125 48.890625 \r\n",
       "Q 56.203125 41.796875 56.203125 29.59375 \r\n",
       "z\r\n",
       "M 47.21875 32.234375 \r\n",
       "Q 47.125 39.59375 43.09375 43.984375 \r\n",
       "Q 39.0625 48.390625 32.421875 48.390625 \r\n",
       "Q 24.90625 48.390625 20.390625 44.140625 \r\n",
       "Q 15.875 39.890625 15.1875 32.171875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-101\"/>\r\n",
       "        <path d=\"M 41.109375 46.296875 \r\n",
       "Q 39.59375 47.171875 37.8125 47.578125 \r\n",
       "Q 36.03125 48 33.890625 48 \r\n",
       "Q 26.265625 48 22.1875 43.046875 \r\n",
       "Q 18.109375 38.09375 18.109375 28.8125 \r\n",
       "L 18.109375 0 \r\n",
       "L 9.078125 0 \r\n",
       "L 9.078125 54.6875 \r\n",
       "L 18.109375 54.6875 \r\n",
       "L 18.109375 46.1875 \r\n",
       "Q 20.953125 51.171875 25.484375 53.578125 \r\n",
       "Q 30.03125 56 36.53125 56 \r\n",
       "Q 37.453125 56 38.578125 55.875 \r\n",
       "Q 39.703125 55.765625 41.0625 55.515625 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-114\"/>\r\n",
       "        <path d=\"M 9.421875 54.6875 \r\n",
       "L 18.40625 54.6875 \r\n",
       "L 18.40625 0 \r\n",
       "L 9.421875 0 \r\n",
       "z\r\n",
       "M 9.421875 75.984375 \r\n",
       "L 18.40625 75.984375 \r\n",
       "L 18.40625 64.59375 \r\n",
       "L 9.421875 64.59375 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-105\"/>\r\n",
       "        <path d=\"M 48.78125 52.59375 \r\n",
       "L 48.78125 44.1875 \r\n",
       "Q 44.96875 46.296875 41.140625 47.34375 \r\n",
       "Q 37.3125 48.390625 33.40625 48.390625 \r\n",
       "Q 24.65625 48.390625 19.8125 42.84375 \r\n",
       "Q 14.984375 37.3125 14.984375 27.296875 \r\n",
       "Q 14.984375 17.28125 19.8125 11.734375 \r\n",
       "Q 24.65625 6.203125 33.40625 6.203125 \r\n",
       "Q 37.3125 6.203125 41.140625 7.25 \r\n",
       "Q 44.96875 8.296875 48.78125 10.40625 \r\n",
       "L 48.78125 2.09375 \r\n",
       "Q 45.015625 0.34375 40.984375 -0.53125 \r\n",
       "Q 36.96875 -1.421875 32.421875 -1.421875 \r\n",
       "Q 20.0625 -1.421875 12.78125 6.34375 \r\n",
       "Q 5.515625 14.109375 5.515625 27.296875 \r\n",
       "Q 5.515625 40.671875 12.859375 48.328125 \r\n",
       "Q 20.21875 56 33.015625 56 \r\n",
       "Q 37.15625 56 41.109375 55.140625 \r\n",
       "Q 45.0625 54.296875 48.78125 52.59375 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-99\"/>\r\n",
       "        <path d=\"M 54.890625 33.015625 \r\n",
       "L 54.890625 0 \r\n",
       "L 45.90625 0 \r\n",
       "L 45.90625 32.71875 \r\n",
       "Q 45.90625 40.484375 42.875 44.328125 \r\n",
       "Q 39.84375 48.1875 33.796875 48.1875 \r\n",
       "Q 26.515625 48.1875 22.3125 43.546875 \r\n",
       "Q 18.109375 38.921875 18.109375 30.90625 \r\n",
       "L 18.109375 0 \r\n",
       "L 9.078125 0 \r\n",
       "L 9.078125 54.6875 \r\n",
       "L 18.109375 54.6875 \r\n",
       "L 18.109375 46.1875 \r\n",
       "Q 21.34375 51.125 25.703125 53.5625 \r\n",
       "Q 30.078125 56 35.796875 56 \r\n",
       "Q 45.21875 56 50.046875 50.171875 \r\n",
       "Q 54.890625 44.34375 54.890625 33.015625 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-110\"/>\r\n",
       "        <path d=\"M 4.890625 31.390625 \r\n",
       "L 31.203125 31.390625 \r\n",
       "L 31.203125 23.390625 \r\n",
       "L 4.890625 23.390625 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-45\"/>\r\n",
       "        <path d=\"M 54.890625 33.015625 \r\n",
       "L 54.890625 0 \r\n",
       "L 45.90625 0 \r\n",
       "L 45.90625 32.71875 \r\n",
       "Q 45.90625 40.484375 42.875 44.328125 \r\n",
       "Q 39.84375 48.1875 33.796875 48.1875 \r\n",
       "Q 26.515625 48.1875 22.3125 43.546875 \r\n",
       "Q 18.109375 38.921875 18.109375 30.90625 \r\n",
       "L 18.109375 0 \r\n",
       "L 9.078125 0 \r\n",
       "L 9.078125 75.984375 \r\n",
       "L 18.109375 75.984375 \r\n",
       "L 18.109375 46.1875 \r\n",
       "Q 21.34375 51.125 25.703125 53.5625 \r\n",
       "Q 30.078125 56 35.796875 56 \r\n",
       "Q 45.21875 56 50.046875 50.171875 \r\n",
       "Q 54.890625 44.34375 54.890625 33.015625 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-104\"/>\r\n",
       "        <path d=\"M 9.421875 75.984375 \r\n",
       "L 18.40625 75.984375 \r\n",
       "L 18.40625 0 \r\n",
       "L 9.421875 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-108\"/>\r\n",
       "        <path d=\"M 44.28125 53.078125 \r\n",
       "L 44.28125 44.578125 \r\n",
       "Q 40.484375 46.53125 36.375 47.5 \r\n",
       "Q 32.28125 48.484375 27.875 48.484375 \r\n",
       "Q 21.1875 48.484375 17.84375 46.4375 \r\n",
       "Q 14.5 44.390625 14.5 40.28125 \r\n",
       "Q 14.5 37.15625 16.890625 35.375 \r\n",
       "Q 19.28125 33.59375 26.515625 31.984375 \r\n",
       "L 29.59375 31.296875 \r\n",
       "Q 39.15625 29.25 43.1875 25.515625 \r\n",
       "Q 47.21875 21.78125 47.21875 15.09375 \r\n",
       "Q 47.21875 7.46875 41.1875 3.015625 \r\n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \r\n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \r\n",
       "Q 10.6875 0.296875 5.421875 2 \r\n",
       "L 5.421875 11.28125 \r\n",
       "Q 10.40625 8.6875 15.234375 7.390625 \r\n",
       "Q 20.0625 6.109375 24.8125 6.109375 \r\n",
       "Q 31.15625 6.109375 34.5625 8.28125 \r\n",
       "Q 37.984375 10.453125 37.984375 14.40625 \r\n",
       "Q 37.984375 18.0625 35.515625 20.015625 \r\n",
       "Q 33.0625 21.96875 24.703125 23.78125 \r\n",
       "L 21.578125 24.515625 \r\n",
       "Q 13.234375 26.265625 9.515625 29.90625 \r\n",
       "Q 5.8125 33.546875 5.8125 39.890625 \r\n",
       "Q 5.8125 47.609375 11.28125 51.796875 \r\n",
       "Q 16.75 56 26.8125 56 \r\n",
       "Q 31.78125 56 36.171875 55.265625 \r\n",
       "Q 40.578125 54.546875 44.28125 53.078125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-115\"/>\r\n",
       "        <path d=\"M 30.609375 48.390625 \r\n",
       "Q 23.390625 48.390625 19.1875 42.75 \r\n",
       "Q 14.984375 37.109375 14.984375 27.296875 \r\n",
       "Q 14.984375 17.484375 19.15625 11.84375 \r\n",
       "Q 23.34375 6.203125 30.609375 6.203125 \r\n",
       "Q 37.796875 6.203125 41.984375 11.859375 \r\n",
       "Q 46.1875 17.53125 46.1875 27.296875 \r\n",
       "Q 46.1875 37.015625 41.984375 42.703125 \r\n",
       "Q 37.796875 48.390625 30.609375 48.390625 \r\n",
       "z\r\n",
       "M 30.609375 56 \r\n",
       "Q 42.328125 56 49.015625 48.375 \r\n",
       "Q 55.71875 40.765625 55.71875 27.296875 \r\n",
       "Q 55.71875 13.875 49.015625 6.21875 \r\n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \r\n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \r\n",
       "Q 5.515625 13.875 5.515625 27.296875 \r\n",
       "Q 5.515625 40.765625 12.171875 48.375 \r\n",
       "Q 18.84375 56 30.609375 56 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-111\"/>\r\n",
       "        <path d=\"M 18.3125 70.21875 \r\n",
       "L 18.3125 54.6875 \r\n",
       "L 36.8125 54.6875 \r\n",
       "L 36.8125 47.703125 \r\n",
       "L 18.3125 47.703125 \r\n",
       "L 18.3125 18.015625 \r\n",
       "Q 18.3125 11.328125 20.140625 9.421875 \r\n",
       "Q 21.96875 7.515625 27.59375 7.515625 \r\n",
       "L 36.8125 7.515625 \r\n",
       "L 36.8125 0 \r\n",
       "L 27.59375 0 \r\n",
       "Q 17.1875 0 13.234375 3.875 \r\n",
       "Q 9.28125 7.765625 9.28125 18.015625 \r\n",
       "L 9.28125 47.703125 \r\n",
       "L 2.6875 47.703125 \r\n",
       "L 2.6875 54.6875 \r\n",
       "L 9.28125 54.6875 \r\n",
       "L 9.28125 70.21875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-116\"/>\r\n",
       "        <path d=\"M 32.171875 -5.078125 \r\n",
       "Q 28.375 -14.84375 24.75 -17.8125 \r\n",
       "Q 21.140625 -20.796875 15.09375 -20.796875 \r\n",
       "L 7.90625 -20.796875 \r\n",
       "L 7.90625 -13.28125 \r\n",
       "L 13.1875 -13.28125 \r\n",
       "Q 16.890625 -13.28125 18.9375 -11.515625 \r\n",
       "Q 21 -9.765625 23.484375 -3.21875 \r\n",
       "L 25.09375 0.875 \r\n",
       "L 2.984375 54.6875 \r\n",
       "L 12.5 54.6875 \r\n",
       "L 29.59375 11.921875 \r\n",
       "L 46.6875 54.6875 \r\n",
       "L 56.203125 54.6875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-121\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-97\"/>\r\n",
       "       <use x=\"61.279297\" xlink:href=\"#DejaVuSans-109\"/>\r\n",
       "       <use x=\"158.691406\" xlink:href=\"#DejaVuSans-101\"/>\r\n",
       "       <use x=\"220.214844\" xlink:href=\"#DejaVuSans-114\"/>\r\n",
       "       <use x=\"261.328125\" xlink:href=\"#DejaVuSans-105\"/>\r\n",
       "       <use x=\"289.111328\" xlink:href=\"#DejaVuSans-99\"/>\r\n",
       "       <use x=\"344.091797\" xlink:href=\"#DejaVuSans-97\"/>\r\n",
       "       <use x=\"405.371094\" xlink:href=\"#DejaVuSans-110\"/>\r\n",
       "       <use x=\"468.75\" xlink:href=\"#DejaVuSans-45\"/>\r\n",
       "       <use x=\"504.833984\" xlink:href=\"#DejaVuSans-99\"/>\r\n",
       "       <use x=\"559.814453\" xlink:href=\"#DejaVuSans-104\"/>\r\n",
       "       <use x=\"623.193359\" xlink:href=\"#DejaVuSans-101\"/>\r\n",
       "       <use x=\"684.716797\" xlink:href=\"#DejaVuSans-109\"/>\r\n",
       "       <use x=\"782.128906\" xlink:href=\"#DejaVuSans-105\"/>\r\n",
       "       <use x=\"809.912109\" xlink:href=\"#DejaVuSans-99\"/>\r\n",
       "       <use x=\"864.892578\" xlink:href=\"#DejaVuSans-97\"/>\r\n",
       "       <use x=\"926.171875\" xlink:href=\"#DejaVuSans-108\"/>\r\n",
       "       <use x=\"953.955078\" xlink:href=\"#DejaVuSans-45\"/>\r\n",
       "       <use x=\"990.039062\" xlink:href=\"#DejaVuSans-115\"/>\r\n",
       "       <use x=\"1042.138672\" xlink:href=\"#DejaVuSans-111\"/>\r\n",
       "       <use x=\"1103.320312\" xlink:href=\"#DejaVuSans-99\"/>\r\n",
       "       <use x=\"1158.300781\" xlink:href=\"#DejaVuSans-105\"/>\r\n",
       "       <use x=\"1186.083984\" xlink:href=\"#DejaVuSans-101\"/>\r\n",
       "       <use x=\"1247.607422\" xlink:href=\"#DejaVuSans-116\"/>\r\n",
       "       <use x=\"1286.816406\" xlink:href=\"#DejaVuSans-121\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_2\">\r\n",
       "     <g id=\"line2d_2\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"109.378409\" xlink:href=\"#mf3ec877f29\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_2\">\r\n",
       "      <!-- acm-sig-proceedings -->\r\n",
       "      <g transform=\"translate(112.137784 335.738437)rotate(-90)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 45.40625 27.984375 \r\n",
       "Q 45.40625 37.75 41.375 43.109375 \r\n",
       "Q 37.359375 48.484375 30.078125 48.484375 \r\n",
       "Q 22.859375 48.484375 18.828125 43.109375 \r\n",
       "Q 14.796875 37.75 14.796875 27.984375 \r\n",
       "Q 14.796875 18.265625 18.828125 12.890625 \r\n",
       "Q 22.859375 7.515625 30.078125 7.515625 \r\n",
       "Q 37.359375 7.515625 41.375 12.890625 \r\n",
       "Q 45.40625 18.265625 45.40625 27.984375 \r\n",
       "z\r\n",
       "M 54.390625 6.78125 \r\n",
       "Q 54.390625 -7.171875 48.1875 -13.984375 \r\n",
       "Q 42 -20.796875 29.203125 -20.796875 \r\n",
       "Q 24.46875 -20.796875 20.265625 -20.09375 \r\n",
       "Q 16.0625 -19.390625 12.109375 -17.921875 \r\n",
       "L 12.109375 -9.1875 \r\n",
       "Q 16.0625 -11.328125 19.921875 -12.34375 \r\n",
       "Q 23.78125 -13.375 27.78125 -13.375 \r\n",
       "Q 36.625 -13.375 41.015625 -8.765625 \r\n",
       "Q 45.40625 -4.15625 45.40625 5.171875 \r\n",
       "L 45.40625 9.625 \r\n",
       "Q 42.625 4.78125 38.28125 2.390625 \r\n",
       "Q 33.9375 0 27.875 0 \r\n",
       "Q 17.828125 0 11.671875 7.65625 \r\n",
       "Q 5.515625 15.328125 5.515625 27.984375 \r\n",
       "Q 5.515625 40.671875 11.671875 48.328125 \r\n",
       "Q 17.828125 56 27.875 56 \r\n",
       "Q 33.9375 56 38.28125 53.609375 \r\n",
       "Q 42.625 51.21875 45.40625 46.390625 \r\n",
       "L 45.40625 54.6875 \r\n",
       "L 54.390625 54.6875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-103\"/>\r\n",
       "        <path d=\"M 18.109375 8.203125 \r\n",
       "L 18.109375 -20.796875 \r\n",
       "L 9.078125 -20.796875 \r\n",
       "L 9.078125 54.6875 \r\n",
       "L 18.109375 54.6875 \r\n",
       "L 18.109375 46.390625 \r\n",
       "Q 20.953125 51.265625 25.265625 53.625 \r\n",
       "Q 29.59375 56 35.59375 56 \r\n",
       "Q 45.5625 56 51.78125 48.09375 \r\n",
       "Q 58.015625 40.1875 58.015625 27.296875 \r\n",
       "Q 58.015625 14.40625 51.78125 6.484375 \r\n",
       "Q 45.5625 -1.421875 35.59375 -1.421875 \r\n",
       "Q 29.59375 -1.421875 25.265625 0.953125 \r\n",
       "Q 20.953125 3.328125 18.109375 8.203125 \r\n",
       "z\r\n",
       "M 48.6875 27.296875 \r\n",
       "Q 48.6875 37.203125 44.609375 42.84375 \r\n",
       "Q 40.53125 48.484375 33.40625 48.484375 \r\n",
       "Q 26.265625 48.484375 22.1875 42.84375 \r\n",
       "Q 18.109375 37.203125 18.109375 27.296875 \r\n",
       "Q 18.109375 17.390625 22.1875 11.75 \r\n",
       "Q 26.265625 6.109375 33.40625 6.109375 \r\n",
       "Q 40.53125 6.109375 44.609375 11.75 \r\n",
       "Q 48.6875 17.390625 48.6875 27.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-112\"/>\r\n",
       "        <path d=\"M 45.40625 46.390625 \r\n",
       "L 45.40625 75.984375 \r\n",
       "L 54.390625 75.984375 \r\n",
       "L 54.390625 0 \r\n",
       "L 45.40625 0 \r\n",
       "L 45.40625 8.203125 \r\n",
       "Q 42.578125 3.328125 38.25 0.953125 \r\n",
       "Q 33.9375 -1.421875 27.875 -1.421875 \r\n",
       "Q 17.96875 -1.421875 11.734375 6.484375 \r\n",
       "Q 5.515625 14.40625 5.515625 27.296875 \r\n",
       "Q 5.515625 40.1875 11.734375 48.09375 \r\n",
       "Q 17.96875 56 27.875 56 \r\n",
       "Q 33.9375 56 38.25 53.625 \r\n",
       "Q 42.578125 51.265625 45.40625 46.390625 \r\n",
       "z\r\n",
       "M 14.796875 27.296875 \r\n",
       "Q 14.796875 17.390625 18.875 11.75 \r\n",
       "Q 22.953125 6.109375 30.078125 6.109375 \r\n",
       "Q 37.203125 6.109375 41.296875 11.75 \r\n",
       "Q 45.40625 17.390625 45.40625 27.296875 \r\n",
       "Q 45.40625 37.203125 41.296875 42.84375 \r\n",
       "Q 37.203125 48.484375 30.078125 48.484375 \r\n",
       "Q 22.953125 48.484375 18.875 42.84375 \r\n",
       "Q 14.796875 37.203125 14.796875 27.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-100\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-97\"/>\r\n",
       "       <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\r\n",
       "       <use x=\"116.259766\" xlink:href=\"#DejaVuSans-109\"/>\r\n",
       "       <use x=\"213.671875\" xlink:href=\"#DejaVuSans-45\"/>\r\n",
       "       <use x=\"249.755859\" xlink:href=\"#DejaVuSans-115\"/>\r\n",
       "       <use x=\"301.855469\" xlink:href=\"#DejaVuSans-105\"/>\r\n",
       "       <use x=\"329.638672\" xlink:href=\"#DejaVuSans-103\"/>\r\n",
       "       <use x=\"393.115234\" xlink:href=\"#DejaVuSans-45\"/>\r\n",
       "       <use x=\"429.199219\" xlink:href=\"#DejaVuSans-112\"/>\r\n",
       "       <use x=\"492.675781\" xlink:href=\"#DejaVuSans-114\"/>\r\n",
       "       <use x=\"531.539062\" xlink:href=\"#DejaVuSans-111\"/>\r\n",
       "       <use x=\"592.720703\" xlink:href=\"#DejaVuSans-99\"/>\r\n",
       "       <use x=\"647.701172\" xlink:href=\"#DejaVuSans-101\"/>\r\n",
       "       <use x=\"709.224609\" xlink:href=\"#DejaVuSans-101\"/>\r\n",
       "       <use x=\"770.748047\" xlink:href=\"#DejaVuSans-100\"/>\r\n",
       "       <use x=\"834.224609\" xlink:href=\"#DejaVuSans-105\"/>\r\n",
       "       <use x=\"862.007812\" xlink:href=\"#DejaVuSans-110\"/>\r\n",
       "       <use x=\"925.386719\" xlink:href=\"#DejaVuSans-103\"/>\r\n",
       "       <use x=\"988.863281\" xlink:href=\"#DejaVuSans-115\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_3\">\r\n",
       "     <g id=\"line2d_3\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"170.251136\" xlink:href=\"#mf3ec877f29\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_3\">\r\n",
       "      <!-- apa -->\r\n",
       "      <g transform=\"translate(173.010511 250.244687)rotate(-90)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-97\"/>\r\n",
       "       <use x=\"61.279297\" xlink:href=\"#DejaVuSans-112\"/>\r\n",
       "       <use x=\"124.755859\" xlink:href=\"#DejaVuSans-97\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_4\">\r\n",
       "     <g id=\"line2d_4\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"231.123864\" xlink:href=\"#mf3ec877f29\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_4\">\r\n",
       "      <!-- mla -->\r\n",
       "      <g transform=\"translate(233.883239 250.286875)rotate(-90)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-109\"/>\r\n",
       "       <use x=\"97.412109\" xlink:href=\"#DejaVuSans-108\"/>\r\n",
       "       <use x=\"125.195312\" xlink:href=\"#DejaVuSans-97\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_5\">\r\n",
       "     <g id=\"line2d_5\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"291.996591\" xlink:href=\"#mf3ec877f29\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_5\">\r\n",
       "      <!-- ieee -->\r\n",
       "      <g transform=\"translate(294.755966 252.8775)rotate(-90)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-105\"/>\r\n",
       "       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-101\"/>\r\n",
       "       <use x=\"89.306641\" xlink:href=\"#DejaVuSans-101\"/>\r\n",
       "       <use x=\"150.830078\" xlink:href=\"#DejaVuSans-101\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_6\">\r\n",
       "     <g id=\"line2d_6\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"352.869318\" xlink:href=\"#mf3ec877f29\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_6\">\r\n",
       "      <!-- harvard3 -->\r\n",
       "      <g transform=\"translate(355.628693 276.910312)rotate(-90)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 2.984375 54.6875 \r\n",
       "L 12.5 54.6875 \r\n",
       "L 29.59375 8.796875 \r\n",
       "L 46.6875 54.6875 \r\n",
       "L 56.203125 54.6875 \r\n",
       "L 35.6875 0 \r\n",
       "L 23.484375 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-118\"/>\r\n",
       "        <path d=\"M 40.578125 39.3125 \r\n",
       "Q 47.65625 37.796875 51.625 33 \r\n",
       "Q 55.609375 28.21875 55.609375 21.1875 \r\n",
       "Q 55.609375 10.40625 48.1875 4.484375 \r\n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \r\n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \r\n",
       "Q 12.796875 0.390625 7.625 2.203125 \r\n",
       "L 7.625 11.71875 \r\n",
       "Q 11.71875 9.328125 16.59375 8.109375 \r\n",
       "Q 21.484375 6.890625 26.8125 6.890625 \r\n",
       "Q 36.078125 6.890625 40.9375 10.546875 \r\n",
       "Q 45.796875 14.203125 45.796875 21.1875 \r\n",
       "Q 45.796875 27.640625 41.28125 31.265625 \r\n",
       "Q 36.765625 34.90625 28.71875 34.90625 \r\n",
       "L 20.21875 34.90625 \r\n",
       "L 20.21875 43.015625 \r\n",
       "L 29.109375 43.015625 \r\n",
       "Q 36.375 43.015625 40.234375 45.921875 \r\n",
       "Q 44.09375 48.828125 44.09375 54.296875 \r\n",
       "Q 44.09375 59.90625 40.109375 62.90625 \r\n",
       "Q 36.140625 65.921875 28.71875 65.921875 \r\n",
       "Q 24.65625 65.921875 20.015625 65.03125 \r\n",
       "Q 15.375 64.15625 9.8125 62.3125 \r\n",
       "L 9.8125 71.09375 \r\n",
       "Q 15.4375 72.65625 20.34375 73.4375 \r\n",
       "Q 25.25 74.21875 29.59375 74.21875 \r\n",
       "Q 40.828125 74.21875 47.359375 69.109375 \r\n",
       "Q 53.90625 64.015625 53.90625 55.328125 \r\n",
       "Q 53.90625 49.265625 50.4375 45.09375 \r\n",
       "Q 46.96875 40.921875 40.578125 39.3125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-51\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-104\"/>\r\n",
       "       <use x=\"63.378906\" xlink:href=\"#DejaVuSans-97\"/>\r\n",
       "       <use x=\"124.658203\" xlink:href=\"#DejaVuSans-114\"/>\r\n",
       "       <use x=\"165.771484\" xlink:href=\"#DejaVuSans-118\"/>\r\n",
       "       <use x=\"224.951172\" xlink:href=\"#DejaVuSans-97\"/>\r\n",
       "       <use x=\"286.230469\" xlink:href=\"#DejaVuSans-114\"/>\r\n",
       "       <use x=\"325.59375\" xlink:href=\"#DejaVuSans-100\"/>\r\n",
       "       <use x=\"389.070312\" xlink:href=\"#DejaVuSans-51\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_2\">\r\n",
       "    <g id=\"ytick_1\">\r\n",
       "     <g id=\"line2d_7\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 0 0 \r\n",
       "L -3.5 0 \r\n",
       "\" id=\"m0e940587ae\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n",
       "      </defs>\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0e940587ae\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_7\">\r\n",
       "      <!-- 0 -->\r\n",
       "      <g transform=\"translate(19.925 228.439219)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 31.78125 66.40625 \r\n",
       "Q 24.171875 66.40625 20.328125 58.90625 \r\n",
       "Q 16.5 51.421875 16.5 36.375 \r\n",
       "Q 16.5 21.390625 20.328125 13.890625 \r\n",
       "Q 24.171875 6.390625 31.78125 6.390625 \r\n",
       "Q 39.453125 6.390625 43.28125 13.890625 \r\n",
       "Q 47.125 21.390625 47.125 36.375 \r\n",
       "Q 47.125 51.421875 43.28125 58.90625 \r\n",
       "Q 39.453125 66.40625 31.78125 66.40625 \r\n",
       "z\r\n",
       "M 31.78125 74.21875 \r\n",
       "Q 44.046875 74.21875 50.515625 64.515625 \r\n",
       "Q 56.984375 54.828125 56.984375 36.375 \r\n",
       "Q 56.984375 17.96875 50.515625 8.265625 \r\n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \r\n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \r\n",
       "Q 6.59375 17.96875 6.59375 36.375 \r\n",
       "Q 6.59375 54.828125 13.0625 64.515625 \r\n",
       "Q 19.53125 74.21875 31.78125 74.21875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-48\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_2\">\r\n",
       "     <g id=\"line2d_8\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0e940587ae\" y=\"199.508239\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_8\">\r\n",
       "      <!-- 100 -->\r\n",
       "      <g transform=\"translate(7.2 203.307457)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 12.40625 8.296875 \r\n",
       "L 28.515625 8.296875 \r\n",
       "L 28.515625 63.921875 \r\n",
       "L 10.984375 60.40625 \r\n",
       "L 10.984375 69.390625 \r\n",
       "L 28.421875 72.90625 \r\n",
       "L 38.28125 72.90625 \r\n",
       "L 38.28125 8.296875 \r\n",
       "L 54.390625 8.296875 \r\n",
       "L 54.390625 0 \r\n",
       "L 12.40625 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-49\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_3\">\r\n",
       "     <g id=\"line2d_9\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0e940587ae\" y=\"174.376477\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_9\">\r\n",
       "      <!-- 200 -->\r\n",
       "      <g transform=\"translate(7.2 178.175696)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 19.1875 8.296875 \r\n",
       "L 53.609375 8.296875 \r\n",
       "L 53.609375 0 \r\n",
       "L 7.328125 0 \r\n",
       "L 7.328125 8.296875 \r\n",
       "Q 12.9375 14.109375 22.625 23.890625 \r\n",
       "Q 32.328125 33.6875 34.8125 36.53125 \r\n",
       "Q 39.546875 41.84375 41.421875 45.53125 \r\n",
       "Q 43.3125 49.21875 43.3125 52.78125 \r\n",
       "Q 43.3125 58.59375 39.234375 62.25 \r\n",
       "Q 35.15625 65.921875 28.609375 65.921875 \r\n",
       "Q 23.96875 65.921875 18.8125 64.3125 \r\n",
       "Q 13.671875 62.703125 7.8125 59.421875 \r\n",
       "L 7.8125 69.390625 \r\n",
       "Q 13.765625 71.78125 18.9375 73 \r\n",
       "Q 24.125 74.21875 28.421875 74.21875 \r\n",
       "Q 39.75 74.21875 46.484375 68.546875 \r\n",
       "Q 53.21875 62.890625 53.21875 53.421875 \r\n",
       "Q 53.21875 48.921875 51.53125 44.890625 \r\n",
       "Q 49.859375 40.875 45.40625 35.40625 \r\n",
       "Q 44.1875 33.984375 37.640625 27.21875 \r\n",
       "Q 31.109375 20.453125 19.1875 8.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-50\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_4\">\r\n",
       "     <g id=\"line2d_10\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0e940587ae\" y=\"149.244716\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_10\">\r\n",
       "      <!-- 300 -->\r\n",
       "      <g transform=\"translate(7.2 153.043934)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_5\">\r\n",
       "     <g id=\"line2d_11\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0e940587ae\" y=\"124.112954\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_11\">\r\n",
       "      <!-- 400 -->\r\n",
       "      <g transform=\"translate(7.2 127.912173)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 37.796875 64.3125 \r\n",
       "L 12.890625 25.390625 \r\n",
       "L 37.796875 25.390625 \r\n",
       "z\r\n",
       "M 35.203125 72.90625 \r\n",
       "L 47.609375 72.90625 \r\n",
       "L 47.609375 25.390625 \r\n",
       "L 58.015625 25.390625 \r\n",
       "L 58.015625 17.1875 \r\n",
       "L 47.609375 17.1875 \r\n",
       "L 47.609375 0 \r\n",
       "L 37.796875 0 \r\n",
       "L 37.796875 17.1875 \r\n",
       "L 4.890625 17.1875 \r\n",
       "L 4.890625 26.703125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-52\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_6\">\r\n",
       "     <g id=\"line2d_12\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0e940587ae\" y=\"98.981193\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_12\">\r\n",
       "      <!-- 500 -->\r\n",
       "      <g transform=\"translate(7.2 102.780412)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 10.796875 72.90625 \r\n",
       "L 49.515625 72.90625 \r\n",
       "L 49.515625 64.59375 \r\n",
       "L 19.828125 64.59375 \r\n",
       "L 19.828125 46.734375 \r\n",
       "Q 21.96875 47.46875 24.109375 47.828125 \r\n",
       "Q 26.265625 48.1875 28.421875 48.1875 \r\n",
       "Q 40.625 48.1875 47.75 41.5 \r\n",
       "Q 54.890625 34.8125 54.890625 23.390625 \r\n",
       "Q 54.890625 11.625 47.5625 5.09375 \r\n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \r\n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \r\n",
       "Q 12.796875 0.140625 7.71875 1.703125 \r\n",
       "L 7.71875 11.625 \r\n",
       "Q 12.109375 9.234375 16.796875 8.0625 \r\n",
       "Q 21.484375 6.890625 26.703125 6.890625 \r\n",
       "Q 35.15625 6.890625 40.078125 11.328125 \r\n",
       "Q 45.015625 15.765625 45.015625 23.390625 \r\n",
       "Q 45.015625 31 40.078125 35.4375 \r\n",
       "Q 35.15625 39.890625 26.703125 39.890625 \r\n",
       "Q 22.75 39.890625 18.8125 39.015625 \r\n",
       "Q 14.890625 38.140625 10.796875 36.28125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-53\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_7\">\r\n",
       "     <g id=\"line2d_13\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0e940587ae\" y=\"73.849431\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_13\">\r\n",
       "      <!-- 600 -->\r\n",
       "      <g transform=\"translate(7.2 77.64865)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 33.015625 40.375 \r\n",
       "Q 26.375 40.375 22.484375 35.828125 \r\n",
       "Q 18.609375 31.296875 18.609375 23.390625 \r\n",
       "Q 18.609375 15.53125 22.484375 10.953125 \r\n",
       "Q 26.375 6.390625 33.015625 6.390625 \r\n",
       "Q 39.65625 6.390625 43.53125 10.953125 \r\n",
       "Q 47.40625 15.53125 47.40625 23.390625 \r\n",
       "Q 47.40625 31.296875 43.53125 35.828125 \r\n",
       "Q 39.65625 40.375 33.015625 40.375 \r\n",
       "z\r\n",
       "M 52.59375 71.296875 \r\n",
       "L 52.59375 62.3125 \r\n",
       "Q 48.875 64.0625 45.09375 64.984375 \r\n",
       "Q 41.3125 65.921875 37.59375 65.921875 \r\n",
       "Q 27.828125 65.921875 22.671875 59.328125 \r\n",
       "Q 17.53125 52.734375 16.796875 39.40625 \r\n",
       "Q 19.671875 43.65625 24.015625 45.921875 \r\n",
       "Q 28.375 48.1875 33.59375 48.1875 \r\n",
       "Q 44.578125 48.1875 50.953125 41.515625 \r\n",
       "Q 57.328125 34.859375 57.328125 23.390625 \r\n",
       "Q 57.328125 12.15625 50.6875 5.359375 \r\n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \r\n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \r\n",
       "Q 6.984375 17.96875 6.984375 36.375 \r\n",
       "Q 6.984375 53.65625 15.1875 63.9375 \r\n",
       "Q 23.390625 74.21875 37.203125 74.21875 \r\n",
       "Q 40.921875 74.21875 44.703125 73.484375 \r\n",
       "Q 48.484375 72.75 52.59375 71.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-54\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-54\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_8\">\r\n",
       "     <g id=\"line2d_14\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0e940587ae\" y=\"48.71767\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_14\">\r\n",
       "      <!-- 700 -->\r\n",
       "      <g transform=\"translate(7.2 52.516889)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 8.203125 72.90625 \r\n",
       "L 55.078125 72.90625 \r\n",
       "L 55.078125 68.703125 \r\n",
       "L 28.609375 0 \r\n",
       "L 18.3125 0 \r\n",
       "L 43.21875 64.59375 \r\n",
       "L 8.203125 64.59375 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-55\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-55\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_9\">\r\n",
       "     <g id=\"line2d_15\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0e940587ae\" y=\"23.585908\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_15\">\r\n",
       "      <!-- 800 -->\r\n",
       "      <g transform=\"translate(7.2 27.385127)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 31.78125 34.625 \r\n",
       "Q 24.75 34.625 20.71875 30.859375 \r\n",
       "Q 16.703125 27.09375 16.703125 20.515625 \r\n",
       "Q 16.703125 13.921875 20.71875 10.15625 \r\n",
       "Q 24.75 6.390625 31.78125 6.390625 \r\n",
       "Q 38.8125 6.390625 42.859375 10.171875 \r\n",
       "Q 46.921875 13.96875 46.921875 20.515625 \r\n",
       "Q 46.921875 27.09375 42.890625 30.859375 \r\n",
       "Q 38.875 34.625 31.78125 34.625 \r\n",
       "z\r\n",
       "M 21.921875 38.8125 \r\n",
       "Q 15.578125 40.375 12.03125 44.71875 \r\n",
       "Q 8.5 49.078125 8.5 55.328125 \r\n",
       "Q 8.5 64.0625 14.71875 69.140625 \r\n",
       "Q 20.953125 74.21875 31.78125 74.21875 \r\n",
       "Q 42.671875 74.21875 48.875 69.140625 \r\n",
       "Q 55.078125 64.0625 55.078125 55.328125 \r\n",
       "Q 55.078125 49.078125 51.53125 44.71875 \r\n",
       "Q 48 40.375 41.703125 38.8125 \r\n",
       "Q 48.828125 37.15625 52.796875 32.3125 \r\n",
       "Q 56.78125 27.484375 56.78125 20.515625 \r\n",
       "Q 56.78125 9.90625 50.3125 4.234375 \r\n",
       "Q 43.84375 -1.421875 31.78125 -1.421875 \r\n",
       "Q 19.734375 -1.421875 13.25 4.234375 \r\n",
       "Q 6.78125 9.90625 6.78125 20.515625 \r\n",
       "Q 6.78125 27.484375 10.78125 32.3125 \r\n",
       "Q 14.796875 37.15625 21.921875 38.8125 \r\n",
       "z\r\n",
       "M 18.3125 54.390625 \r\n",
       "Q 18.3125 48.734375 21.84375 45.5625 \r\n",
       "Q 25.390625 42.390625 31.78125 42.390625 \r\n",
       "Q 38.140625 42.390625 41.71875 45.5625 \r\n",
       "Q 45.3125 48.734375 45.3125 54.390625 \r\n",
       "Q 45.3125 60.0625 41.71875 63.234375 \r\n",
       "Q 38.140625 66.40625 31.78125 66.40625 \r\n",
       "Q 25.390625 66.40625 21.84375 63.234375 \r\n",
       "Q 18.3125 60.0625 18.3125 54.390625 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-56\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-56\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_23\">\r\n",
       "    <path d=\"M 33.2875 224.64 \r\n",
       "L 33.2875 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_24\">\r\n",
       "    <path d=\"M 368.0875 224.64 \r\n",
       "L 368.0875 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_25\">\r\n",
       "    <path d=\"M 33.2875 224.64 \r\n",
       "L 368.0875 224.64 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_26\">\r\n",
       "    <path d=\"M 33.2875 7.2 \r\n",
       "L 368.0875 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "  </g>\r\n",
       " </g>\r\n",
       " <defs>\r\n",
       "  <clipPath id=\"pb49a6a26ec\">\r\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"7.2\"/>\r\n",
       "  </clipPath>\r\n",
       " </defs>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "testingstyle = gs.predict(X_sentence)\n",
    "testingstyle1 = gs.predict(X_test_sentence)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(testingstyle)\n",
    "plt.xticks(rotation=45)\n",
    "plt.hist(testingstyle1)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = self.input_size,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialise hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "\n",
    "        # Initialise internal state\n",
    "        c0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size)\n",
    "\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "\n",
    "        return output, (hn, cn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = LSTM(input_size, hidden_size, output_size, num_layers)\n",
    "        # self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.fc = nn.Linear(hidden_size + X_style.shape[2], output_size)\n",
    "        # self.fc1 = nn.Linear(output_size, output_size)\n",
    "\n",
    "    def forward(self, x, style):\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        output = torch.cat((output, torch.tensor(style)), 2)\n",
    "        output = self.fc(output)\n",
    "        # output = self.fc1(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hyperparameters\n",
    "'''\n",
    "num_epochs = 800\n",
    "learning_rate = 0.001\n",
    "\n",
    "we_feature_size = EMBEDDING_DIM\n",
    "ner_feature_size = len(ner_dict.keys()) + 1\n",
    "scibert_feature_size = 768\n",
    "\n",
    "input_size = we_feature_size + ner_feature_size + scibert_feature_size #+ len(categories) # Number of features (change accordingly)\n",
    "hidden_size = 25 #+ X_style.shape[2] # Number of features in the hidden state\n",
    "num_layers = 1 # Number of stacked LSTM layers\n",
    "\n",
    "output_size = len(all_tags) # Number of output classes\n",
    "model = Net(input_size, hidden_size, output_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loss Function and Optimiser\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=len(all_tags))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss after minibatch     1: 2.62793\n",
      "Epoch: 0, loss after minibatch     2: 2.55478\n",
      "Epoch: 0, loss after minibatch     3: 2.49102\n",
      "Epoch: 0, loss after minibatch     4: 2.43839\n",
      "Epoch: 0, loss after minibatch     5: 2.41641\n",
      "Epoch: 0, loss after minibatch     6: 2.36021\n",
      "Epoch: 0, loss after minibatch     7: 2.34183\n",
      "Epoch: 0, loss after minibatch     8: 2.30827\n",
      "Epoch: 0, loss after minibatch     9: 2.26682\n",
      "Epoch: 0, loss after minibatch    10: 2.25848\n",
      "+-------------+---------------------+----------------------+----------------------+\n",
      "|     Tag     |      Precision      |        Recall        |        FBeta         |\n",
      "+-------------+---------------------+----------------------+----------------------+\n",
      "|  publisher  |         0.0         |         0.0          |         0.0          |\n",
      "|   location  |         0.0         |         0.0          |         0.0          |\n",
      "| institution |         0.0         |         0.0          |         0.0          |\n",
      "|    other    |         0.0         |         0.0          |         0.0          |\n",
      "|    editor   |         0.0         |         0.0          |         0.0          |\n",
      "|    title    |         0.75        | 0.006802721088435374 | 0.013483146067415729 |\n",
      "|    pages    |         0.0         |         0.0          |         0.0          |\n",
      "|  booktitle  |         0.0         |         0.0          |         0.0          |\n",
      "|     tech    |         0.0         |         0.0          |         0.0          |\n",
      "|     note    |         0.0         |         0.0          |         0.0          |\n",
      "|    punct    | 0.04002219601858917 |  0.6926770708283313  | 0.07567213114754098  |\n",
      "|    author   | 0.32882502113271345 |  0.8530701754385965  | 0.47467968273337396  |\n",
      "|   journal   |         0.0         |         0.0          |         0.0          |\n",
      "|     date    |         0.0         |         0.0          |         0.0          |\n",
      "|    volume   |         0.0         |         0.0          |         0.0          |\n",
      "+-------------+---------------------+----------------------+----------------------+\n",
      "9.795282125473022 10.0 current epoch 16: saving best model...\n",
      "9.322327435016632 9.795282125473022 current epoch 17: saving best model...\n",
      "8.887531876564026 9.322327435016632 current epoch 18: saving best model...\n",
      "8.495957732200623 8.887531876564026 current epoch 19: saving best model...\n",
      "8.138741731643677 8.495957732200623 current epoch 20: saving best model...\n",
      "7.785484194755554 8.138741731643677 current epoch 21: saving best model...\n",
      "7.469021379947662 7.785484194755554 current epoch 22: saving best model...\n",
      "7.1688655614852905 7.469021379947662 current epoch 23: saving best model...\n",
      "6.8971699476242065 7.1688655614852905 current epoch 24: saving best model...\n",
      "6.647159039974213 6.8971699476242065 current epoch 25: saving best model...\n",
      "6.416506707668304 6.647159039974213 current epoch 26: saving best model...\n",
      "6.208432078361511 6.416506707668304 current epoch 27: saving best model...\n",
      "5.997186541557312 6.208432078361511 current epoch 28: saving best model...\n",
      "5.812211096286774 5.997186541557312 current epoch 29: saving best model...\n",
      "5.654408395290375 5.812211096286774 current epoch 30: saving best model...\n",
      "5.502821505069733 5.654408395290375 current epoch 31: saving best model...\n",
      "5.35643857717514 5.502821505069733 current epoch 32: saving best model...\n",
      "5.227236688137054 5.35643857717514 current epoch 33: saving best model...\n",
      "5.089277505874634 5.227236688137054 current epoch 34: saving best model...\n",
      "4.946801394224167 5.089277505874634 current epoch 35: saving best model...\n",
      "4.82865571975708 4.946801394224167 current epoch 36: saving best model...\n",
      "4.693408101797104 4.82865571975708 current epoch 37: saving best model...\n",
      "4.572676062583923 4.693408101797104 current epoch 38: saving best model...\n",
      "4.463791579008102 4.572676062583923 current epoch 39: saving best model...\n",
      "4.368119329214096 4.463791579008102 current epoch 40: saving best model...\n",
      "4.279394209384918 4.368119329214096 current epoch 41: saving best model...\n",
      "4.1974805891513824 4.279394209384918 current epoch 42: saving best model...\n",
      "4.1171470284461975 4.1974805891513824 current epoch 43: saving best model...\n",
      "4.036445289850235 4.1171470284461975 current epoch 44: saving best model...\n",
      "3.9560614228248596 4.036445289850235 current epoch 45: saving best model...\n",
      "3.8813724517822266 3.9560614228248596 current epoch 46: saving best model...\n",
      "3.8080317676067352 3.8813724517822266 current epoch 47: saving best model...\n",
      "3.7414556443691254 3.8080317676067352 current epoch 48: saving best model...\n",
      "3.678930342197418 3.7414556443691254 current epoch 49: saving best model...\n",
      "3.6188269555568695 3.678930342197418 current epoch 50: saving best model...\n",
      "3.5616142451763153 3.6188269555568695 current epoch 51: saving best model...\n",
      "3.5074550807476044 3.5616142451763153 current epoch 52: saving best model...\n",
      "3.4574922621250153 3.5074550807476044 current epoch 53: saving best model...\n",
      "3.4058702886104584 3.4574922621250153 current epoch 54: saving best model...\n",
      "3.350373387336731 3.4058702886104584 current epoch 55: saving best model...\n",
      "3.297956168651581 3.350373387336731 current epoch 56: saving best model...\n",
      "3.246739000082016 3.297956168651581 current epoch 57: saving best model...\n",
      "3.1986348628997803 3.246739000082016 current epoch 58: saving best model...\n",
      "3.156893640756607 3.1986348628997803 current epoch 59: saving best model...\n",
      "3.1121967136859894 3.156893640756607 current epoch 60: saving best model...\n",
      "3.0657361149787903 3.1121967136859894 current epoch 61: saving best model...\n",
      "3.020155608654022 3.0657361149787903 current epoch 62: saving best model...\n",
      "2.973159521818161 3.020155608654022 current epoch 63: saving best model...\n",
      "2.9283111095428467 2.973159521818161 current epoch 64: saving best model...\n",
      "2.8900690972805023 2.9283111095428467 current epoch 65: saving best model...\n",
      "2.850179672241211 2.8900690972805023 current epoch 66: saving best model...\n",
      "2.8169459104537964 2.850179672241211 current epoch 67: saving best model...\n",
      "2.782987952232361 2.8169459104537964 current epoch 68: saving best model...\n",
      "2.7545446157455444 2.782987952232361 current epoch 69: saving best model...\n",
      "2.7277217358350754 2.7545446157455444 current epoch 70: saving best model...\n",
      "2.701857030391693 2.7277217358350754 current epoch 71: saving best model...\n",
      "2.679714024066925 2.701857030391693 current epoch 72: saving best model...\n",
      "2.6589318811893463 2.679714024066925 current epoch 73: saving best model...\n",
      "2.641328275203705 2.6589318811893463 current epoch 75: saving best model...\n",
      "2.598574608564377 2.641328275203705 current epoch 76: saving best model...\n",
      "2.5713057965040207 2.598574608564377 current epoch 77: saving best model...\n",
      "2.5556850731372833 2.5713057965040207 current epoch 78: saving best model...\n",
      "2.53734490275383 2.5556850731372833 current epoch 79: saving best model...\n",
      "2.5258814245462418 2.53734490275383 current epoch 80: saving best model...\n",
      "2.5214736461639404 2.5258814245462418 current epoch 83: saving best model...\n",
      "2.402935892343521 2.5214736461639404 current epoch 84: saving best model...\n",
      "2.357183873653412 2.402935892343521 current epoch 85: saving best model...\n",
      "2.3145540952682495 2.357183873653412 current epoch 86: saving best model...\n",
      "2.310783475637436 2.3145540952682495 current epoch 87: saving best model...\n",
      "2.28247107565403 2.310783475637436 current epoch 88: saving best model...\n",
      "2.2633875906467438 2.28247107565403 current epoch 90: saving best model...\n",
      "Epoch: 100, loss after minibatch     1: 0.20963\n",
      "Epoch: 100, loss after minibatch     2: 0.22533\n",
      "Epoch: 100, loss after minibatch     3: 0.23281\n",
      "Epoch: 100, loss after minibatch     4: 0.28265\n",
      "Epoch: 100, loss after minibatch     5: 0.26016\n",
      "Epoch: 100, loss after minibatch     6: 0.21021\n",
      "Epoch: 100, loss after minibatch     7: 0.22327\n",
      "Epoch: 100, loss after minibatch     8: 0.21654\n",
      "Epoch: 100, loss after minibatch     9: 0.28346\n",
      "Epoch: 100, loss after minibatch    10: 0.23870\n",
      "+-------------+---------------------+---------------------+---------------------+\n",
      "|     Tag     |      Precision      |        Recall       |        FBeta        |\n",
      "+-------------+---------------------+---------------------+---------------------+\n",
      "|  publisher  |       0.59375       |         0.76        |  0.6666666666666666 |\n",
      "|   location  |        0.875        |  0.7777777777777778 |  0.823529411764706  |\n",
      "| institution |         0.5         |  0.5714285714285714 |  0.5333333333333333 |\n",
      "|    other    |  0.9444444444444444 |  0.7083333333333334 |  0.8095238095238096 |\n",
      "|    editor   |         0.85        |  0.8947368421052632 |  0.8717948717948718 |\n",
      "|    title    |  0.8702594810379242 |  0.9886621315192744 |  0.9256900212314226 |\n",
      "|    pages    |  0.9649122807017544 |  0.9649122807017544 |  0.9649122807017544 |\n",
      "|  booktitle  |  0.9318181818181818 |  0.780952380952381  |  0.849740932642487  |\n",
      "|     tech    |         1.0         | 0.26666666666666666 |  0.4210526315789474 |\n",
      "|     note    |         0.0         |         0.0         |         0.0         |\n",
      "|    punct    | 0.05945713054717794 |  0.9939975990396158 | 0.11220272376177247 |\n",
      "|    author   |  0.9388185654008439 |  0.9758771929824561 |  0.956989247311828  |\n",
      "|   journal   |  0.8775510204081632 |  0.9214285714285714 |  0.8989547038327526 |\n",
      "|     date    |         0.97        |  0.8083333333333333 |  0.8818181818181818 |\n",
      "|    volume   |       0.890625      |  0.9193548387096774 |  0.9047619047619047 |\n",
      "+-------------+---------------------+---------------------+---------------------+\n",
      "2.1397962123155594 2.2633875906467438 current epoch 102: saving best model...\n",
      "2.059311404824257 2.1397962123155594 current epoch 103: saving best model...\n",
      "2.0179643034934998 2.059311404824257 current epoch 104: saving best model...\n",
      "1.9831809848546982 2.0179643034934998 current epoch 105: saving best model...\n",
      "1.9491199553012848 1.9831809848546982 current epoch 106: saving best model...\n",
      "1.923920899629593 1.9491199553012848 current epoch 107: saving best model...\n",
      "1.9009612202644348 1.923920899629593 current epoch 108: saving best model...\n",
      "1.8807882815599442 1.9009612202644348 current epoch 109: saving best model...\n",
      "1.862384393811226 1.8807882815599442 current epoch 110: saving best model...\n",
      "1.8449819684028625 1.862384393811226 current epoch 111: saving best model...\n",
      "1.8278150260448456 1.8449819684028625 current epoch 112: saving best model...\n",
      "1.811098262667656 1.8278150260448456 current epoch 113: saving best model...\n",
      "1.7945119440555573 1.811098262667656 current epoch 114: saving best model...\n",
      "1.7781090289354324 1.7945119440555573 current epoch 115: saving best model...\n",
      "1.7617370188236237 1.7781090289354324 current epoch 116: saving best model...\n",
      "1.7455287873744965 1.7617370188236237 current epoch 117: saving best model...\n",
      "1.7296989411115646 1.7455287873744965 current epoch 118: saving best model...\n",
      "1.7143970727920532 1.7296989411115646 current epoch 119: saving best model...\n",
      "1.6994457393884659 1.7143970727920532 current epoch 120: saving best model...\n",
      "1.6844393461942673 1.6994457393884659 current epoch 121: saving best model...\n",
      "1.6697030812501907 1.6844393461942673 current epoch 122: saving best model...\n",
      "1.6547588557004929 1.6697030812501907 current epoch 123: saving best model...\n",
      "1.6400384902954102 1.6547588557004929 current epoch 124: saving best model...\n",
      "1.6258593797683716 1.6400384902954102 current epoch 125: saving best model...\n",
      "1.61208376288414 1.6258593797683716 current epoch 126: saving best model...\n",
      "1.598834678530693 1.61208376288414 current epoch 127: saving best model...\n",
      "1.5861437618732452 1.598834678530693 current epoch 128: saving best model...\n",
      "1.5755490064620972 1.5861437618732452 current epoch 129: saving best model...\n",
      "1.5658914297819138 1.5755490064620972 current epoch 130: saving best model...\n",
      "1.555492103099823 1.5658914297819138 current epoch 131: saving best model...\n",
      "1.5402152389287949 1.555492103099823 current epoch 132: saving best model...\n",
      "1.523830108344555 1.5402152389287949 current epoch 133: saving best model...\n",
      "1.5083552449941635 1.523830108344555 current epoch 134: saving best model...\n",
      "1.4948964193463326 1.5083552449941635 current epoch 135: saving best model...\n",
      "1.4821929708123207 1.4948964193463326 current epoch 136: saving best model...\n",
      "1.4720210656523705 1.4821929708123207 current epoch 137: saving best model...\n",
      "1.4619900360703468 1.4720210656523705 current epoch 138: saving best model...\n",
      "1.4535453766584396 1.4619900360703468 current epoch 139: saving best model...\n",
      "1.4432391375303268 1.4535453766584396 current epoch 140: saving best model...\n",
      "1.4334231615066528 1.4432391375303268 current epoch 141: saving best model...\n",
      "1.4226147904992104 1.4334231615066528 current epoch 142: saving best model...\n",
      "1.414272591471672 1.4226147904992104 current epoch 143: saving best model...\n",
      "1.404471106827259 1.414272591471672 current epoch 144: saving best model...\n",
      "1.396070010960102 1.404471106827259 current epoch 145: saving best model...\n",
      "1.3921423181891441 1.396070010960102 current epoch 146: saving best model...\n",
      "1.390263132750988 1.3921423181891441 current epoch 147: saving best model...\n",
      "1.3888457790017128 1.390263132750988 current epoch 148: saving best model...\n",
      "1.3862400650978088 1.3888457790017128 current epoch 149: saving best model...\n",
      "1.3845951929688454 1.3862400650978088 current epoch 151: saving best model...\n",
      "1.3790548741817474 1.3845951929688454 current epoch 152: saving best model...\n",
      "1.3781652227044106 1.3790548741817474 current epoch 158: saving best model...\n",
      "1.3211860284209251 1.3781652227044106 current epoch 160: saving best model...\n",
      "1.3098964914679527 1.3211860284209251 current epoch 161: saving best model...\n",
      "1.283451646566391 1.3098964914679527 current epoch 162: saving best model...\n",
      "1.2466478124260902 1.283451646566391 current epoch 163: saving best model...\n",
      "1.2175902351737022 1.2466478124260902 current epoch 164: saving best model...\n",
      "1.199509359896183 1.2175902351737022 current epoch 165: saving best model...\n",
      "1.1902043968439102 1.199509359896183 current epoch 166: saving best model...\n",
      "1.181056410074234 1.1902043968439102 current epoch 167: saving best model...\n",
      "1.1741285622119904 1.181056410074234 current epoch 168: saving best model...\n",
      "1.1689072400331497 1.1741285622119904 current epoch 169: saving best model...\n",
      "1.1653912365436554 1.1689072400331497 current epoch 173: saving best model...\n",
      "1.160731203854084 1.1653912365436554 current epoch 174: saving best model...\n",
      "1.1492727100849152 1.160731203854084 current epoch 175: saving best model...\n",
      "1.1384273245930672 1.1492727100849152 current epoch 176: saving best model...\n",
      "1.1292333230376244 1.1384273245930672 current epoch 177: saving best model...\n",
      "1.1182934641838074 1.1292333230376244 current epoch 178: saving best model...\n",
      "1.1107858270406723 1.1182934641838074 current epoch 179: saving best model...\n",
      "1.1060500666499138 1.1107858270406723 current epoch 180: saving best model...\n",
      "1.09653852134943 1.1060500666499138 current epoch 181: saving best model...\n",
      "1.0867212489247322 1.09653852134943 current epoch 182: saving best model...\n",
      "1.0772587582468987 1.0867212489247322 current epoch 183: saving best model...\n",
      "1.0694119855761528 1.0772587582468987 current epoch 184: saving best model...\n",
      "1.0626055747270584 1.0694119855761528 current epoch 185: saving best model...\n",
      "1.0572605207562447 1.0626055747270584 current epoch 186: saving best model...\n",
      "1.0534591600298882 1.0572605207562447 current epoch 187: saving best model...\n",
      "Epoch: 200, loss after minibatch     1: 0.10870\n",
      "Epoch: 200, loss after minibatch     2: 0.13415\n",
      "Epoch: 200, loss after minibatch     3: 0.14756\n",
      "Epoch: 200, loss after minibatch     4: 0.14707\n",
      "Epoch: 200, loss after minibatch     5: 0.12085\n",
      "Epoch: 200, loss after minibatch     6: 0.11939\n",
      "Epoch: 200, loss after minibatch     7: 0.14231\n",
      "Epoch: 200, loss after minibatch     8: 0.15105\n",
      "Epoch: 200, loss after minibatch     9: 0.15185\n",
      "Epoch: 200, loss after minibatch    10: 0.07665\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "|     Tag     |      Precision      |       Recall       |        FBeta        |\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "|  publisher  |  0.8888888888888888 |        0.96        |  0.923076923076923  |\n",
      "|   location  |         1.0         | 0.8888888888888888 |  0.9411764705882353 |\n",
      "| institution |  0.9047619047619048 | 0.9047619047619048 |  0.9047619047619048 |\n",
      "|    other    |  0.9652173913043478 |       0.925        |  0.9446808510638298 |\n",
      "|    editor   |         1.0         |        1.0         |         1.0         |\n",
      "|    title    |  0.9732142857142857 | 0.9886621315192744 |  0.9808773903262092 |\n",
      "|    pages    |  0.991304347826087  |        1.0         |  0.9956331877729258 |\n",
      "|  booktitle  |         1.0         |        1.0         |         1.0         |\n",
      "|     tech    |  0.9655172413793104 | 0.9333333333333333 |  0.9491525423728815 |\n",
      "|     note    |         1.0         |        0.5         |  0.6666666666666666 |\n",
      "|    punct    | 0.05965970277837605 | 0.9975990396158463 | 0.11258637041051349 |\n",
      "|    author   |  0.9783080260303688 | 0.9890350877192983 |  0.9836423118865868 |\n",
      "|   journal   |         1.0         | 0.9714285714285714 |  0.9855072463768115 |\n",
      "|     date    |        0.975        |       0.975        |        0.975        |\n",
      "|    volume   |         1.0         |        1.0         |         1.0         |\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "0.9859558753669262 1.0534591600298882 current epoch 205: saving best model...\n",
      "0.9490241706371307 0.9859558753669262 current epoch 206: saving best model...\n",
      "0.9285210594534874 0.9490241706371307 current epoch 207: saving best model...\n",
      "0.909460611641407 0.9285210594534874 current epoch 208: saving best model...\n",
      "0.8987527750432491 0.909460611641407 current epoch 209: saving best model...\n",
      "0.8893228508532047 0.8987527750432491 current epoch 210: saving best model...\n",
      "0.8808654993772507 0.8893228508532047 current epoch 211: saving best model...\n",
      "0.8732838742434978 0.8808654993772507 current epoch 212: saving best model...\n",
      "0.8656389340758324 0.8732838742434978 current epoch 213: saving best model...\n",
      "0.858682282269001 0.8656389340758324 current epoch 214: saving best model...\n",
      "0.8519500605762005 0.858682282269001 current epoch 215: saving best model...\n",
      "0.8455094397068024 0.8519500605762005 current epoch 216: saving best model...\n",
      "0.8392112404108047 0.8455094397068024 current epoch 217: saving best model...\n",
      "0.8330803886055946 0.8392112404108047 current epoch 218: saving best model...\n",
      "0.8270336464047432 0.8330803886055946 current epoch 219: saving best model...\n",
      "0.8210655674338341 0.8270336464047432 current epoch 220: saving best model...\n",
      "0.8152194544672966 0.8210655674338341 current epoch 221: saving best model...\n",
      "0.8094952329993248 0.8152194544672966 current epoch 222: saving best model...\n",
      "0.8038636073470116 0.8094952329993248 current epoch 223: saving best model...\n",
      "0.7983099073171616 0.8038636073470116 current epoch 224: saving best model...\n",
      "0.7928197830915451 0.7983099073171616 current epoch 225: saving best model...\n",
      "0.7873419113457203 0.7928197830915451 current epoch 226: saving best model...\n",
      "0.7819558084011078 0.7873419113457203 current epoch 227: saving best model...\n",
      "0.7767078094184399 0.7819558084011078 current epoch 228: saving best model...\n",
      "0.7714717537164688 0.7767078094184399 current epoch 229: saving best model...\n",
      "0.7663072049617767 0.7714717537164688 current epoch 230: saving best model...\n",
      "0.7611840814352036 0.7663072049617767 current epoch 231: saving best model...\n",
      "0.7560792453587055 0.7611840814352036 current epoch 232: saving best model...\n",
      "0.7510146275162697 0.7560792453587055 current epoch 233: saving best model...\n",
      "0.745989341288805 0.7510146275162697 current epoch 234: saving best model...\n",
      "0.7409865781664848 0.745989341288805 current epoch 235: saving best model...\n",
      "0.7359975166618824 0.7409865781664848 current epoch 236: saving best model...\n",
      "0.7310211658477783 0.7359975166618824 current epoch 237: saving best model...\n",
      "0.726056657731533 0.7310211658477783 current epoch 238: saving best model...\n",
      "0.7210905477404594 0.726056657731533 current epoch 239: saving best model...\n",
      "0.7161918804049492 0.7210905477404594 current epoch 240: saving best model...\n",
      "0.7112431228160858 0.7161918804049492 current epoch 241: saving best model...\n",
      "0.7063367776572704 0.7112431228160858 current epoch 242: saving best model...\n",
      "0.7014339715242386 0.7063367776572704 current epoch 243: saving best model...\n",
      "0.6964386254549026 0.7014339715242386 current epoch 244: saving best model...\n",
      "0.6917809508740902 0.6964386254549026 current epoch 245: saving best model...\n",
      "0.6868381313979626 0.6917809508740902 current epoch 246: saving best model...\n",
      "0.6821868233382702 0.6868381313979626 current epoch 247: saving best model...\n",
      "0.6774330995976925 0.6821868233382702 current epoch 248: saving best model...\n",
      "0.6728788465261459 0.6774330995976925 current epoch 249: saving best model...\n",
      "0.6685284115374088 0.6728788465261459 current epoch 250: saving best model...\n",
      "0.6645378284156322 0.6685284115374088 current epoch 251: saving best model...\n",
      "0.660389494150877 0.6645378284156322 current epoch 252: saving best model...\n",
      "0.6559512466192245 0.660389494150877 current epoch 253: saving best model...\n",
      "0.6519088633358479 0.6559512466192245 current epoch 254: saving best model...\n",
      "0.6484409384429455 0.6519088633358479 current epoch 255: saving best model...\n",
      "0.6452362760901451 0.6484409384429455 current epoch 256: saving best model...\n",
      "0.6426534205675125 0.6452362760901451 current epoch 257: saving best model...\n",
      "0.6398403905332088 0.6426534205675125 current epoch 258: saving best model...\n",
      "0.6376951970160007 0.6398403905332088 current epoch 259: saving best model...\n",
      "0.6358678936958313 0.6376951970160007 current epoch 260: saving best model...\n",
      "0.634315736591816 0.6358678936958313 current epoch 261: saving best model...\n",
      "0.6333119384944439 0.634315736591816 current epoch 262: saving best model...\n",
      "0.632829949259758 0.6333119384944439 current epoch 263: saving best model...\n",
      "0.628143522888422 0.632829949259758 current epoch 275: saving best model...\n",
      "0.623763732612133 0.628143522888422 current epoch 277: saving best model...\n",
      "0.6063049398362637 0.623763732612133 current epoch 278: saving best model...\n",
      "0.6028047129511833 0.6063049398362637 current epoch 279: saving best model...\n",
      "0.6024180091917515 0.6028047129511833 current epoch 282: saving best model...\n",
      "Epoch: 300, loss after minibatch     1: 0.05583\n",
      "Epoch: 300, loss after minibatch     2: 0.08402\n",
      "Epoch: 300, loss after minibatch     3: 0.07594\n",
      "Epoch: 300, loss after minibatch     4: 0.07172\n",
      "Epoch: 300, loss after minibatch     5: 0.08321\n",
      "Epoch: 300, loss after minibatch     6: 0.06579\n",
      "Epoch: 300, loss after minibatch     7: 0.06485\n",
      "Epoch: 300, loss after minibatch     8: 0.06217\n",
      "Epoch: 300, loss after minibatch     9: 0.08590\n",
      "Epoch: 300, loss after minibatch    10: 0.05333\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "|     Tag     |      Precision      |       Recall       |        FBeta        |\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "|  publisher  |  0.9259259259259259 |        1.0         |  0.9615384615384615 |\n",
      "|   location  |         1.0         | 0.9444444444444444 |  0.9714285714285714 |\n",
      "| institution |         1.0         | 0.9047619047619048 |  0.9500000000000001 |\n",
      "|    other    |         1.0         |       0.925        |  0.961038961038961  |\n",
      "|    editor   |         1.0         |        1.0         |         1.0         |\n",
      "|    title    |  0.9566160520607375 |        1.0         |  0.9778270509977828 |\n",
      "|    pages    |         1.0         |        1.0         |         1.0         |\n",
      "|  booktitle  |         1.0         | 0.9047619047619048 |  0.9500000000000001 |\n",
      "|     tech    |         1.0         |        1.0         |         1.0         |\n",
      "|     note    |         1.0         |        1.0         |         1.0         |\n",
      "|    punct    | 0.05979470246213481 |        1.0         | 0.11284204822541317 |\n",
      "|    author   |  0.9912472647702407 | 0.993421052631579  |  0.9923329682365828 |\n",
      "|   journal   |  0.9655172413793104 |        1.0         |  0.9824561403508771 |\n",
      "|     date    |  0.9912280701754386 | 0.9416666666666667 |  0.9658119658119658 |\n",
      "|    volume   |         1.0         |        1.0         |         1.0         |\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "0.5763935092836618 0.6024180091917515 current epoch 303: saving best model...\n",
      "0.5276129115372896 0.5763935092836618 current epoch 304: saving best model...\n",
      "0.514518890529871 0.5276129115372896 current epoch 305: saving best model...\n",
      "0.5037295576184988 0.514518890529871 current epoch 306: saving best model...\n",
      "0.49909235537052155 0.5037295576184988 current epoch 307: saving best model...\n",
      "0.49635443463921547 0.49909235537052155 current epoch 308: saving best model...\n",
      "0.4957912992686033 0.49635443463921547 current epoch 309: saving best model...\n",
      "0.4952230844646692 0.4957912992686033 current epoch 310: saving best model...\n",
      "0.49473986960947514 0.4952230844646692 current epoch 313: saving best model...\n",
      "0.49068037047982216 0.49473986960947514 current epoch 314: saving best model...\n",
      "0.4831751976162195 0.49068037047982216 current epoch 315: saving best model...\n",
      "0.4724252335727215 0.4831751976162195 current epoch 316: saving best model...\n",
      "0.4610519465059042 0.4724252335727215 current epoch 317: saving best model...\n",
      "0.45234369672834873 0.4610519465059042 current epoch 318: saving best model...\n",
      "0.4468675032258034 0.45234369672834873 current epoch 319: saving best model...\n",
      "0.44290141202509403 0.4468675032258034 current epoch 320: saving best model...\n",
      "0.4397772569209337 0.44290141202509403 current epoch 321: saving best model...\n",
      "0.43637796491384506 0.4397772569209337 current epoch 322: saving best model...\n",
      "0.43291813135147095 0.43637796491384506 current epoch 323: saving best model...\n",
      "0.42924121953547 0.43291813135147095 current epoch 324: saving best model...\n",
      "0.42576119489967823 0.42924121953547 current epoch 325: saving best model...\n",
      "0.42236330546438694 0.42576119489967823 current epoch 326: saving best model...\n",
      "0.41889534518122673 0.42236330546438694 current epoch 327: saving best model...\n",
      "0.41576095670461655 0.41889534518122673 current epoch 328: saving best model...\n",
      "0.4128839951008558 0.41576095670461655 current epoch 329: saving best model...\n",
      "0.4103350732475519 0.4128839951008558 current epoch 330: saving best model...\n",
      "0.40867074579000473 0.4103350732475519 current epoch 331: saving best model...\n",
      "0.40629824809730053 0.40867074579000473 current epoch 332: saving best model...\n",
      "0.40378228202462196 0.40629824809730053 current epoch 333: saving best model...\n",
      "0.4019063878804445 0.40378228202462196 current epoch 334: saving best model...\n",
      "0.3994903452694416 0.4019063878804445 current epoch 335: saving best model...\n",
      "0.3963538147509098 0.3994903452694416 current epoch 336: saving best model...\n",
      "0.3943930435925722 0.3963538147509098 current epoch 337: saving best model...\n",
      "0.39193436317145824 0.3943930435925722 current epoch 338: saving best model...\n",
      "0.3884981293231249 0.39193436317145824 current epoch 339: saving best model...\n",
      "0.3851153776049614 0.3884981293231249 current epoch 340: saving best model...\n",
      "0.3828261625021696 0.3851153776049614 current epoch 341: saving best model...\n",
      "0.37966108694672585 0.3828261625021696 current epoch 342: saving best model...\n",
      "0.376571424305439 0.37966108694672585 current epoch 343: saving best model...\n",
      "0.3740820735692978 0.376571424305439 current epoch 344: saving best model...\n",
      "0.3717769365757704 0.3740820735692978 current epoch 345: saving best model...\n",
      "0.36861149966716766 0.3717769365757704 current epoch 346: saving best model...\n",
      "0.36641407012939453 0.36861149966716766 current epoch 347: saving best model...\n",
      "0.365043081343174 0.36641407012939453 current epoch 348: saving best model...\n",
      "0.3630001228302717 0.365043081343174 current epoch 349: saving best model...\n",
      "0.3602974470704794 0.3630001228302717 current epoch 350: saving best model...\n",
      "0.3580621648579836 0.3602974470704794 current epoch 351: saving best model...\n",
      "0.3569851201027632 0.3580621648579836 current epoch 352: saving best model...\n",
      "0.3555339351296425 0.3569851201027632 current epoch 353: saving best model...\n",
      "0.35300937481224537 0.3555339351296425 current epoch 354: saving best model...\n",
      "0.3511961102485657 0.35300937481224537 current epoch 355: saving best model...\n",
      "0.35029866360127926 0.3511961102485657 current epoch 356: saving best model...\n",
      "0.34861475601792336 0.35029866360127926 current epoch 357: saving best model...\n",
      "0.34672494046390057 0.34861475601792336 current epoch 358: saving best model...\n",
      "0.3461637310683727 0.34672494046390057 current epoch 359: saving best model...\n",
      "0.34599023684859276 0.3461637310683727 current epoch 360: saving best model...\n",
      "0.3449578136205673 0.34599023684859276 current epoch 361: saving best model...\n",
      "0.3428090959787369 0.3449578136205673 current epoch 363: saving best model...\n",
      "0.34144340083003044 0.3428090959787369 current epoch 364: saving best model...\n",
      "0.3393948618322611 0.34144340083003044 current epoch 365: saving best model...\n",
      "0.33857864886522293 0.3393948618322611 current epoch 368: saving best model...\n",
      "0.3375629298388958 0.33857864886522293 current epoch 369: saving best model...\n",
      "0.3353124838322401 0.3375629298388958 current epoch 370: saving best model...\n",
      "0.3352975081652403 0.3353124838322401 current epoch 371: saving best model...\n",
      "Epoch: 400, loss after minibatch     1: 0.03058\n",
      "Epoch: 400, loss after minibatch     2: 0.03988\n",
      "Epoch: 400, loss after minibatch     3: 0.03561\n",
      "Epoch: 400, loss after minibatch     4: 0.03783\n",
      "Epoch: 400, loss after minibatch     5: 0.03772\n",
      "Epoch: 400, loss after minibatch     6: 0.03144\n",
      "Epoch: 400, loss after minibatch     7: 0.03448\n",
      "Epoch: 400, loss after minibatch     8: 0.03178\n",
      "Epoch: 400, loss after minibatch     9: 0.03612\n",
      "Epoch: 400, loss after minibatch    10: 0.01487\n",
      "+-------------+----------------------+--------------------+---------------------+\n",
      "|     Tag     |      Precision       |       Recall       |        FBeta        |\n",
      "+-------------+----------------------+--------------------+---------------------+\n",
      "|  publisher  |         1.0          |        1.0         |         1.0         |\n",
      "|   location  |         1.0          |        1.0         |         1.0         |\n",
      "| institution |         1.0          | 0.9523809523809523 |  0.975609756097561  |\n",
      "|    other    |         1.0          | 0.9916666666666667 |   0.99581589958159  |\n",
      "|    editor   | 0.03356890459363958  |        1.0         | 0.06495726495726496 |\n",
      "|    title    |  0.9977324263038548  | 0.9977324263038548 |  0.9977324263038548 |\n",
      "|    pages    |         1.0          |        1.0         |         1.0         |\n",
      "|  booktitle  |         1.0          |        1.0         |         1.0         |\n",
      "|     tech    |         1.0          |        1.0         |         1.0         |\n",
      "|     note    |         1.0          |        1.0         |         1.0         |\n",
      "|    punct    | 0.062238493723849375 |        1.0         |  0.1171836533727228 |\n",
      "|    author   |  0.9956331877729258  |        1.0         |  0.9978118161925601 |\n",
      "|   journal   |         1.0          |        1.0         |         1.0         |\n",
      "|     date    |         1.0          |        1.0         |         1.0         |\n",
      "|    volume   |         1.0          |        1.0         |         1.0         |\n",
      "+-------------+----------------------+--------------------+---------------------+\n",
      "0.33029816299676895 0.3352975081652403 current epoch 400: saving best model...\n",
      "0.32189344707876444 0.33029816299676895 current epoch 401: saving best model...\n",
      "0.31670402083545923 0.32189344707876444 current epoch 402: saving best model...\n",
      "0.3102892627939582 0.31670402083545923 current epoch 403: saving best model...\n",
      "0.3049814011901617 0.3102892627939582 current epoch 404: saving best model...\n",
      "0.30092762131243944 0.3049814011901617 current epoch 405: saving best model...\n",
      "0.2972435923293233 0.30092762131243944 current epoch 406: saving best model...\n",
      "0.2941363053396344 0.2972435923293233 current epoch 407: saving best model...\n",
      "0.2909391522407532 0.2941363053396344 current epoch 408: saving best model...\n",
      "0.28807119000703096 0.2909391522407532 current epoch 409: saving best model...\n",
      "0.2855031583458185 0.28807119000703096 current epoch 410: saving best model...\n",
      "0.2827949486672878 0.2855031583458185 current epoch 411: saving best model...\n",
      "0.28038730937987566 0.2827949486672878 current epoch 412: saving best model...\n",
      "0.2780147083103657 0.28038730937987566 current epoch 413: saving best model...\n",
      "0.2755885673686862 0.2780147083103657 current epoch 414: saving best model...\n",
      "0.27322626765817404 0.2755885673686862 current epoch 415: saving best model...\n",
      "0.27082533575594425 0.27322626765817404 current epoch 416: saving best model...\n",
      "0.26853939797729254 0.27082533575594425 current epoch 417: saving best model...\n",
      "0.26658264081925154 0.26853939797729254 current epoch 418: saving best model...\n",
      "0.2646369906142354 0.26658264081925154 current epoch 419: saving best model...\n",
      "0.26275783590972424 0.2646369906142354 current epoch 420: saving best model...\n",
      "0.26084908470511436 0.26275783590972424 current epoch 421: saving best model...\n",
      "0.258995414711535 0.26084908470511436 current epoch 422: saving best model...\n",
      "0.257161021232605 0.258995414711535 current epoch 423: saving best model...\n",
      "0.2554910834878683 0.257161021232605 current epoch 424: saving best model...\n",
      "0.2538089184090495 0.2554910834878683 current epoch 425: saving best model...\n",
      "0.2522254092618823 0.2538089184090495 current epoch 426: saving best model...\n",
      "0.25062741059809923 0.2522254092618823 current epoch 427: saving best model...\n",
      "0.24906799476593733 0.25062741059809923 current epoch 428: saving best model...\n",
      "0.24751397129148245 0.24906799476593733 current epoch 429: saving best model...\n",
      "0.24598105158656836 0.24751397129148245 current epoch 430: saving best model...\n",
      "0.24447394907474518 0.24598105158656836 current epoch 431: saving best model...\n",
      "0.24297716468572617 0.24447394907474518 current epoch 432: saving best model...\n",
      "0.24151298496872187 0.24297716468572617 current epoch 433: saving best model...\n",
      "0.2400520658120513 0.24151298496872187 current epoch 434: saving best model...\n",
      "0.23862590547651052 0.2400520658120513 current epoch 435: saving best model...\n",
      "0.2371914079412818 0.23862590547651052 current epoch 436: saving best model...\n",
      "0.23579894937574863 0.2371914079412818 current epoch 437: saving best model...\n",
      "0.23438410926610231 0.23579894937574863 current epoch 438: saving best model...\n",
      "0.23302137944847345 0.23438410926610231 current epoch 439: saving best model...\n",
      "0.23162543214857578 0.23302137944847345 current epoch 440: saving best model...\n",
      "0.23027043882757425 0.23162543214857578 current epoch 441: saving best model...\n",
      "0.2288782661780715 0.23027043882757425 current epoch 442: saving best model...\n",
      "0.2275291346013546 0.2288782661780715 current epoch 443: saving best model...\n",
      "0.22613952588289976 0.2275291346013546 current epoch 444: saving best model...\n",
      "0.22483716253191233 0.22613952588289976 current epoch 445: saving best model...\n",
      "0.22349420469254255 0.22483716253191233 current epoch 446: saving best model...\n",
      "0.22223041206598282 0.22349420469254255 current epoch 447: saving best model...\n",
      "0.22088742908090353 0.22223041206598282 current epoch 448: saving best model...\n",
      "0.21967358887195587 0.22088742908090353 current epoch 449: saving best model...\n",
      "0.21834294963628054 0.21967358887195587 current epoch 450: saving best model...\n",
      "0.2171624144539237 0.21834294963628054 current epoch 451: saving best model...\n",
      "0.2158206943422556 0.2171624144539237 current epoch 452: saving best model...\n",
      "0.21467774268239737 0.2158206943422556 current epoch 453: saving best model...\n",
      "0.2134025814011693 0.21467774268239737 current epoch 454: saving best model...\n",
      "0.2122589135542512 0.2134025814011693 current epoch 455: saving best model...\n",
      "0.21102771814912558 0.2122589135542512 current epoch 456: saving best model...\n",
      "0.20983423572033644 0.21102771814912558 current epoch 457: saving best model...\n",
      "0.20856855250895023 0.20983423572033644 current epoch 458: saving best model...\n",
      "0.20754887256771326 0.20856855250895023 current epoch 459: saving best model...\n",
      "0.20642354246228933 0.20754887256771326 current epoch 460: saving best model...\n",
      "0.20525838062167168 0.20642354246228933 current epoch 461: saving best model...\n",
      "0.20411508902907372 0.20525838062167168 current epoch 462: saving best model...\n",
      "0.2030156133696437 0.20411508902907372 current epoch 463: saving best model...\n",
      "0.20151134673506021 0.2030156133696437 current epoch 464: saving best model...\n",
      "0.20042186230421066 0.20151134673506021 current epoch 465: saving best model...\n",
      "0.19916428811848164 0.20042186230421066 current epoch 466: saving best model...\n",
      "0.19848985597491264 0.19916428811848164 current epoch 467: saving best model...\n",
      "0.19727058336138725 0.19848985597491264 current epoch 468: saving best model...\n",
      "0.19632912892848253 0.19727058336138725 current epoch 469: saving best model...\n",
      "0.19481991464272141 0.19632912892848253 current epoch 470: saving best model...\n",
      "0.1938669285736978 0.19481991464272141 current epoch 471: saving best model...\n",
      "0.19257135409861803 0.1938669285736978 current epoch 472: saving best model...\n",
      "0.19159540114924312 0.19257135409861803 current epoch 473: saving best model...\n",
      "0.19031093129888177 0.19159540114924312 current epoch 474: saving best model...\n",
      "0.1895426125265658 0.19031093129888177 current epoch 475: saving best model...\n",
      "0.1884731538593769 0.1895426125265658 current epoch 476: saving best model...\n",
      "0.18741935677826405 0.1884731538593769 current epoch 477: saving best model...\n",
      "0.1860059262253344 0.18741935677826405 current epoch 478: saving best model...\n",
      "0.18493913812562823 0.1860059262253344 current epoch 479: saving best model...\n",
      "0.18358824402093887 0.18493913812562823 current epoch 480: saving best model...\n",
      "0.18237957870587707 0.18358824402093887 current epoch 481: saving best model...\n",
      "0.18140833918005228 0.18237957870587707 current epoch 482: saving best model...\n",
      "0.1804480506107211 0.18140833918005228 current epoch 483: saving best model...\n",
      "0.17924277810379863 0.1804480506107211 current epoch 484: saving best model...\n",
      "0.1782352183945477 0.17924277810379863 current epoch 485: saving best model...\n",
      "0.17739206738770008 0.1782352183945477 current epoch 486: saving best model...\n",
      "0.1763559845276177 0.17739206738770008 current epoch 487: saving best model...\n",
      "0.17539016483351588 0.1763559845276177 current epoch 488: saving best model...\n",
      "0.1745216748677194 0.17539016483351588 current epoch 489: saving best model...\n",
      "0.17310422332957387 0.1745216748677194 current epoch 490: saving best model...\n",
      "0.17223568772897124 0.17310422332957387 current epoch 491: saving best model...\n",
      "0.17122539458796382 0.17223568772897124 current epoch 492: saving best model...\n",
      "0.1703021014109254 0.17122539458796382 current epoch 493: saving best model...\n",
      "0.16932267230004072 0.1703021014109254 current epoch 494: saving best model...\n",
      "0.16813847981393337 0.16932267230004072 current epoch 495: saving best model...\n",
      "0.16714712604880333 0.16813847981393337 current epoch 496: saving best model...\n",
      "0.1658522649668157 0.16714712604880333 current epoch 497: saving best model...\n",
      "0.16516547463834286 0.1658522649668157 current epoch 498: saving best model...\n",
      "0.16429967619478703 0.16516547463834286 current epoch 499: saving best model...\n",
      "Epoch: 500, loss after minibatch     1: 0.01412\n",
      "Epoch: 500, loss after minibatch     2: 0.01983\n",
      "Epoch: 500, loss after minibatch     3: 0.01832\n",
      "Epoch: 500, loss after minibatch     4: 0.01927\n",
      "Epoch: 500, loss after minibatch     5: 0.01843\n",
      "Epoch: 500, loss after minibatch     6: 0.01614\n",
      "Epoch: 500, loss after minibatch     7: 0.01740\n",
      "Epoch: 500, loss after minibatch     8: 0.01629\n",
      "Epoch: 500, loss after minibatch     9: 0.01722\n",
      "Epoch: 500, loss after minibatch    10: 0.00637\n",
      "+-------------+----------------------+--------+--------------------+\n",
      "|     Tag     |      Precision       | Recall |       FBeta        |\n",
      "+-------------+----------------------+--------+--------------------+\n",
      "|  publisher  |         1.0          |  1.0   |        1.0         |\n",
      "|   location  |         1.0          |  1.0   |        1.0         |\n",
      "| institution |         1.0          |  1.0   |        1.0         |\n",
      "|    other    |         1.0          |  1.0   |        1.0         |\n",
      "|    editor   | 0.010514665190924184 |  1.0   | 0.0208105147864184 |\n",
      "|    title    |         1.0          |  1.0   |        1.0         |\n",
      "|    pages    |         1.0          |  1.0   |        1.0         |\n",
      "|  booktitle  |         1.0          |  1.0   |        1.0         |\n",
      "|     tech    |         1.0          |  1.0   |        1.0         |\n",
      "|     note    |         1.0          |  1.0   |        1.0         |\n",
      "|    punct    | 0.06859919295067117  |  1.0   | 0.1283908754623921 |\n",
      "|    author   |         1.0          |  1.0   |        1.0         |\n",
      "|   journal   |         1.0          |  1.0   |        1.0         |\n",
      "|     date    |         1.0          |  1.0   |        1.0         |\n",
      "|    volume   |         1.0          |  1.0   |        1.0         |\n",
      "+-------------+----------------------+--------+--------------------+\n",
      "0.16338085662573576 0.16429967619478703 current epoch 500: saving best model...\n",
      "0.16299417056143284 0.16338085662573576 current epoch 501: saving best model...\n",
      "0.16290437476709485 0.16299417056143284 current epoch 502: saving best model...\n",
      "0.16165810730308294 0.16290437476709485 current epoch 503: saving best model...\n",
      "0.16043787589296699 0.16165810730308294 current epoch 504: saving best model...\n",
      "0.1598769254051149 0.16043787589296699 current epoch 505: saving best model...\n",
      "0.1595206973142922 0.1598769254051149 current epoch 506: saving best model...\n",
      "0.15781632158905268 0.1595206973142922 current epoch 507: saving best model...\n",
      "0.15748724667355418 0.15781632158905268 current epoch 508: saving best model...\n",
      "0.15730890491977334 0.15748724667355418 current epoch 509: saving best model...\n",
      "0.15625726338475943 0.15730890491977334 current epoch 510: saving best model...\n",
      "0.15546540776267648 0.15625726338475943 current epoch 511: saving best model...\n",
      "0.1550728641450405 0.15546540776267648 current epoch 512: saving best model...\n",
      "0.15471720229834318 0.1550728641450405 current epoch 513: saving best model...\n",
      "0.15451251855120063 0.15471720229834318 current epoch 514: saving best model...\n",
      "0.15383149357512593 0.15451251855120063 current epoch 515: saving best model...\n",
      "0.15282577089965343 0.15383149357512593 current epoch 516: saving best model...\n",
      "0.15252490947023034 0.15282577089965343 current epoch 517: saving best model...\n",
      "0.15169148286804557 0.15252490947023034 current epoch 518: saving best model...\n",
      "0.14489776082336903 0.15169148286804557 current epoch 569: saving best model...\n",
      "0.14038401283323765 0.14489776082336903 current epoch 570: saving best model...\n",
      "0.1356405015103519 0.14038401283323765 current epoch 571: saving best model...\n",
      "0.13381999777629972 0.1356405015103519 current epoch 572: saving best model...\n",
      "0.1315716770477593 0.13381999777629972 current epoch 573: saving best model...\n",
      "0.12963868444785476 0.1315716770477593 current epoch 574: saving best model...\n",
      "0.1282094269990921 0.12963868444785476 current epoch 575: saving best model...\n",
      "0.12669539265334606 0.1282094269990921 current epoch 576: saving best model...\n",
      "0.12534852977842093 0.12669539265334606 current epoch 577: saving best model...\n",
      "0.12413967214524746 0.12534852977842093 current epoch 578: saving best model...\n",
      "0.12297541089355946 0.12413967214524746 current epoch 579: saving best model...\n",
      "0.1218869467265904 0.12297541089355946 current epoch 580: saving best model...\n",
      "0.12085965322330594 0.1218869467265904 current epoch 581: saving best model...\n",
      "0.11987718939781189 0.12085965322330594 current epoch 582: saving best model...\n",
      "0.11893181642517447 0.11987718939781189 current epoch 583: saving best model...\n",
      "0.11802973318845034 0.11893181642517447 current epoch 584: saving best model...\n",
      "0.11715326411649585 0.11802973318845034 current epoch 585: saving best model...\n",
      "0.11630447814241052 0.11715326411649585 current epoch 586: saving best model...\n",
      "0.11547337658703327 0.11630447814241052 current epoch 587: saving best model...\n",
      "0.11467365827411413 0.11547337658703327 current epoch 588: saving best model...\n",
      "0.1139023955911398 0.11467365827411413 current epoch 589: saving best model...\n",
      "0.11315076565369964 0.1139023955911398 current epoch 590: saving best model...\n",
      "0.11241910373792052 0.11315076565369964 current epoch 591: saving best model...\n",
      "0.11169991688802838 0.11241910373792052 current epoch 592: saving best model...\n",
      "0.11099591292440891 0.11169991688802838 current epoch 593: saving best model...\n",
      "0.11030045244842768 0.11099591292440891 current epoch 594: saving best model...\n",
      "0.10961181297898293 0.11030045244842768 current epoch 595: saving best model...\n",
      "0.1089407168328762 0.10961181297898293 current epoch 596: saving best model...\n",
      "0.10827578231692314 0.1089407168328762 current epoch 597: saving best model...\n",
      "0.10761083569377661 0.10827578231692314 current epoch 598: saving best model...\n",
      "0.10691683506593108 0.10761083569377661 current epoch 599: saving best model...\n",
      "Epoch: 600, loss after minibatch     1: 0.00854\n",
      "Epoch: 600, loss after minibatch     2: 0.01277\n",
      "Epoch: 600, loss after minibatch     3: 0.01233\n",
      "Epoch: 600, loss after minibatch     4: 0.01217\n",
      "Epoch: 600, loss after minibatch     5: 0.01221\n",
      "Epoch: 600, loss after minibatch     6: 0.01057\n",
      "Epoch: 600, loss after minibatch     7: 0.01147\n",
      "Epoch: 600, loss after minibatch     8: 0.01143\n",
      "Epoch: 600, loss after minibatch     9: 0.01092\n",
      "Epoch: 600, loss after minibatch    10: 0.00383\n",
      "+-------------+----------------------+--------+---------------------+\n",
      "|     Tag     |      Precision       | Recall |        FBeta        |\n",
      "+-------------+----------------------+--------+---------------------+\n",
      "|  publisher  |         1.0          |  1.0   |         1.0         |\n",
      "|   location  |         1.0          |  1.0   |         1.0         |\n",
      "| institution |         1.0          |  1.0   |         1.0         |\n",
      "|    other    |         1.0          |  1.0   |         1.0         |\n",
      "|    editor   |  0.0407725321888412  |  1.0   | 0.07835051546391752 |\n",
      "|    title    |         1.0          |  1.0   |         1.0         |\n",
      "|    pages    |         1.0          |  1.0   |         1.0         |\n",
      "|  booktitle  |         1.0          |  1.0   |         1.0         |\n",
      "|     tech    |         1.0          |  1.0   |         1.0         |\n",
      "|     note    |         1.0          |  1.0   |         1.0         |\n",
      "|    punct    | 0.061776920795016316 |  1.0   | 0.11636516029894531 |\n",
      "|    author   |         1.0          |  1.0   |         1.0         |\n",
      "|   journal   |         1.0          |  1.0   |         1.0         |\n",
      "|     date    |         1.0          |  1.0   |         1.0         |\n",
      "|    volume   |         1.0          |  1.0   |         1.0         |\n",
      "+-------------+----------------------+--------+---------------------+\n",
      "0.10624707024544477 0.10691683506593108 current epoch 600: saving best model...\n",
      "0.10561172920279205 0.10624707024544477 current epoch 601: saving best model...\n",
      "0.10498428321443498 0.10561172920279205 current epoch 602: saving best model...\n",
      "0.10438411147333682 0.10498428321443498 current epoch 603: saving best model...\n",
      "0.10379471443593502 0.10438411147333682 current epoch 604: saving best model...\n",
      "0.1032029427587986 0.10379471443593502 current epoch 605: saving best model...\n",
      "0.10261500254273415 0.1032029427587986 current epoch 606: saving best model...\n",
      "0.10202009463682771 0.10261500254273415 current epoch 607: saving best model...\n",
      "0.10143789881840348 0.10202009463682771 current epoch 608: saving best model...\n",
      "0.10085240914486349 0.10143789881840348 current epoch 609: saving best model...\n",
      "0.10023702820762992 0.10085240914486349 current epoch 610: saving best model...\n",
      "0.09962254692800343 0.10023702820762992 current epoch 611: saving best model...\n",
      "0.09904551855288446 0.09962254692800343 current epoch 612: saving best model...\n",
      "0.09850048646330833 0.09904551855288446 current epoch 613: saving best model...\n",
      "0.09797916002571583 0.09850048646330833 current epoch 614: saving best model...\n",
      "0.09745720564387739 0.09797916002571583 current epoch 615: saving best model...\n",
      "0.09693432063795626 0.09745720564387739 current epoch 616: saving best model...\n",
      "0.09642251953482628 0.09693432063795626 current epoch 617: saving best model...\n",
      "0.09591506165452302 0.09642251953482628 current epoch 618: saving best model...\n",
      "0.0954123823903501 0.09591506165452302 current epoch 619: saving best model...\n",
      "0.09491482586599886 0.0954123823903501 current epoch 620: saving best model...\n",
      "0.09441898460499942 0.09491482586599886 current epoch 621: saving best model...\n",
      "0.09390913438983262 0.09441898460499942 current epoch 622: saving best model...\n",
      "0.09341198857873678 0.09390913438983262 current epoch 623: saving best model...\n",
      "0.09293230017647147 0.09341198857873678 current epoch 624: saving best model...\n",
      "0.09244472556747496 0.09293230017647147 current epoch 625: saving best model...\n",
      "0.09194193920120597 0.09244472556747496 current epoch 626: saving best model...\n",
      "0.0914736685808748 0.09194193920120597 current epoch 627: saving best model...\n",
      "0.09100925596430898 0.0914736685808748 current epoch 628: saving best model...\n",
      "0.090545924147591 0.09100925596430898 current epoch 629: saving best model...\n",
      "0.09008429991081357 0.090545924147591 current epoch 630: saving best model...\n",
      "0.08962930785492063 0.09008429991081357 current epoch 631: saving best model...\n",
      "0.08916797675192356 0.08962930785492063 current epoch 632: saving best model...\n",
      "0.08871180447749794 0.08916797675192356 current epoch 633: saving best model...\n",
      "0.08844828489236534 0.08871180447749794 current epoch 634: saving best model...\n",
      "0.08800730691291392 0.08844828489236534 current epoch 635: saving best model...\n",
      "0.08754002093337476 0.08800730691291392 current epoch 636: saving best model...\n",
      "0.08703340799547732 0.08754002093337476 current epoch 637: saving best model...\n",
      "0.08657554374076426 0.08703340799547732 current epoch 638: saving best model...\n",
      "0.08614880149252713 0.08657554374076426 current epoch 639: saving best model...\n",
      "0.08569418708793819 0.08614880149252713 current epoch 640: saving best model...\n",
      "0.08525782148353755 0.08569418708793819 current epoch 641: saving best model...\n",
      "0.0848325863480568 0.08525782148353755 current epoch 642: saving best model...\n",
      "0.08439236436970532 0.0848325863480568 current epoch 643: saving best model...\n",
      "0.08398446021601558 0.08439236436970532 current epoch 644: saving best model...\n",
      "0.08357621659524739 0.08398446021601558 current epoch 645: saving best model...\n",
      "0.083163175964728 0.08357621659524739 current epoch 646: saving best model...\n",
      "0.08274829294532537 0.083163175964728 current epoch 647: saving best model...\n",
      "0.0823456128127873 0.08274829294532537 current epoch 648: saving best model...\n",
      "0.08193566161207855 0.0823456128127873 current epoch 649: saving best model...\n",
      "0.08153814519755542 0.08193566161207855 current epoch 650: saving best model...\n",
      "0.0811140276491642 0.08153814519755542 current epoch 651: saving best model...\n",
      "0.08069229940883815 0.0811140276491642 current epoch 652: saving best model...\n",
      "0.08027878729626536 0.08069229940883815 current epoch 653: saving best model...\n",
      "0.08007037523202598 0.08027878729626536 current epoch 654: saving best model...\n",
      "0.07957747066393495 0.08007037523202598 current epoch 655: saving best model...\n",
      "0.07911349134519696 0.07957747066393495 current epoch 656: saving best model...\n",
      "0.07869950216263533 0.07911349134519696 current epoch 657: saving best model...\n",
      "0.07827569940127432 0.07869950216263533 current epoch 658: saving best model...\n",
      "0.07786939269863069 0.07827569940127432 current epoch 659: saving best model...\n",
      "0.07750364253297448 0.07786939269863069 current epoch 660: saving best model...\n",
      "0.07742559630423784 0.07750364253297448 current epoch 661: saving best model...\n",
      "0.07712493813596666 0.07742559630423784 current epoch 662: saving best model...\n",
      "0.07672096835449338 0.07712493813596666 current epoch 663: saving best model...\n",
      "0.0763588531408459 0.07672096835449338 current epoch 664: saving best model...\n",
      "0.0759410539176315 0.0763588531408459 current epoch 665: saving best model...\n",
      "0.07554085063748062 0.0759410539176315 current epoch 666: saving best model...\n",
      "0.0751387879718095 0.07554085063748062 current epoch 667: saving best model...\n",
      "0.07475969544611871 0.0751387879718095 current epoch 668: saving best model...\n",
      "0.07433841121383011 0.07475969544611871 current epoch 669: saving best model...\n",
      "0.0739253368228674 0.07433841121383011 current epoch 670: saving best model...\n",
      "0.07349378755316138 0.0739253368228674 current epoch 671: saving best model...\n",
      "0.07312082219868898 0.07349378755316138 current epoch 672: saving best model...\n",
      "0.07272239751182497 0.07312082219868898 current epoch 673: saving best model...\n",
      "0.07231965265236795 0.07272239751182497 current epoch 674: saving best model...\n",
      "0.07197645725682378 0.07231965265236795 current epoch 675: saving best model...\n",
      "0.07166369562037289 0.07197645725682378 current epoch 676: saving best model...\n",
      "0.07129373773932457 0.07166369562037289 current epoch 677: saving best model...\n",
      "0.07086957129649818 0.07129373773932457 current epoch 678: saving best model...\n",
      "0.07059366791509092 0.07086957129649818 current epoch 679: saving best model...\n",
      "0.07032134523615241 0.07059366791509092 current epoch 680: saving best model...\n",
      "0.06899903854355216 0.07032134523615241 current epoch 686: saving best model...\n",
      "0.06813110806979239 0.06899903854355216 current epoch 687: saving best model...\n",
      "0.06759178126230836 0.06813110806979239 current epoch 688: saving best model...\n",
      "0.0670791040174663 0.06759178126230836 current epoch 689: saving best model...\n",
      "0.06665964145213366 0.0670791040174663 current epoch 690: saving best model...\n",
      "0.06614897027611732 0.06665964145213366 current epoch 691: saving best model...\n",
      "0.06586334109306335 0.06614897027611732 current epoch 692: saving best model...\n",
      "0.06545210257172585 0.06586334109306335 current epoch 693: saving best model...\n",
      "0.06510378140956163 0.06545210257172585 current epoch 694: saving best model...\n",
      "0.0646998065058142 0.06510378140956163 current epoch 695: saving best model...\n",
      "0.06436056550592184 0.0646998065058142 current epoch 696: saving best model...\n",
      "0.0640261466614902 0.06436056550592184 current epoch 697: saving best model...\n",
      "0.06369279068894684 0.0640261466614902 current epoch 698: saving best model...\n",
      "0.06332803796976805 0.06369279068894684 current epoch 699: saving best model...\n",
      "Epoch: 700, loss after minibatch     1: 0.00498\n",
      "Epoch: 700, loss after minibatch     2: 0.00732\n",
      "Epoch: 700, loss after minibatch     3: 0.00729\n",
      "Epoch: 700, loss after minibatch     4: 0.00694\n",
      "Epoch: 700, loss after minibatch     5: 0.00733\n",
      "Epoch: 700, loss after minibatch     6: 0.00638\n",
      "Epoch: 700, loss after minibatch     7: 0.00711\n",
      "Epoch: 700, loss after minibatch     8: 0.00712\n",
      "Epoch: 700, loss after minibatch     9: 0.00633\n",
      "Epoch: 700, loss after minibatch    10: 0.00220\n",
      "+-------------+----------------------+--------+---------------------+\n",
      "|     Tag     |      Precision       | Recall |        FBeta        |\n",
      "+-------------+----------------------+--------+---------------------+\n",
      "|  publisher  |         1.0          |  1.0   |         1.0         |\n",
      "|   location  |         1.0          |  1.0   |         1.0         |\n",
      "| institution |         1.0          |  1.0   |         1.0         |\n",
      "|    other    |         1.0          |  1.0   |         1.0         |\n",
      "|    editor   | 0.005875077303648732 |  1.0   | 0.01168152474638795 |\n",
      "|    title    |         1.0          |  1.0   |         1.0         |\n",
      "|    pages    |         1.0          |  1.0   |         1.0         |\n",
      "|  booktitle  |         1.0          |  1.0   |         1.0         |\n",
      "|     tech    |         1.0          |  1.0   |         1.0         |\n",
      "|     note    |         1.0          |  1.0   |         1.0         |\n",
      "|    punct    | 0.07773422918999627  |  1.0   | 0.14425491384535458 |\n",
      "|    author   |         1.0          |  1.0   |         1.0         |\n",
      "|   journal   |         1.0          |  1.0   |         1.0         |\n",
      "|     date    |         1.0          |  1.0   |         1.0         |\n",
      "|    volume   |         1.0          |  1.0   |         1.0         |\n",
      "+-------------+----------------------+--------+---------------------+\n",
      "0.06300805509090424 0.06332803796976805 current epoch 700: saving best model...\n",
      "0.06266175000928342 0.06300805509090424 current epoch 701: saving best model...\n",
      "0.062333302572369576 0.06266175000928342 current epoch 702: saving best model...\n",
      "0.06198265892453492 0.062333302572369576 current epoch 703: saving best model...\n",
      "0.061643457505851984 0.06198265892453492 current epoch 704: saving best model...\n",
      "0.061303096590563655 0.061643457505851984 current epoch 705: saving best model...\n",
      "0.06098956521600485 0.061303096590563655 current epoch 706: saving best model...\n",
      "0.06066346541047096 0.06098956521600485 current epoch 707: saving best model...\n",
      "0.060391227481886744 0.06066346541047096 current epoch 708: saving best model...\n",
      "0.06002147193066776 0.060391227481886744 current epoch 709: saving best model...\n",
      "0.05976190697401762 0.06002147193066776 current epoch 710: saving best model...\n",
      "0.05947908200323582 0.05976190697401762 current epoch 711: saving best model...\n",
      "0.0590174978133291 0.05947908200323582 current epoch 712: saving best model...\n",
      "0.05866554286330938 0.0590174978133291 current epoch 713: saving best model...\n",
      "0.058326571714133024 0.05866554286330938 current epoch 714: saving best model...\n",
      "0.058002821169793606 0.058326571714133024 current epoch 715: saving best model...\n",
      "0.057651537703350186 0.058002821169793606 current epoch 716: saving best model...\n",
      "0.05730187497101724 0.057651537703350186 current epoch 717: saving best model...\n",
      "0.05702219787053764 0.05730187497101724 current epoch 718: saving best model...\n",
      "0.05671028792858124 0.05702219787053764 current epoch 719: saving best model...\n",
      "0.056356856133788824 0.05671028792858124 current epoch 720: saving best model...\n",
      "0.056065233307890594 0.056356856133788824 current epoch 721: saving best model...\n",
      "0.05569444189313799 0.056065233307890594 current epoch 722: saving best model...\n",
      "0.05539098451845348 0.05569444189313799 current epoch 723: saving best model...\n",
      "0.05503167933784425 0.05539098451845348 current epoch 724: saving best model...\n",
      "0.054783021681942046 0.05503167933784425 current epoch 725: saving best model...\n",
      "0.05353441508486867 0.054783021681942046 current epoch 736: saving best model...\n",
      "0.05250377033371478 0.05353441508486867 current epoch 737: saving best model...\n",
      "0.051525325048714876 0.05250377033371478 current epoch 738: saving best model...\n",
      "0.05077338695991784 0.051525325048714876 current epoch 739: saving best model...\n",
      "0.05030372296459973 0.05077338695991784 current epoch 740: saving best model...\n",
      "0.049823363428004086 0.05030372296459973 current epoch 741: saving best model...\n",
      "0.04941139533184469 0.049823363428004086 current epoch 742: saving best model...\n",
      "0.04904917743988335 0.04941139533184469 current epoch 743: saving best model...\n",
      "0.0487094639101997 0.04904917743988335 current epoch 744: saving best model...\n",
      "0.048337022541090846 0.0487094639101997 current epoch 745: saving best model...\n",
      "0.047984589473344386 0.048337022541090846 current epoch 746: saving best model...\n",
      "0.04766847740393132 0.047984589473344386 current epoch 747: saving best model...\n",
      "0.04732032655738294 0.04766847740393132 current epoch 748: saving best model...\n",
      "0.04703534080181271 0.04732032655738294 current epoch 749: saving best model...\n",
      "0.04673296818509698 0.04703534080181271 current epoch 750: saving best model...\n",
      "0.04646713985130191 0.04673296818509698 current epoch 751: saving best model...\n",
      "0.046178757678717375 0.04646713985130191 current epoch 752: saving best model...\n",
      "0.04590133565943688 0.046178757678717375 current epoch 753: saving best model...\n",
      "0.04561548389028758 0.04590133565943688 current epoch 754: saving best model...\n",
      "0.04533965035807341 0.04561548389028758 current epoch 755: saving best model...\n",
      "0.04498294310178608 0.04533965035807341 current epoch 756: saving best model...\n",
      "0.04470966861117631 0.04498294310178608 current epoch 757: saving best model...\n",
      "0.044435598072595894 0.04470966861117631 current epoch 758: saving best model...\n",
      "0.04415248706936836 0.044435598072595894 current epoch 759: saving best model...\n",
      "0.043881646008230746 0.04415248706936836 current epoch 760: saving best model...\n",
      "0.04363977548200637 0.043881646008230746 current epoch 761: saving best model...\n",
      "0.04342251643538475 0.04363977548200637 current epoch 762: saving best model...\n",
      "0.04320727940648794 0.04342251643538475 current epoch 763: saving best model...\n",
      "0.042979412130080163 0.04320727940648794 current epoch 764: saving best model...\n",
      "0.04273475578520447 0.042979412130080163 current epoch 765: saving best model...\n",
      "0.04247574135661125 0.04273475578520447 current epoch 766: saving best model...\n",
      "0.04221511026844382 0.04247574135661125 current epoch 767: saving best model...\n",
      "0.0418996661901474 0.04221511026844382 current epoch 768: saving best model...\n",
      "0.04167312721256167 0.0418996661901474 current epoch 769: saving best model...\n",
      "0.041477045393548906 0.04167312721256167 current epoch 770: saving best model...\n",
      "0.041256582015194 0.041477045393548906 current epoch 771: saving best model...\n",
      "0.04101052798796445 0.041256582015194 current epoch 772: saving best model...\n",
      "0.04075239098165184 0.04101052798796445 current epoch 773: saving best model...\n",
      "0.04051557520870119 0.04075239098165184 current epoch 774: saving best model...\n",
      "0.0403774461010471 0.04051557520870119 current epoch 775: saving best model...\n",
      "0.040346087072975934 0.0403774461010471 current epoch 776: saving best model...\n",
      "0.04031198809389025 0.040346087072975934 current epoch 780: saving best model...\n",
      "0.04021765373181552 0.04031198809389025 current epoch 781: saving best model...\n",
      "0.040111565380357206 0.04021765373181552 current epoch 782: saving best model...\n",
      "0.03999127389397472 0.040111565380357206 current epoch 783: saving best model...\n",
      "\n",
      "+-------------+---------------------+---------------------+---------------------+\n",
      "|     Tag     |      Precision      |        Recall       |        FBeta        |\n",
      "+-------------+---------------------+---------------------+---------------------+\n",
      "|  publisher  |       0.58125       |  0.5568862275449101 |  0.5688073394495412 |\n",
      "|   location  |  0.7484276729559748 |  0.7933333333333333 |  0.7702265372168285 |\n",
      "| institution |  0.5476190476190477 |  0.6133333333333333 |  0.5786163522012578 |\n",
      "|    other    |  0.7124711316397229 |  0.7141203703703703 |  0.7132947976878613 |\n",
      "|    editor   |  0.7747747747747747 |         0.5         |  0.607773851590106  |\n",
      "|    title    |  0.9507515190278222 |  0.9486279514996809 |  0.9496885481552467 |\n",
      "|    pages    |  0.9055415617128464 |  0.9374185136897001 |  0.9212043561819345 |\n",
      "|  booktitle  |  0.738255033557047  |  0.7768361581920904 |  0.7570543702684103 |\n",
      "|     tech    |  0.6896551724137931 |  0.5882352941176471 |  0.6349206349206349 |\n",
      "|     note    | 0.09523809523809523 | 0.05405405405405406 | 0.06896551724137931 |\n",
      "|    punct    |  0.9981703260146374 |  0.9973408675419644 |  0.9977554243910548 |\n",
      "|    author   |  0.9496336996336996 |  0.9464557347125038 |  0.9480420539387475 |\n",
      "|   journal   |  0.8372513562386981 |  0.8518859245630175 |  0.8445052439580483 |\n",
      "|     date    |  0.9104704097116844 |  0.9287925696594427 |  0.9195402298850573 |\n",
      "|    volume   |  0.8524945770065075 |  0.8543478260869565 |  0.8534201954397395 |\n",
      "+-------------+---------------------+---------------------+---------------------+\n",
      "Test F1 macro score: 0.7422543635017231\n",
      "Test F1 micro score: 0.9227280458075854\n",
      "[['0.55689', '0.04790', '0.06587', '0.04790', '0.02395', '0.00599', '0.01796', '0.12575', '0.00599', '0.00599', '0.00000', '0.01198', '0.05389', '0.02994', '0.00000'], ['0.06000', '0.79333', '0.03333', '0.02000', '0.00667', '0.00000', '0.02000', '0.04667', '0.00000', '0.00000', '0.00000', '0.00667', '0.00667', '0.00000', '0.00667'], ['0.14667', '0.02667', '0.61333', '0.04000', '0.01333', '0.00000', '0.00000', '0.02667', '0.00000', '0.00000', '0.00000', '0.00000', '0.12000', '0.01333', '0.00000'], ['0.01157', '0.00463', '0.00810', '0.71412', '0.01273', '0.04282', '0.02546', '0.06829', '0.00116', '0.00347', '0.00000', '0.06366', '0.01389', '0.01042', '0.01968'], ['0.02326', '0.01163', '0.00000', '0.05233', '0.50000', '0.00581', '0.00581', '0.05814', '0.00000', '0.00000', '0.00000', '0.26744', '0.05233', '0.02326', '0.00000'], ['0.00096', '0.00160', '0.00000', '0.00670', '0.00000', '0.94863', '0.00000', '0.00862', '0.00255', '0.00032', '0.00223', '0.01468', '0.01244', '0.00064', '0.00064'], ['0.00261', '0.00000', '0.00000', '0.01956', '0.00000', '0.00000', '0.93742', '0.00261', '0.00130', '0.00130', '0.00000', '0.00000', '0.00000', '0.00130', '0.03390'], ['0.01554', '0.01130', '0.01271', '0.03955', '0.00989', '0.04096', '0.00706', '0.77684', '0.00141', '0.00565', '0.00282', '0.00282', '0.06215', '0.00706', '0.00424'], ['0.01471', '0.00000', '0.01471', '0.17647', '0.00000', '0.00000', '0.00000', '0.00000', '0.58824', '0.04412', '0.00000', '0.00000', '0.14706', '0.01471', '0.00000'], ['0.05405', '0.00000', '0.02703', '0.48649', '0.00000', '0.00000', '0.00000', '0.10811', '0.05405', '0.05405', '0.02703', '0.00000', '0.10811', '0.08108', '0.00000'], ['0.00000', '0.00000', '0.00000', '0.00017', '0.00000', '0.00083', '0.00066', '0.00083', '0.00000', '0.00000', '0.99734', '0.00000', '0.00000', '0.00017', '0.00000'], ['0.00061', '0.00061', '0.00000', '0.01917', '0.00000', '0.01278', '0.00243', '0.00000', '0.00000', '0.00000', '0.00030', '0.94646', '0.00852', '0.00669', '0.00243'], ['0.00920', '0.00092', '0.00276', '0.02944', '0.00092', '0.02760', '0.00368', '0.04600', '0.00092', '0.00552', '0.00000', '0.01012', '0.85189', '0.00368', '0.00736'], ['0.00000', '0.01084', '0.00155', '0.01858', '0.00000', '0.01084', '0.00619', '0.00619', '0.00310', '0.00000', '0.00000', '0.00310', '0.00619', '0.92879', '0.00464'], ['0.00435', '0.00217', '0.00000', '0.05217', '0.00000', '0.00435', '0.04565', '0.00870', '0.00217', '0.00000', '0.00000', '0.00000', '0.02391', '0.00217', '0.85435']]\n",
      "[[, punct, punct] [5, other, other] [], punct, punct] [P., author, author] [J., author, author] [Steinhardt, author, author] [and, author, author] [F., author, author] [S., author, author] [Accetta, author, author] [,, punct, punct] [Phys, journal, journal] [., punct, punct] [Rev, journal, journal] [., punct, punct] [Lett, journal, journal] [., punct, punct] [64, volume, volume] [,, punct, punct] [2740, pages, pages] [(, punct, punct] [1990, date, date] [), punct, punct] [., punct, punct] \n",
      "[[, punct, punct] [29, other, other] [], punct, punct] [S., author, author] [M., author, author] [Salamon, author, author] [,, punct, punct] [Quaternionic, title, title] [structures, title, title] [and, title, title] [twistor, title, title] [spaces, title, title] [,, punct, punct] [in, booktitle, other] [Global, booktitle, booktitle] [Riemannian, booktitle, booktitle] [geometry, booktitle, booktitle] \n",
      "[[, punct, punct] [AxKo, other, other] [], punct, punct] [J, author, author] [., punct, punct] [Ax, author, title] [and, author, author] [S., author, author] [Kochen, author, author] [,, punct, punct] [``, title, title] [Diophantine, title, title] [problems, title, title] [over, title, title] [local, title, title] [rings, title, title] [I, title, title] [,, punct, punct] [``, title, title] [Amer, journal, journal] [., punct, punct] [J, journal, journal] [., punct, punct] [Math, journal, journal] [., punct, punct] [87, volume, volume] [(, punct, punct] [1965, date, date] [), punct, punct] [,, punct, punct] [605-630, pages, pages] [., punct, punct] \n",
      "[Danvy, author, author] [,, punct, punct] [O., author, author] [and, author, author] [A., author, author] [Filinski, author, author] [., punct, punct] [Representing, title, title] [control, title, title] [:, punct, punct] [A, title, title] [study, title, title] [of, title, title] [the, title, title] [CPS, title, title] [transformation, title, title] [., punct, punct] [Tech, tech, tech] [., punct, punct] [Rpt, tech, tech] [., punct, punct] [CIS-91-2, tech, other] [., punct, punct] [Kansas, institution, other] [State, institution, publisher] [University, institution, publisher] [,, punct, punct] [1991, date, date] [., punct, punct] \n",
      "[[, punct, punct] [11, other, other] [], punct, punct] [Keller, author, author] [G., other, author] [Stochastic, title, author] [stability, title, title] [in, title, title] [some, title, title] [chaotic, title, title] [dynamical, title, title] [systems, title, title] [,, punct, punct] [Mh, journal, journal] [., punct, punct] [Math, journal, journal] [., punct, punct] [,, punct, punct] [94, volume, volume] [(, punct, punct] [1982, date, date] [), punct, punct] [,, punct, punct] [313-333, pages, pages] [., punct, punct] \n",
      "[[, punct, punct] [6, other, other] [], punct, punct] [Chen, author, author] [,, punct, punct] [J, author, author] [., punct, punct] [(, punct, punct] [1992, date, date] [), punct, punct] [., punct, punct] [Some, title, title] [results, title, title] [on, title, title] [2, title, title] [n-, title, title] [k, title, title] [fractional, title, title] [factorial, title, title] [designs, title, title] [and, title, title] [search, title, title] [for, title, title] [minimum, title, title] [aberration, title, title] [designs, title, title] [., punct, punct] [Ann, journal, journal] [., punct, punct] [Statist, journal, journal] [., punct, punct] [20, volume, volume] [2124, pages, volume] [--, pages, pages] [2141, pages, pages] [., punct, punct] [MR1193330, other, other] \n",
      "[Yao, author, author] [,, punct, punct] [A, author, author] [., punct, punct] [(, punct, punct] [1982, date, date] [), punct, punct] [,, punct, punct] [Protocols, title, title] [for, title, title] [secure, title, title] [computations, title, title] [,, punct, punct] [in, other, other] [`, punct, punct] [FOCS, booktitle, booktitle] [1982, booktitle, date] [:, punct, punct] [Proceedings, booktitle, booktitle] [of, booktitle, booktitle] [23rd, booktitle, booktitle] [Annual, booktitle, booktitle] [Symposium, booktitle, booktitle] [on, booktitle, booktitle] [Foundations, booktitle, booktitle] [of, booktitle, booktitle] [Computer, booktitle, booktitle] [Science, booktitle, booktitle] [', punct, booktitle] [,, punct, punct] [IEEE, other, location] [Computer, other, booktitle] [Society, other, publisher] [,, punct, punct] [pp, pages, pages] [., punct, punct] [160, pages, pages] [--, pages, pages] [164, pages, pages] [., punct, punct] [doi:10.1109/SFCS.1982.38, other, other] \n",
      "[Barabasi, author, author] [,, punct, punct] [A., author, author] [L., author, author] [,, punct, punct] [&, punct, punct] [Albert, author, author] [,, punct, punct] [R., other, author] [(, punct, punct] [1999, date, date] [), punct, punct] [., punct, punct] [Emergence, title, title] [of, title, title] [scaling, title, title] [in, title, title] [random, title, title] [networks, title, title] [., punct, punct] [Science, journal, journal] [,, punct, punct] [286, volume, volume] [,, punct, punct] [509, pages, pages] [--, pages, pages] [512, pages, pages] [., punct, punct] \n",
      "[32, other, other] [., punct, punct] [S., author, author] [Capozziello, author, author] [,, punct, punct] [G., author, author] [Iovane, author, author] [,, punct, punct] [G., author, author] [Lambiase, author, author] [,, punct, punct] [and, author, author] [C., author, author] [Stornaiolo, author, author] [,, punct, punct] [Europhys, journal, journal] [., punct, punct] [Lett, journal, journal] [., punct, punct] [46, volume, volume] [(, punct, punct] [1999, date, date] [), punct, punct] [710, pages, pages] [., punct, punct] \n",
      "[[, punct, punct] [14, other, other] [], punct, punct] [,, punct, punct] [Formal, title, title] [languages, title, title] [and, title, title] [global, title, title] [cellular, title, title] [automaton, title, title] [behavior, title, title] [,, punct, punct] [In, other, other] [Gutowitz, other, other] [[, punct, punct] [6, other, other] [], punct, punct] [., punct, punct] \n",
      "[[, punct, punct] [12, other, other] [], punct, punct] [K., author, author] [Bernl¨ohr, author, author] [et, author, author] [al, author, author] [., punct, punct] [,, punct, punct] [Nucl, journal, journal] [., punct, punct] [Instr, journal, journal] [., punct, punct] [and, journal, journal] [Meth, journal, journal] [., punct, punct] [A, journal, journal] [369, volume, volume] [(, punct, punct] [1996, date, date] [), punct, punct] [293, pages, pages] [., punct, punct] \n",
      "[G., author, author] [Moore, author, author] [and, author, author] [N., author, author] [Seiberg, author, author] [,, punct, punct] [Phys.Lett, journal, journal] [., punct, punct] [B, journal, journal] [220, volume, volume] [(, punct, punct] [1989, date, date] [), punct, punct] [422, pages, pages] [;, punct, punct] \n",
      "[[, punct, punct] [10, other, other] [], punct, punct] [Conlon, author, author] [,, punct, punct] [T., author, author] [,, punct, punct] [Crane, author, author] [,, punct, punct] [M., author, author] [,, punct, punct] [Ruskin, author, author] [,, punct, punct] [H., author, author] [J., author, author] [,, punct, punct] [Physica, journal, journal] [A, journal, journal] [387, volume, volume] [(, punct, punct] [21, other, pages] [), punct, punct] [(, punct, punct] [2008, date, date] [), punct, punct] [5197-5204, pages, pages] [., punct, punct] \n",
      "[[, punct, punct] [G-N-N, other, other] [], punct, punct] [B., author, author] [Gidas, author, author] [,, punct, punct] [W., author, author] [-M, author, author] [., punct, punct] [Ni, author, author] [,, punct, punct] [and, author, author] [L., author, author] [Nirenberg, author, author] [,, punct, punct] [Symmetry, title, title] [of, title, title] [positive, title, title] [solutions, title, title] [of, title, title] [nonlinear, title, title] [elliptic, title, title] [equations, title, title] [in, title, title] [R, title, title] [n, title, title] [,, punct, punct] [Adv, journal, journal] [., punct, punct] [in, journal, journal] [Math, journal, journal] [., punct, punct] [Supplementary, journal, journal] [Stud, journal, journal] [., punct, punct] [7A, volume, volume] [(, punct, punct] [1981, date, date] [), punct, punct] [pp, pages, pages] [369, pages, pages] [--, pages, pages] [402, pages, pages] \n",
      "[L., author, author] [G., author, author] [Valiant, author, author] [., punct, punct] [A, title, title] [Bridging, title, title] [Model, title, title] [for, title, title] [Parallel, title, title] [Computation, title, title] [., punct, punct] [Communications, journal, journal] [of, journal, journal] [the, journal, journal] [ACM, journal, journal] [,, punct, punct] [33, volume, volume] [(, punct, punct] [8, volume, volume] [), punct, punct] [,, punct, punct] [103-111, pages, pages] [,, punct, punct] [1990, date, date] [., punct, punct] \n",
      "[Neistein, author, author] [E., author, author] [,, punct, punct] [Khochfar, author, author] [S., author, author] [,, punct, punct] [Dalla, author, author] [Vecchia, author, author] [C., author, author] [,, punct, punct] [Schaye, author, author] [S., author, author] [,, punct, punct] [2011, date, date] [b, date, date] [,, punct, punct] [preprint, tech, tech] [,, punct, punct] [arXiv:1109.4635, other, other] \n",
      "[[, punct, punct] [14, other, other] [], punct, punct] [Anteneodo, author, author] [C., author, author] [,, punct, punct] [Tsallis, author, author] [C., author, author] [,, punct, punct] [J, journal, author] [., punct, punct] [Math, journal, journal] [., punct, punct] [Phys, journal, journal] [., punct, punct] [,, punct, punct] [44, volume, volume] [(, punct, punct] [2003, date, date] [), punct, punct] [5194, pages, pages] [;, punct, punct] \n",
      "[[, punct, punct] [30, other, other] [], punct, punct] [Siciak, author, author] [J., author, author] [,, punct, punct] [On, title, title] [some, title, title] [extremal, title, title] [functions, title, title] [and, title, title] [their, title, title] [applications, title, title] [in, title, title] [the, title, title] [theory, title, title] [of, title, title] [analytic, title, title] [functions, title, title] [of, title, title] [several, title, title] [complex, title, title] [variables, title, title] [,, punct, punct] [Trans, journal, booktitle] [., punct, punct] [Amer, journal, journal] [., punct, punct] [Math, journal, booktitle] [., punct, punct] [Soc, journal, other] [., punct, punct] [,, punct, punct] [105, volume, pages] [(, punct, punct] [1962, date, date] [), punct, punct] [,, punct, punct] [322-357, pages, pages] [., punct, punct] \n",
      "[[, punct, punct] [7, other, other] [], punct, punct] [S., author, author] [Maslov, author, author] [,, punct, punct] [Y, author, author] [., punct, punct] [-C, author, author] [., punct, punct] [Zhang, author, author] [,, punct, punct] [International, journal, journal] [Journal, journal, journal] [of, journal, journal] [Theoretical, journal, journal] [and, journal, journal] [Applied, journal, journal] [Finance, journal, journal] [1, volume, volume] [,, punct, punct] [1998, date, date] [,, punct, punct] [377, pages, pages] [--, pages, pages] [387, pages, pages] \n",
      "[Vander, author, author] [Linden, author, author] [,, punct, punct] [K., author, author] [;, punct, punct] [Cumming, author, author] [,, punct, punct] [S., author, other] [;, punct, punct] [and, author, author] [Martin, author, author] [,, punct, punct] [J, author, author] [., punct, punct] [1992, date, date] [., punct, punct] [Expressing, title, title] [local, title, title] [rhetorical, title, title] [relations, title, title] [in, title, title] [instructional, title, title] [text, title, title] [., punct, punct] [Technical, tech, tech] [Report, tech, tech] [92-43, tech, tech] [,, punct, punct] [University, institution, institution] [of, institution, institution] [Colorado, institution, institution] [., punct, punct] [To, note, institution] [appear, note, tech] [in, note, booktitle] [Computational, note, booktitle] [Linguistics, note, booktitle] [., punct, punct] \n",
      "[Lyne, author, author] [A., author, author] [,, punct, punct] [Ritchings, author, author] [R., author, author] [,, punct, punct] [Smith, author, author] [F., author, author] [,, punct, punct] [1975, date, date] [,, punct, punct] [MNRAS, journal, journal] [,, punct, punct] [171, volume, volume] [,, punct, punct] [579, pages, pages] \n",
      "[[, punct, punct] [KK, other, other] [], punct, punct] [S-J, author, author] [., punct, punct] [Kang, author, author] [and, author, author] [M., author, author] [Kashiwara, author, author] [,, punct, punct] [Quantized, title, journal] [affine, title, title] [algebras, title, title] [and, title, title] [crystals, title, title] [with, title, title] [core, title, title] [,, punct, punct] [Commun, journal, journal] [., punct, punct] [Math, journal, journal] [., punct, punct] [Phys, journal, journal] [., punct, punct] [195, volume, journal] [(, punct, punct] [1998, date, date] [), punct, punct] [725-740, pages, other] [., punct, punct] \n",
      "[Reichman, author, author] [,, punct, punct] [R., other, other] [(, punct, punct] [1985, date, date] [), punct, punct] [., punct, punct] [Getting, title, title] [Computers, title, title] [to, title, title] [Talk, title, title] [Like, title, title] [You, title, title] [and, title, title] [Me, title, title] [:, punct, punct] [Discourse, title, title] [Context, title, title] [,, punct, punct] [Focus, title, publisher] [,, punct, punct] [and, title, location] [Semantics, title, location] [., punct, punct] [Cambridge, location, location] [,, punct, punct] [MA, location, location] [:, punct, punct] [MIT, publisher, publisher] [Press, publisher, publisher] [., punct, punct] \n",
      "[[, punct, punct] [16, other, other] [], punct, punct] [S., author, author] [Browne, author, author] [,, punct, punct] [W., author, author] [Whitt, author, author] [,, punct, punct] [Adv, journal, author] [., punct, punct] [Appl, journal, journal] [., punct, punct] [Prob, journal, booktitle] [., punct, punct] [28, volume, volume] [,, punct, punct] [1996, date, date] [,, punct, punct] [1145, pages, pages] [--, pages, pages] [1176, pages, pages] \n",
      "[[, punct, punct] [34, other, other] [], punct, punct] [C., author, author] [J., author, author] [Skinner, author, author] [and, author, author] [M., author, author] [J., author, author] [Elliot, author, author] [,, punct, punct] [A, title, title] [measure, title, title] [of, title, title] [disclosure, title, title] [risk, title, title] [for, title, title] [microdata, title, title] [,, punct, punct] [J., journal, journal] [Roy, journal, journal] [., punct, punct] [Statist, journal, journal] [., punct, punct] [Soc, journal, journal] [., punct, punct] [B, journal, journal] [64, volume, volume] [:, punct, punct] [855, pages, pages] [--, pages, pages] [867, pages, pages] [,, punct, punct] [2002, date, date] [., punct, punct] [MR1979391, other, other] \n",
      "[Rivest, author, author] [,, punct, punct] [R., author, author] [L., other, author] [(, punct, punct] [1987, date, date] [), punct, punct] [., punct, punct] [Learning, title, title] [decision, title, title] [lists, title, title] [., punct, punct] [Machine, booktitle, booktitle] [Learning, booktitle, booktitle] [,, punct, punct] [2, volume, volume] [(, punct, punct] [3, volume, other] [), punct, punct] [,, punct, punct] [229-246, pages, pages] [., punct, punct] \n",
      "[Sekine, author, author] [,, punct, punct] [T., author, other] [,, punct, punct] [(, punct, punct] [2001, date, date] [), punct, punct] [., punct, punct] [Modeling, title, title] [and, title, title] [Forecasting, title, title] [Inflation, title, title] [in, title, title] [Japan, title, title] [,, punct, punct] [IMF, tech, tech] [Working, tech, tech] [Paper, tech, tech] [,, punct, punct] [WP/01/82, other, other] \n",
      "[Kirkpatrick, author, author] [,, punct, punct] [S., author, author] [,, punct, punct] [Gelatt, author, author] [,, punct, punct] [C., author, author] [,, punct, punct] [&, punct, punct] [Vecchi, author, author] [,, punct, punct] [M., other, other] [(, punct, punct] [1983, date, date] [), punct, punct] [., punct, punct] [Optimization, title, title] [by, title, title] [simulated, title, title] [annealing, title, title] [., punct, punct] [Science, journal, journal] [,, punct, punct] [220, volume, volume] [(, punct, punct] [4598, volume, pages] [), punct, punct] [,, punct, punct] [671-680, pages, pages] [., punct, punct] \n",
      "[[, punct, punct] [Vo1, other, other] [], punct, punct] [V., author, author] [E., author, author] [Voskresenskii, author, author] [,, punct, punct] [R-equivalence, title, title] [questions, title, title] [on, title, title] [semisimple, title, title] [groups, title, title] [(, punct, punct] [in, other, other] [Rus-, other, other] [sian, other, booktitle] [), punct, punct] [,, punct, punct] [Zap, booktitle, booktitle] [., punct, punct] [Nauc, booktitle, location] [., punct, punct] [Sem, booktitle, booktitle] [., punct, punct] [Leningrad, publisher, booktitle] [Otdel, publisher, booktitle] [Mat, publisher, date] [., punct, punct] [Inst, publisher, date] [., punct, punct] [Steklov, publisher, booktitle] [(, punct, punct] [LOMI, publisher, date] [), punct, punct] [86, volume, pages] [(, punct, punct] [1979, date, date] [), punct, punct] [,, punct, punct] [49, pages, pages] [--, pages, pages] [65, pages, pages] [and, pages, pages] [189, pages, pages] [--, pages, pages] [190, pages, pages] [., punct, punct] \n",
      "[[, punct, punct] [14, other, other] [], punct, punct] [O'Brien, author, author] [W., author, author] [A., author, author] [,, punct, punct] [Grovit-Ferbas, author, author] [K., author, author] [,, punct, punct] [Namazi, author, author] [A., author, author] [,, punct, punct] [et, author, author] [al, author, author] [., punct, punct] [:, punct, punct] [Human, title, title] [immunodefi-, title, title] [ciency, title, title] [virus, title, title] [type, title, title] [1, title, title] [replication, title, title] [can, title, title] [be, title, title] [increased, title, other] [in, title, other] [peripheral, title, other] [blood, title, other] [of, title, other] [seropositive, title, other] [patients, title, title] [after, title, title] [influenza, title, title] [vaccination, title, title] [., punct, punct] [Blood, journal, other] [1995, date, date] [,, punct, punct] [86, volume, volume] [:, punct, punct] [1082, pages, pages] [--, pages, pages] [1089, pages, pages] [., punct, punct] \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from sklearn_hierarchical_classification.classifier import HierarchicalClassifier\n",
    "from sklearn_hierarchical_classification.constants import ROOT\n",
    "from sklearn_hierarchical_classification.metrics import h_fbeta_score, multi_labeled\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix as cm\n",
    "from sklearn.model_selection import KFold\n",
    "from prettytable import PrettyTable\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "\n",
    "X_train, y_train = torch.tensor(X_train), torch.tensor(y_train)\n",
    "X_test, y_test = torch.tensor(X_test), torch.tensor(y_test)\n",
    "\n",
    "\n",
    "def categorical_accuracy(outputs, y, pad_index):\n",
    "    max_outputs = outputs.argmax(dim = 1, keepdim=True)\n",
    "    non_padded_elements = (y != pad_index).nonzero()\n",
    "    correct = max_outputs[non_padded_elements].squeeze(1).eq(y[non_padded_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_padded_elements].shape[0]])\n",
    "\n",
    "def get_max_outputs(outputs):\n",
    "    max_outputs = outputs.argmax(dim = -1)\n",
    "    return max_outputs\n",
    "\n",
    "def print_report(report):\n",
    "    table = PrettyTable(float_format=\"1.5f\")\n",
    "    table.field_names = [\"Tag\", \"Precision\", \"Recall\", \"FBeta\"]\n",
    "    for i in range(len(tag_arr)):\n",
    "      tag, scores = [tag_arr[i]], list(map(lambda metric: metric[i], report))[:-1] # exclude support metric\n",
    "      tag.extend(scores)\n",
    "      table.add_row(tag)\n",
    "    print(table)\n",
    "\n",
    "def print_statistics(X_test, y_test, y_pred, model):\n",
    "    macro_score = f1_score(y_test, y_pred, average='macro')\n",
    "    micro_score = f1_score(y_test, y_pred, average='micro')\n",
    "    cMtx = cm(y_test, y_pred)\n",
    "    normalized_cMtx = []\n",
    "    for row in cMtx:\n",
    "        total = sum(row)\n",
    "        if total != 0:\n",
    "            row = list(map(lambda value: \"{:.5f}\".format(value / total), row))\n",
    "        normalized_cMtx.append(row)\n",
    "    print('Test F1 macro score: {}'.format(macro_score))\n",
    "    print('Test F1 micro score: {}'.format(micro_score))\n",
    "    print(normalized_cMtx)\n",
    "\n",
    "def print_tags(y_pred, limit = 30):\n",
    "    for i, ref_pred in enumerate(y_pred):\n",
    "        if i == limit:\n",
    "            return\n",
    "        ref = ref_test[i]\n",
    "        output = \"\"\n",
    "        for index, token_tag in enumerate(ref):\n",
    "            token, tag = token_tag[0], token_tag[1]\n",
    "            output +=  (\"[\" + \", \".join([token, tag, tag_arr[ref_pred[index]]]) + \"] \")\n",
    "        print(output)\n",
    "\n",
    "\n",
    "# model_filename is used to save the model\n",
    "def train(train_dataset, model_filename=None):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256)\n",
    "    min_loss = 10.0\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            X_train, y_train = data\n",
    "            if i == len(train_loader)-1:\n",
    "                X_train_style = X_style[i*256:]\n",
    "            else:\n",
    "                X_train_style = X_style[i*256:(i+1)*256]\n",
    "            \n",
    "            outputs = model.forward(X_train, X_train_style)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = outputs.view(-1, outputs.shape[-1]) # [batch_size * seq_len, output_dim]\n",
    "            y_train = y_train.view(-1) # [batch_size * seq_len]\n",
    "            \n",
    "            # Get the loss function\n",
    "            loss = criterion(outputs, y_train.long())\n",
    "\n",
    "            # Calculate loss\n",
    "            loss.backward()\n",
    "            total_train_loss += loss.item()\n",
    "            # Backpropagation\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print loss at every 100th epoch\n",
    "            if epoch % 100 == 0:\n",
    "                print(\"Epoch: %d, loss after minibatch %5d: %1.5f\" % (epoch, i+1, loss.item()))\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            report = precision_recall_fscore_support(y_train.long(), \\\n",
    "                                                    get_max_outputs(outputs.detach()), \\\n",
    "                                                    average=None, \\\n",
    "                                                    zero_division=0, \\\n",
    "                                                    labels = [i for i in range(len(all_tags))])\n",
    "            print_report(report)\n",
    "        if total_train_loss < min_loss:\n",
    "            print(total_train_loss, min_loss, 'current epoch {}: saving best model...'.format(epoch))\n",
    "            min_loss = total_train_loss\n",
    "            torch.save(model.state_dict(), './models/checkpoint.pt')\n",
    "\n",
    "        \n",
    "    model.load_state_dict(torch.load('./models/checkpoint.pt'))\n",
    "\n",
    "def test(model, X_test, y_test):\n",
    "    with torch.no_grad():\n",
    "        print()\n",
    "        outputs = model.forward(X_test, X_test_style)\n",
    "\n",
    "        outputs_squeezed = outputs.view(-1, outputs.shape[-1]) # [batch_size * seq_len, output_dim]\n",
    "        y_test_squeezed = y_test.view(-1) # [batch_size * seq_len]\n",
    "\n",
    "        # Get the loss function\n",
    "        loss = criterion(outputs_squeezed, y_test_squeezed.long())\n",
    "        \n",
    "        int_y_test = y_test.int().tolist()\n",
    "        output_probs = outputs\n",
    "        y_true, y_pred = [], []\n",
    "        for idx, row in enumerate(int_y_test):\n",
    "            padding_idx = row.index(len(all_tags))\n",
    "            y_true.extend(row[:padding_idx])\n",
    "            test_row = output_probs[idx][:padding_idx]\n",
    "            y_pred.extend(get_max_outputs(test_row))\n",
    "\n",
    "        report = precision_recall_fscore_support(y_true, \\\n",
    "                                                y_pred, \\\n",
    "                                                average=None, \\\n",
    "                                                zero_division=0, \\\n",
    "                                                labels = [i for i in range(len(all_tags))])\n",
    "        print_report(report)  \n",
    "        print_statistics(X_test, y_true, y_pred, model)\n",
    "        print_tags(get_max_outputs(outputs.detach()))\n",
    "training_set = TensorDataset(X_train, y_train)\n",
    "train(training_set)\n",
    "model.load_state_dict(torch.load('./models/checkpoint.pt'))\n",
    "model_filename = \"./models/swe_we_cstyle_ner_2fc.pt\" # change accordingly based on features being run\n",
    "\n",
    "torch.save(model.state_dict(), str(model_filename))\n",
    "test(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "6e61ca267e818e5d9a430b77a02997073c0a797ef0ae9dc5b9fbd2a4d1cc9a76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
