{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install torch\n",
    "!pip install fasttext\n",
    "!pip install sklearn-hierarchical-classification\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ongwe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Preprocess Data\n",
    "'''\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import fasttext\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk import word_tokenize\n",
    "\n",
    "OTHER_TAG = \"other\"\n",
    "PUNCT_TAG = \"punct\"\n",
    "\n",
    "with open('./utils/tags.txt', encoding=\"utf-8\", errors='ignore') as f:\n",
    "    tags = set([str.rstrip(tag) for tag in f.readlines()])\n",
    "\n",
    "with open('./utils/tags_hierarchy.txt', encoding=\"utf-8\", errors='ignore') as f:\n",
    "    hierarchy_tags = set([str.rstrip(tag) for tag in f.readlines()])\n",
    "\n",
    "def remove_labels(text):\n",
    "    return re.sub(r'\\<\\/?[\\w-]*\\>\\s*', \"\", text).strip()\n",
    "\n",
    "def tag_token(token, tag):\n",
    "    if token in string.punctuation:\n",
    "        return (token, PUNCT_TAG)\n",
    "    return (token, tag)\n",
    "\n",
    "def get_tagged_tokens(ref, groups):\n",
    "    tagged_tokens = []\n",
    "    relevant_groups = list(filter(lambda group: group[1] in tags, groups))\n",
    "    tag_dict = dict()\n",
    "    for group in relevant_groups:\n",
    "        text = remove_labels(group[0])\n",
    "        tokens = word_tokenize(text)\n",
    "        for token in tokens:\n",
    "            if token not in tag_dict:\n",
    "                tag_dict[token] = [group[1]]\n",
    "            else:\n",
    "                tag_dict[token].append(group[1])\n",
    "    tokenized_ref = word_tokenize(remove_labels(ref))\n",
    "    tagged_tokens = []\n",
    "    for token in tokenized_ref:\n",
    "        if token in tag_dict and tag_dict[token]: # still has a tag\n",
    "            tag = tag_dict[token][0]\n",
    "            tag_dict[token].pop(0)\n",
    "        else:\n",
    "            tag = OTHER_TAG\n",
    "        tagged_tokens.append(tag_token(token, tag))\n",
    "    return tagged_tokens\n",
    "\n",
    "def find_groups(text):\n",
    "    groups = re.findall(r'(\\<(.*)\\>.*\\<\\/\\2\\>)', text) # group: (<tag> ... <tag>, tag)\n",
    "    if not groups:\n",
    "        return []\n",
    "    combined = []\n",
    "    for group in groups:\n",
    "        new_group = re.sub(r'\\<\\/?'+ group[1] + '\\>\\s*', \"\", group[0]).strip()\n",
    "        combined.append(group)\n",
    "        combined.extend(find_groups(new_group))\n",
    "    return combined\n",
    "\n",
    "''' Attach tags to each token '''\n",
    "def attach_tags(dataset_path):\n",
    "    dataset = []\n",
    "    with open(dataset_path, encoding=\"utf-8\", errors='ignore') as f:\n",
    "        refs = f.readlines()\n",
    "        for ref in refs:\n",
    "            groups = find_groups(ref)\n",
    "            # groups = re.findall(r'(\\<(.*)\\>.*\\<\\/\\2\\>)', ref) # format (<tag>...</tag>, tag)\n",
    "            tagged_tokens = get_tagged_tokens(ref, groups)\n",
    "            dataset.append(tagged_tokens)\n",
    "    return dataset\n",
    "\n",
    "''' Removes labels and tokenizes '''\n",
    "def tokenize_dataset(dataset_path, sep=\" \"):\n",
    "    dataset = []\n",
    "    with open(dataset_path, encoding=\"utf-8\", errors='ignore') as f:\n",
    "        refs = f.readlines()\n",
    "        for ref in refs:\n",
    "            ref = remove_labels(ref) \n",
    "            tokenized = sep.join(word_tokenize(ref))\n",
    "            dataset.append(tokenized)\n",
    "    return dataset\n",
    "\n",
    "def map_to_index(keys, idx_start=0):\n",
    "    key_to_idx, keys_arr, idx = {}, [], idx_start\n",
    "    for key in keys:\n",
    "        key_to_idx[key] = idx\n",
    "        keys_arr.append(key)\n",
    "        idx += 1\n",
    "    return key_to_idx, keys_arr\n",
    "\n",
    "all_tags = tags \n",
    "all_tags.add(OTHER_TAG)\n",
    "all_tags.add(PUNCT_TAG)\n",
    "tag_to_idx, tag_arr = map_to_index(all_tags)\n",
    "\n",
    "dataset_path = './dataset/standardized_dataset.txt'\n",
    "dataset = attach_tags(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "def train_word_embedding_model(dataset_paths, embedding_dim, use_subwords=False, use_hierarchy=False):\n",
    "    embedding_dataset_path = './dataset/word_embedding_dataset.txt'\n",
    "    hierarchy_dataset_path = './dataset/umass-citation/training'\n",
    "\n",
    "    word_embedding_dataset = []\n",
    "    for dataset_path in dataset_paths:\n",
    "      word_embedding_dataset.extend(tokenize_dataset(dataset_path, sep=\" \"))\n",
    "    with open(embedding_dataset_path, 'w', errors='ignore') as f:\n",
    "        # fasttext tokenizes by whitespaces\n",
    "        f.write(\"\\n\".join(word_embedding_dataset))\n",
    "    if use_subwords:\n",
    "      model_path = './models/subword_embedding.bin'\n",
    "      model = fasttext.train_unsupervised(embedding_dataset_path, dim = embedding_dim, minn = 3, maxn = 6, wordNgrams=6) \n",
    "    elif use_hierarchy:\n",
    "        model_path = './models/hierarchy_word_embedding.bin'\n",
    "        model = fasttext.train_unsupervised(embedding_dataset_path, dim=embedding_dim, maxn=0)\n",
    "    else:\n",
    "      model_path = './models/word_embedding.bin'\n",
    "      model = fasttext.train_unsupervised(embedding_dataset_path, dim=embedding_dim, maxn=0)\n",
    "    model.save_model(model_path)\n",
    "    return model\n",
    "\n",
    "we_dataset_dir_path = \"./dataset/cstyle_dataset\"\n",
    "we_dataset_paths = [join(we_dataset_dir_path, f) for f in listdir(we_dataset_dir_path) if isfile(join(we_dataset_dir_path, f))]\n",
    "\n",
    "''' 1. Word Embeddings: Without Pretrained Word Embeddings '''\n",
    "# WE_model = train_word_embedding_model(we_dataset_paths, embedding_dim = EMBEDDING_DIM)\n",
    "\n",
    "''' 2. Subword Embeddings: Without Pretrained Subword Embeddings '''\n",
    "SWE_model = train_word_embedding_model(we_dataset_paths, embedding_dim = EMBEDDING_DIM, use_subwords = True)\n",
    "\n",
    "curr_WE_model = SWE_model # Change accordingly\n",
    "\n",
    "def get_word_vector(token):\n",
    "    return curr_WE_model.get_word_vector(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Named Entity Recognition'''\n",
    "import spacy\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "ner_dict = {\n",
    "        'ORG': 0,\n",
    "        \"NORP\": 1,\n",
    "        \"GPE\": 2,\n",
    "        \"PERSON\": 3,\n",
    "        \"LANGUAGE\": 4,\n",
    "        \"DATE\": 5,\n",
    "        \"TIME\": 6,\n",
    "        \"PRODUCT\": 7,\n",
    "        \"EVENT\": 8,\n",
    "        \"ORDINAL\": 9\n",
    "}\n",
    "\n",
    "# text should be tokenized and joined together by whitespaces\n",
    "def generate_ner_features(text):\n",
    "    doc = spacy_nlp(text)\n",
    "    entities = doc.ents\n",
    "    default_feature = [0 for _ in range(len(ner_dict.keys()) + 1)]\n",
    "    default_feature[-1] = 1 \n",
    "    entity_to_label = defaultdict(lambda: default_feature)\n",
    "    for entity in entities:\n",
    "        entity_tokens = entity.text.split(\" \")\n",
    "        label = entity.label_\n",
    "        if label in ner_dict:\n",
    "            features = [0 for _ in range(len(ner_dict.keys()) + 1)]\n",
    "            features[ner_dict[label]] = 1\n",
    "            for token in entity_tokens:\n",
    "                entity_to_label[token] = features\n",
    "    return entity_to_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "model_version = 'allenai/scibert_scivocab_cased'\n",
    "do_lower_case = False\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
    "\n",
    "def get_tokens_and_segments_tensors(text):\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segment_ids = [1] * len(indexed_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = torch.tensor([segment_ids])\n",
    "    return tokens_tensor, segments_tensor\n",
    "\n",
    "def average(embeddings):\n",
    "    if len(embeddings.size()) == 1: # if only one embedding, just return\n",
    "        return embeddings\n",
    "    averaged_embedding = np.array([0 for _ in range(len(embeddings[0]))])\n",
    "    for embedding in embeddings:\n",
    "        averaged_embedding = np.add(averaged_embedding, embedding)\n",
    "    return np.true_divide(averaged_embedding, len(embeddings))\n",
    "\n",
    "from transformers import BertForPreTraining, BertConfig \n",
    "\n",
    "config = BertConfig.from_json_file('./models/bert/fine_tuned_bert/config.json')\n",
    "bert_model = BertModel(config)\n",
    "bert_model.eval()\n",
    "\n",
    "def get_scibert_vector(text):\n",
    "    tokens_tensor, segments_tensor =get_tokens_and_segments_tensors(text)\n",
    "    outputs = bert_model(tokens_tensor, segments_tensor)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    token_embeddings = torch.squeeze(hidden_states[-1], dim=0).detach()\n",
    "    embeddings = dict()\n",
    "    tokens = text.split(\" \")\n",
    "    curr = 0\n",
    "    for token in tokens:\n",
    "        end = curr + len(tokenizer.tokenize(token))\n",
    "        start, end = curr, end\n",
    "        embeddings[token] = average(token_embeddings[start: end]).tolist()\n",
    "        curr = end\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(tokenize_dataset(dataset_path), columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "\n",
    "def token_transformer(sentence, add_noise=False):\n",
    "    sentence = re.sub(r\"\\b[A-Z]\\b\", 'L', sentence) #uppercase letters\n",
    "    sentence = re.sub(r\"\\b[a-z]\\b\", 'l', sentence) #lowercase letters\n",
    "    if add_noise:\n",
    "        sentence = re.sub(r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\b', random.choices(['M',''], weights=[3,1])[0], sentence)\n",
    "        sentence = re.sub(r'\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\b', random.choices(['m',''], weights=[3,1])[0], sentence)\n",
    "    else:\n",
    "        sentence = re.sub(r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\b', 'M', sentence) #random.choices(['M',''], weights=[3,1])[0], sentence)\n",
    "        sentence = re.sub(r'\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\b', 'm', sentence) #random.choices(['m',''], weights=[3,1])[0], sentence)\n",
    "    sentence = re.sub(r\"\\b[a-z][a-z]+\\b\", 'w', sentence) #lowercase words\n",
    "    sentence = re.sub(r\"\\b[A-Z][a-z]+\\b\", 'W', sentence) #uppercase words\n",
    "    sentence = re.sub(r'\\b(1\\d{3}|20[012]\\d)\\b', 'y', sentence)\n",
    "    sentence = re.sub(r\"\\b[0-9]+\\b\", 'n', sentence) #numbers\n",
    "    return sentence\n",
    "\n",
    "def token_punctuation(sentence, add_noise=False):\n",
    "    if add_noise:\n",
    "        words = sentence.split()\n",
    "        newSentence = []\n",
    "        for word in words: # for every word with punctuation, there is a 10% chance of omitting one type of punctuation.\n",
    "            newSentence.append(word.translate({ord(i):\" {} \".format(random.choices([i, ''], weights=[9,1])[0]) for i in ',.()[]:;\\'\\\"-'}))\n",
    "        sentence = \" \".join(newSentence)\n",
    "        return sentence\n",
    "    \n",
    "    else:\n",
    "        return sentence.translate({ord(i):\" {} \".format(i) for i in ',.()[]:;\\'\\\"-'})\n",
    "    \n",
    "\n",
    "''' Preprocess text column -- separate symbols with spaces to preserve punctuation as tokens'''\n",
    "def preprocess(df, add_noise=False): # text is a dataframe column\n",
    "    df.text = df.text.apply(token_punctuation, add_noise)\n",
    "    df.text = df.text.apply(token_transformer, add_noise)\n",
    "    return df\n",
    "df = preprocess(df, False) # boolean flag to add noise. For training purposes to be robust against noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, plot_confusion_matrix, confusion_matrix as cm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', TfidfVectorizer(lowercase=False, token_pattern=r\"\\S+\")), #r\"(?u)(\\b\\w+\\b|[\\.\\\"(),\\'\\[\\]:;])\")),\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('classifier', LogisticRegression(class_weight='balanced'))\n",
    "])\n",
    "params = {'features__ngram_range': [(1,3)],\n",
    "    'classifier__C': [0.001],\n",
    "         'classifier__max_iter': [500],}\n",
    "gs = GridSearchCV(pipeline, params, refit=True, cv=2, scoring='f1_macro', verbose=10)\n",
    "from joblib import load\n",
    "\n",
    "gs = load('models\\\\cstyle_LR_augmentedfeatures_3grams_noisyinput_noUnknown.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mla', 'mla', 'mla', ..., 'acm-sig-proceedings',\n",
       "       'acm-sig-proceedings', 'harvard3'], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.predict(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "categories = [['acm-sig-proceedings'], ['american-chemical-society'], ['apa'],\n",
    "       ['chicago-author-date'], ['harvard3'], ['ieee'], ['mla']]#, ['unknown']]\n",
    "enc.fit(categories)\n",
    "# enc.fit(gs.predict(df.text).reshape(-1, 1))\n",
    "\n",
    "def style_to_feature(style, train=False):\n",
    "    local_df = pd.DataFrame()\n",
    "    fstyle = list(map(lambda s: [s], style))\n",
    "    if train:\n",
    "        enc.fit(fstyle)\n",
    "    y_transformed = enc.transform(fstyle).toarray()\n",
    "\n",
    "    return y_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_encoder(styles):\n",
    "    cat2vec = {}\n",
    "    vec = []\n",
    "    for i in range(len(categories)):\n",
    "        cat2vec[categories[i][0]] = [0 for j in range(len(categories))]\n",
    "        cat2vec[categories[i][0]][i] = 1\n",
    "    for style in styles:\n",
    "        vec.append(cat2vec[style])\n",
    "    return np.array(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "''' Get inputs and outputs for model '''\n",
    "ref_train, ref_test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "def get_x_y(refs, train=False):\n",
    "    X, y = [], []\n",
    "    X_style, y_style = [], []\n",
    "    for ref in refs:\n",
    "        X_ref, y_ref = [], []\n",
    "        joined_ref = \" \".join(list(map(lambda x: x[0], ref))) # concatenate tokens using whitespace\n",
    "        X_style.append(joined_ref)\n",
    "        ner_features = generate_ner_features(joined_ref)\n",
    "        scibert_features = get_scibert_vector(joined_ref)\n",
    "        style = style_to_feature(gs.predict([joined_ref]))\n",
    "        for token, tag in ref:\n",
    "            features = get_word_vector(token)\n",
    "            features = np.hstack([features, np.array(ner_features[token]), np.array(scibert_features[token])])#, style[0]])\n",
    "            X_ref.append(features)\n",
    "            y_ref.append(tag_to_idx[tag])\n",
    "\n",
    "        X.append(X_ref)\n",
    "        y.append(y_ref)\n",
    "\n",
    "    style = gs.predict(X_style)\n",
    "    feature_style = style_to_feature(style, train)#style_encoder(style)#\n",
    "    y_style = []\n",
    "    idx = 0\n",
    "    for ref in refs:\n",
    "        sentence_style = []\n",
    "        feat = feature_style[idx]\n",
    "        for token, tag in ref:\n",
    "            sentence_style.append(feat)\n",
    "            features = np.concatenate((features, feature_style[idx]), axis=None)\n",
    "        y_style.append(sentence_style)\n",
    "        idx += 1\n",
    "    return X, y, X_style, y_style\n",
    "\n",
    "def add_padding(matrix, padding_value, max_length):\n",
    "    return pad_sequences(matrix, maxlen=max_length, padding='post', truncating='pre', value=padding_value, dtype='float32')\n",
    "\n",
    "X_train, y_train, X_sentence, X_style = get_x_y(ref_train, True)\n",
    "X_test, y_test, X_test_sentence, X_test_style = get_x_y(ref_test)\n",
    "\n",
    "padding_value = float(len(all_tags))\n",
    "max_length = max(map(lambda ref: len(ref), X_train + X_test))\n",
    "\n",
    "X_train = add_padding(X_train, padding_value, max_length)\n",
    "X_test = add_padding(X_test, padding_value, max_length)\n",
    "y_train = add_padding(y_train, padding_value, max_length)\n",
    "y_test = add_padding(y_test, padding_value, max_length)\n",
    "X_style = add_padding(X_style, padding_value, max_length)\n",
    "X_test_style = add_padding(X_test_style, padding_value, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5],\n",
       " [Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, '')])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "testingstyle = gs.predict(X_sentence)\n",
    "testingstyle1 = gs.predict(X_test_sentence)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(testingstyle)\n",
    "plt.xticks(rotation=45)\n",
    "plt.hist(testingstyle1)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = self.input_size,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.num_layers,\n",
    "            batch_first = True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialise hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "\n",
    "        # Initialise internal state\n",
    "        c0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size)\n",
    "\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "\n",
    "        return output, (hn, cn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = LSTM(input_size, hidden_size, output_size, num_layers)\n",
    "        # self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.fc = nn.Linear(hidden_size + X_style.shape[2], output_size)\n",
    "        # self.fc1 = nn.Linear(output_size, output_size)\n",
    "\n",
    "    def forward(self, x, style):\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        output = torch.cat((output, torch.tensor(style)), 2)\n",
    "        output = self.fc(output)\n",
    "        # output = self.fc1(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hyperparameters\n",
    "'''\n",
    "num_epochs = 800\n",
    "learning_rate = 0.001\n",
    "\n",
    "we_feature_size = EMBEDDING_DIM\n",
    "ner_feature_size = len(ner_dict.keys()) + 1\n",
    "scibert_feature_size = 768\n",
    "\n",
    "input_size = we_feature_size + ner_feature_size + scibert_feature_size #+ len(categories) # Number of features (change accordingly)\n",
    "hidden_size = 25 #+ X_style.shape[2] # Number of features in the hidden state\n",
    "num_layers = 1 # Number of stacked LSTM layers\n",
    "\n",
    "output_size = len(all_tags) # Number of output classes\n",
    "model = Net(input_size, hidden_size, output_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loss Function and Optimiser\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=len(all_tags))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss after minibatch     1: 2.74462\n",
      "Epoch: 0, loss after minibatch     2: 2.66284\n",
      "Epoch: 0, loss after minibatch     3: 2.58331\n",
      "Epoch: 0, loss after minibatch     4: 2.51561\n",
      "Epoch: 0, loss after minibatch     5: 2.46081\n",
      "Epoch: 0, loss after minibatch     6: 2.40639\n",
      "Epoch: 0, loss after minibatch     7: 2.37204\n",
      "Epoch: 0, loss after minibatch     8: 2.33233\n",
      "Epoch: 0, loss after minibatch     9: 2.28181\n",
      "Epoch: 0, loss after minibatch    10: 2.27827\n",
      "+-------------+---------------------+---------------------+---------------------+\n",
      "|     Tag     |      Precision      |        Recall       |        FBeta        |\n",
      "+-------------+---------------------+---------------------+---------------------+\n",
      "|  publisher  |         0.0         |         0.0         |         0.0         |\n",
      "|    punct    |  0.8178807947019867 |  0.2965186074429772 |  0.4352422907488987 |\n",
      "|    editor   |         0.0         |         0.0         |         0.0         |\n",
      "|    pages    |         0.0         |         0.0         |         0.0         |\n",
      "|     note    |         0.0         |         0.0         |         0.0         |\n",
      "|   journal   |         0.0         |         0.0         |         0.0         |\n",
      "|   location  |         0.0         |         0.0         |         0.0         |\n",
      "|    other    |         0.0         |         0.0         |         0.0         |\n",
      "|    author   | 0.20723684210526316 |  0.9671052631578947 | 0.34133126934984515 |\n",
      "|     date    |         0.0         |         0.0         |         0.0         |\n",
      "|    volume   |         0.0         |         0.0         |         0.0         |\n",
      "|  booktitle  |         0.0         |         0.0         |         0.0         |\n",
      "| institution |         0.0         |         0.0         |         0.0         |\n",
      "|     tech    |         0.0         |         0.0         |         0.0         |\n",
      "|    title    |  0.3157894736842105 | 0.05442176870748299 | 0.09284332688588008 |\n",
      "+-------------+---------------------+---------------------+---------------------+\n",
      "9.95934909582138 10.0 current epoch 19: saving best model...\n",
      "9.659822225570679 9.95934909582138 current epoch 20: saving best model...\n",
      "9.384375870227814 9.659822225570679 current epoch 21: saving best model...\n",
      "9.129601299762726 9.384375870227814 current epoch 22: saving best model...\n",
      "8.892553806304932 9.129601299762726 current epoch 23: saving best model...\n",
      "8.670374095439911 8.892553806304932 current epoch 24: saving best model...\n",
      "8.4603191614151 8.670374095439911 current epoch 25: saving best model...\n",
      "8.25848537683487 8.4603191614151 current epoch 26: saving best model...\n",
      "8.060087263584137 8.25848537683487 current epoch 27: saving best model...\n",
      "7.855676591396332 8.060087263584137 current epoch 28: saving best model...\n",
      "7.657077312469482 7.855676591396332 current epoch 29: saving best model...\n",
      "7.480120122432709 7.657077312469482 current epoch 30: saving best model...\n",
      "7.310094594955444 7.480120122432709 current epoch 31: saving best model...\n",
      "7.151484668254852 7.310094594955444 current epoch 32: saving best model...\n",
      "6.999400436878204 7.151484668254852 current epoch 33: saving best model...\n",
      "6.850852310657501 6.999400436878204 current epoch 34: saving best model...\n",
      "6.705732583999634 6.850852310657501 current epoch 35: saving best model...\n",
      "6.566716492176056 6.705732583999634 current epoch 36: saving best model...\n",
      "6.432861566543579 6.566716492176056 current epoch 37: saving best model...\n",
      "6.30011510848999 6.432861566543579 current epoch 38: saving best model...\n",
      "6.158482730388641 6.30011510848999 current epoch 39: saving best model...\n",
      "6.018080353736877 6.158482730388641 current epoch 40: saving best model...\n",
      "5.880745768547058 6.018080353736877 current epoch 41: saving best model...\n",
      "5.750613212585449 5.880745768547058 current epoch 42: saving best model...\n",
      "5.621314823627472 5.750613212585449 current epoch 43: saving best model...\n",
      "5.493685364723206 5.621314823627472 current epoch 44: saving best model...\n",
      "5.3810760378837585 5.493685364723206 current epoch 45: saving best model...\n",
      "5.275051265954971 5.3810760378837585 current epoch 46: saving best model...\n",
      "5.175019204616547 5.275051265954971 current epoch 47: saving best model...\n",
      "5.077378481626511 5.175019204616547 current epoch 48: saving best model...\n",
      "4.978245168924332 5.077378481626511 current epoch 49: saving best model...\n",
      "4.88671875 4.978245168924332 current epoch 50: saving best model...\n",
      "4.796515017747879 4.88671875 current epoch 51: saving best model...\n",
      "4.715047121047974 4.796515017747879 current epoch 52: saving best model...\n",
      "4.638177961111069 4.715047121047974 current epoch 53: saving best model...\n",
      "4.560256272554398 4.638177961111069 current epoch 54: saving best model...\n",
      "4.496974170207977 4.560256272554398 current epoch 55: saving best model...\n",
      "4.450517296791077 4.496974170207977 current epoch 56: saving best model...\n",
      "4.391916453838348 4.450517296791077 current epoch 57: saving best model...\n",
      "4.33477059006691 4.391916453838348 current epoch 58: saving best model...\n",
      "4.269559174776077 4.33477059006691 current epoch 59: saving best model...\n",
      "4.216657757759094 4.269559174776077 current epoch 60: saving best model...\n",
      "4.170224279165268 4.216657757759094 current epoch 61: saving best model...\n",
      "4.0373693108558655 4.170224279165268 current epoch 65: saving best model...\n",
      "3.9693042039871216 4.0373693108558655 current epoch 66: saving best model...\n",
      "3.881938010454178 3.9693042039871216 current epoch 67: saving best model...\n",
      "3.818485051393509 3.881938010454178 current epoch 68: saving best model...\n",
      "3.7761687636375427 3.818485051393509 current epoch 69: saving best model...\n",
      "3.7274274826049805 3.7761687636375427 current epoch 70: saving best model...\n",
      "3.686225414276123 3.7274274826049805 current epoch 71: saving best model...\n",
      "3.653334617614746 3.686225414276123 current epoch 72: saving best model...\n",
      "3.6262119114398956 3.653334617614746 current epoch 73: saving best model...\n",
      "3.594269096851349 3.6262119114398956 current epoch 74: saving best model...\n",
      "3.56173512339592 3.594269096851349 current epoch 75: saving best model...\n",
      "3.5264121890068054 3.56173512339592 current epoch 76: saving best model...\n",
      "3.481190115213394 3.5264121890068054 current epoch 77: saving best model...\n",
      "3.4242344200611115 3.481190115213394 current epoch 78: saving best model...\n",
      "3.372794270515442 3.4242344200611115 current epoch 79: saving best model...\n",
      "3.33148655295372 3.372794270515442 current epoch 80: saving best model...\n",
      "3.2951104938983917 3.33148655295372 current epoch 81: saving best model...\n",
      "3.26116219162941 3.2951104938983917 current epoch 82: saving best model...\n",
      "3.2289303839206696 3.26116219162941 current epoch 83: saving best model...\n",
      "3.197222411632538 3.2289303839206696 current epoch 84: saving best model...\n",
      "3.1660716831684113 3.197222411632538 current epoch 85: saving best model...\n",
      "3.1360689103603363 3.1660716831684113 current epoch 86: saving best model...\n",
      "3.1068707704544067 3.1360689103603363 current epoch 87: saving best model...\n",
      "3.0780070424079895 3.1068707704544067 current epoch 88: saving best model...\n",
      "3.0485010147094727 3.0780070424079895 current epoch 89: saving best model...\n",
      "3.019372880458832 3.0485010147094727 current epoch 90: saving best model...\n",
      "2.991713047027588 3.019372880458832 current epoch 91: saving best model...\n",
      "2.96556556224823 2.991713047027588 current epoch 92: saving best model...\n",
      "2.9405335187911987 2.96556556224823 current epoch 93: saving best model...\n",
      "2.9161962270736694 2.9405335187911987 current epoch 94: saving best model...\n",
      "2.892170488834381 2.9161962270736694 current epoch 95: saving best model...\n",
      "2.867974132299423 2.892170488834381 current epoch 96: saving best model...\n",
      "2.843558430671692 2.867974132299423 current epoch 97: saving best model...\n",
      "2.8199346363544464 2.843558430671692 current epoch 98: saving best model...\n",
      "2.7979148030281067 2.8199346363544464 current epoch 99: saving best model...\n",
      "Epoch: 100, loss after minibatch     1: 0.25126\n",
      "Epoch: 100, loss after minibatch     2: 0.28468\n",
      "Epoch: 100, loss after minibatch     3: 0.29131\n",
      "Epoch: 100, loss after minibatch     4: 0.28699\n",
      "Epoch: 100, loss after minibatch     5: 0.28516\n",
      "Epoch: 100, loss after minibatch     6: 0.25526\n",
      "Epoch: 100, loss after minibatch     7: 0.29641\n",
      "Epoch: 100, loss after minibatch     8: 0.26527\n",
      "Epoch: 100, loss after minibatch     9: 0.28670\n",
      "Epoch: 100, loss after minibatch    10: 0.27341\n",
      "+-------------+----------------------+----------------------+---------------------+\n",
      "|     Tag     |      Precision       |        Recall        |        FBeta        |\n",
      "+-------------+----------------------+----------------------+---------------------+\n",
      "|  publisher  |  0.6428571428571429  |         0.72         |  0.6792452830188679 |\n",
      "|    punct    |         1.0          |  0.9939975990396158  |  0.9969897652016857 |\n",
      "|    editor   |  0.8235294117647058  |  0.7368421052631579  |  0.7777777777777778 |\n",
      "|    pages    | 0.008475217555807794 |  0.9824561403508771  |  0.0168054617750769 |\n",
      "|     note    |         0.0          |         0.0          |         0.0         |\n",
      "|   journal   |  0.7928994082840237  |  0.9571428571428572  |  0.8673139158576051 |\n",
      "|   location  | 0.42424242424242425  |  0.7777777777777778  |  0.5490196078431373 |\n",
      "|    other    |  0.8484848484848485  |         0.7          |  0.7671232876712328 |\n",
      "|    author   |  0.9474789915966386  |  0.9890350877192983  |  0.9678111587982833 |\n",
      "|     date    |  0.9595959595959596  |  0.7916666666666666  |  0.867579908675799  |\n",
      "|    volume   |  0.9180327868852459  |  0.9032258064516129  |  0.9105691056910569 |\n",
      "|  booktitle  |  0.808695652173913   |  0.8857142857142857  |  0.8454545454545455 |\n",
      "| institution |         0.5          | 0.047619047619047616 | 0.08695652173913042 |\n",
      "|     tech    |         0.0          |         0.0          |         0.0         |\n",
      "|    title    |  0.9415584415584416  |  0.9863945578231292  |  0.9634551495016611 |\n",
      "+-------------+----------------------+----------------------+---------------------+\n",
      "2.776461571455002 2.7979148030281067 current epoch 100: saving best model...\n",
      "2.756011426448822 2.776461571455002 current epoch 101: saving best model...\n",
      "2.7365715205669403 2.756011426448822 current epoch 102: saving best model...\n",
      "2.715037539601326 2.7365715205669403 current epoch 103: saving best model...\n",
      "2.7013311833143234 2.715037539601326 current epoch 104: saving best model...\n",
      "2.6753308326005936 2.7013311833143234 current epoch 105: saving best model...\n",
      "2.6559849977493286 2.6753308326005936 current epoch 106: saving best model...\n",
      "2.633265972137451 2.6559849977493286 current epoch 107: saving best model...\n",
      "2.612704575061798 2.633265972137451 current epoch 108: saving best model...\n",
      "2.578361287713051 2.612704575061798 current epoch 109: saving best model...\n",
      "2.558809533715248 2.578361287713051 current epoch 110: saving best model...\n",
      "2.5458030998706818 2.558809533715248 current epoch 111: saving best model...\n",
      "2.52242873609066 2.5458030998706818 current epoch 112: saving best model...\n",
      "2.4890712946653366 2.52242873609066 current epoch 113: saving best model...\n",
      "2.4658185988664627 2.4890712946653366 current epoch 114: saving best model...\n",
      "2.4479640424251556 2.4658185988664627 current epoch 115: saving best model...\n",
      "2.427226275205612 2.4479640424251556 current epoch 116: saving best model...\n",
      "2.4075607508420944 2.427226275205612 current epoch 117: saving best model...\n",
      "2.3949290215969086 2.4075607508420944 current epoch 118: saving best model...\n",
      "2.3829513788223267 2.3949290215969086 current epoch 119: saving best model...\n",
      "2.376764267683029 2.3829513788223267 current epoch 121: saving best model...\n",
      "2.3591932505369186 2.376764267683029 current epoch 122: saving best model...\n",
      "2.3366298228502274 2.3591932505369186 current epoch 123: saving best model...\n",
      "2.3174408972263336 2.3366298228502274 current epoch 124: saving best model...\n",
      "2.3086267709732056 2.3174408972263336 current epoch 125: saving best model...\n",
      "2.2868528068065643 2.3086267709732056 current epoch 126: saving best model...\n",
      "2.266730159521103 2.2868528068065643 current epoch 127: saving best model...\n",
      "2.256301924586296 2.266730159521103 current epoch 128: saving best model...\n",
      "2.24761863052845 2.256301924586296 current epoch 129: saving best model...\n",
      "2.2300144135951996 2.24761863052845 current epoch 130: saving best model...\n",
      "2.211227521300316 2.2300144135951996 current epoch 131: saving best model...\n",
      "2.18765552341938 2.211227521300316 current epoch 132: saving best model...\n",
      "2.157582327723503 2.18765552341938 current epoch 134: saving best model...\n",
      "2.1317386776208878 2.157582327723503 current epoch 138: saving best model...\n",
      "2.1160624772310257 2.1317386776208878 current epoch 139: saving best model...\n",
      "2.103676825761795 2.1160624772310257 current epoch 140: saving best model...\n",
      "2.087297201156616 2.103676825761795 current epoch 141: saving best model...\n",
      "2.0744832158088684 2.087297201156616 current epoch 142: saving best model...\n",
      "2.053478389978409 2.0744832158088684 current epoch 143: saving best model...\n",
      "2.0314209908246994 2.053478389978409 current epoch 144: saving best model...\n",
      "2.011141285300255 2.0314209908246994 current epoch 145: saving best model...\n",
      "1.9958847165107727 2.011141285300255 current epoch 146: saving best model...\n",
      "1.97552190721035 1.9958847165107727 current epoch 147: saving best model...\n",
      "1.95486581325531 1.97552190721035 current epoch 148: saving best model...\n",
      "1.9382584989070892 1.95486581325531 current epoch 149: saving best model...\n",
      "1.9300958812236786 1.9382584989070892 current epoch 150: saving best model...\n",
      "1.8705581575632095 1.9300958812236786 current epoch 155: saving best model...\n",
      "1.842407464981079 1.8705581575632095 current epoch 156: saving best model...\n",
      "1.828725427389145 1.842407464981079 current epoch 157: saving best model...\n",
      "1.8016401529312134 1.828725427389145 current epoch 158: saving best model...\n",
      "1.7808174192905426 1.8016401529312134 current epoch 159: saving best model...\n",
      "1.7686049789190292 1.7808174192905426 current epoch 160: saving best model...\n",
      "1.7590182572603226 1.7686049789190292 current epoch 161: saving best model...\n",
      "1.749649628996849 1.7590182572603226 current epoch 162: saving best model...\n",
      "1.737144410610199 1.749649628996849 current epoch 163: saving best model...\n",
      "1.7232816815376282 1.737144410610199 current epoch 165: saving best model...\n",
      "1.705333635210991 1.7232816815376282 current epoch 166: saving best model...\n",
      "1.6956793516874313 1.705333635210991 current epoch 167: saving best model...\n",
      "1.6793444752693176 1.6956793516874313 current epoch 168: saving best model...\n",
      "1.6641580164432526 1.6793444752693176 current epoch 169: saving best model...\n",
      "1.6508301049470901 1.6641580164432526 current epoch 170: saving best model...\n",
      "1.6406673938035965 1.6508301049470901 current epoch 171: saving best model...\n",
      "1.6318034082651138 1.6406673938035965 current epoch 172: saving best model...\n",
      "1.6244620829820633 1.6318034082651138 current epoch 173: saving best model...\n",
      "1.6151321828365326 1.6244620829820633 current epoch 174: saving best model...\n",
      "1.6132980734109879 1.6151321828365326 current epoch 175: saving best model...\n",
      "1.6125259399414062 1.6132980734109879 current epoch 177: saving best model...\n",
      "1.6039957255125046 1.6125259399414062 current epoch 178: saving best model...\n",
      "1.6002320647239685 1.6039957255125046 current epoch 179: saving best model...\n",
      "1.5903322398662567 1.6002320647239685 current epoch 181: saving best model...\n",
      "1.5846147388219833 1.5903322398662567 current epoch 183: saving best model...\n",
      "1.5400103703141212 1.5846147388219833 current epoch 187: saving best model...\n",
      "1.5151949003338814 1.5400103703141212 current epoch 188: saving best model...\n",
      "1.4913399294018745 1.5151949003338814 current epoch 190: saving best model...\n",
      "1.4671437218785286 1.4913399294018745 current epoch 191: saving best model...\n",
      "1.436690241098404 1.4671437218785286 current epoch 192: saving best model...\n",
      "1.420758306980133 1.436690241098404 current epoch 193: saving best model...\n",
      "1.4067915454506874 1.420758306980133 current epoch 194: saving best model...\n",
      "1.3935488164424896 1.4067915454506874 current epoch 195: saving best model...\n",
      "1.380382664501667 1.3935488164424896 current epoch 196: saving best model...\n",
      "1.3678847253322601 1.380382664501667 current epoch 197: saving best model...\n",
      "1.357836939394474 1.3678847253322601 current epoch 198: saving best model...\n",
      "1.3484712466597557 1.357836939394474 current epoch 199: saving best model...\n",
      "Epoch: 200, loss after minibatch     1: 0.12208\n",
      "Epoch: 200, loss after minibatch     2: 0.15046\n",
      "Epoch: 200, loss after minibatch     3: 0.13998\n",
      "Epoch: 200, loss after minibatch     4: 0.14383\n",
      "Epoch: 200, loss after minibatch     5: 0.13877\n",
      "Epoch: 200, loss after minibatch     6: 0.12779\n",
      "Epoch: 200, loss after minibatch     7: 0.14685\n",
      "Epoch: 200, loss after minibatch     8: 0.13011\n",
      "Epoch: 200, loss after minibatch     9: 0.14131\n",
      "Epoch: 200, loss after minibatch    10: 0.09936\n",
      "+-------------+---------------------+--------------------+----------------------+\n",
      "|     Tag     |      Precision      |       Recall       |        FBeta         |\n",
      "+-------------+---------------------+--------------------+----------------------+\n",
      "|  publisher  |        0.048        |        0.96        | 0.09142857142857141  |\n",
      "|    punct    |         1.0         | 0.9987995198079231 |  0.9993993993993994  |\n",
      "|    editor   |         1.0         |        1.0         |         1.0          |\n",
      "|    pages    | 0.00894679014283472 |        1.0         | 0.017734909769757314 |\n",
      "|     note    |         0.0         |        0.0         |         0.0          |\n",
      "|   journal   |  0.9652777777777778 | 0.9928571428571429 |  0.9788732394366197  |\n",
      "|   location  |  0.8571428571428571 |        1.0         |  0.923076923076923   |\n",
      "|    other    |  0.9459459459459459 |       0.875        |  0.9090909090909091  |\n",
      "|    author   |  0.9806451612903225 |        1.0         |  0.990228013029316   |\n",
      "|     date    |  0.9823008849557522 |       0.925        |  0.9527896995708154  |\n",
      "|    volume   |         1.0         |        1.0         |         1.0          |\n",
      "|  booktitle  |  0.9619047619047619 | 0.9619047619047619 |  0.9619047619047619  |\n",
      "| institution |  0.5925925925925926 | 0.7619047619047619 |  0.6666666666666666  |\n",
      "|     tech    |         1.0         |        0.4         |  0.5714285714285715  |\n",
      "|    title    |  0.975609756097561  | 0.9977324263038548 |  0.9865470852017937  |\n",
      "+-------------+---------------------+--------------------+----------------------+\n",
      "1.3405363634228706 1.3484712466597557 current epoch 200: saving best model...\n",
      "1.3298133686184883 1.3405363634228706 current epoch 201: saving best model...\n",
      "1.3196559622883797 1.3298133686184883 current epoch 202: saving best model...\n",
      "1.310974195599556 1.3196559622883797 current epoch 203: saving best model...\n",
      "1.302097536623478 1.310974195599556 current epoch 204: saving best model...\n",
      "1.2918299585580826 1.302097536623478 current epoch 205: saving best model...\n",
      "1.2840178310871124 1.2918299585580826 current epoch 206: saving best model...\n",
      "1.2761538624763489 1.2840178310871124 current epoch 207: saving best model...\n",
      "1.2683784812688828 1.2761538624763489 current epoch 208: saving best model...\n",
      "1.2601488381624222 1.2683784812688828 current epoch 209: saving best model...\n",
      "1.2533849701285362 1.2601488381624222 current epoch 210: saving best model...\n",
      "1.2454068139195442 1.2533849701285362 current epoch 211: saving best model...\n",
      "1.2415848970413208 1.2454068139195442 current epoch 212: saving best model...\n",
      "1.231691338121891 1.2415848970413208 current epoch 213: saving best model...\n",
      "1.2243272736668587 1.231691338121891 current epoch 214: saving best model...\n",
      "1.2157647013664246 1.2243272736668587 current epoch 215: saving best model...\n",
      "1.209481567144394 1.2157647013664246 current epoch 216: saving best model...\n",
      "1.1969998925924301 1.209481567144394 current epoch 217: saving best model...\n",
      "1.18946024030447 1.1969998925924301 current epoch 218: saving best model...\n",
      "1.1835448071360588 1.18946024030447 current epoch 219: saving best model...\n",
      "1.1763301864266396 1.1835448071360588 current epoch 220: saving best model...\n",
      "1.1712452545762062 1.1763301864266396 current epoch 221: saving best model...\n",
      "1.1669504418969154 1.1712452545762062 current epoch 222: saving best model...\n",
      "1.1627852767705917 1.1669504418969154 current epoch 223: saving best model...\n",
      "1.1575860306620598 1.1627852767705917 current epoch 224: saving best model...\n",
      "1.1495876833796501 1.1575860306620598 current epoch 225: saving best model...\n",
      "1.1414234787225723 1.1495876833796501 current epoch 226: saving best model...\n",
      "1.1376537308096886 1.1414234787225723 current epoch 227: saving best model...\n",
      "1.1302612945437431 1.1376537308096886 current epoch 228: saving best model...\n",
      "1.128781534731388 1.1302612945437431 current epoch 229: saving best model...\n",
      "1.1228028684854507 1.128781534731388 current epoch 238: saving best model...\n",
      "1.1214586570858955 1.1228028684854507 current epoch 239: saving best model...\n",
      "1.119851253926754 1.1214586570858955 current epoch 240: saving best model...\n",
      "1.1145489886403084 1.119851253926754 current epoch 241: saving best model...\n",
      "1.097355142235756 1.1145489886403084 current epoch 244: saving best model...\n",
      "1.0627557933330536 1.097355142235756 current epoch 245: saving best model...\n",
      "1.0534517914056778 1.0627557933330536 current epoch 246: saving best model...\n",
      "1.0394627153873444 1.0534517914056778 current epoch 247: saving best model...\n",
      "1.0257994309067726 1.0394627153873444 current epoch 248: saving best model...\n",
      "1.0097536817193031 1.0257994309067726 current epoch 249: saving best model...\n",
      "1.0011359304189682 1.0097536817193031 current epoch 250: saving best model...\n",
      "0.9969728216528893 1.0011359304189682 current epoch 251: saving best model...\n",
      "0.9916504435241222 0.9969728216528893 current epoch 252: saving best model...\n",
      "0.9856229051947594 0.9916504435241222 current epoch 253: saving best model...\n",
      "0.9730164259672165 0.9856229051947594 current epoch 254: saving best model...\n",
      "0.9604383483529091 0.9730164259672165 current epoch 255: saving best model...\n",
      "0.956877950578928 0.9604383483529091 current epoch 256: saving best model...\n",
      "0.9462464042007923 0.956877950578928 current epoch 257: saving best model...\n",
      "0.9396341294050217 0.9462464042007923 current epoch 258: saving best model...\n",
      "0.935698926448822 0.9396341294050217 current epoch 259: saving best model...\n",
      "0.9312757924199104 0.935698926448822 current epoch 260: saving best model...\n",
      "0.9236678294837475 0.9312757924199104 current epoch 261: saving best model...\n",
      "0.9166142679750919 0.9236678294837475 current epoch 262: saving best model...\n",
      "0.9101786129176617 0.9166142679750919 current epoch 263: saving best model...\n",
      "0.9046764597296715 0.9101786129176617 current epoch 264: saving best model...\n",
      "0.8993885070085526 0.9046764597296715 current epoch 265: saving best model...\n",
      "0.8922362253069878 0.8993885070085526 current epoch 266: saving best model...\n",
      "0.8825610466301441 0.8922362253069878 current epoch 267: saving best model...\n",
      "0.8786810860037804 0.8825610466301441 current epoch 268: saving best model...\n",
      "0.8763547465205193 0.8786810860037804 current epoch 269: saving best model...\n",
      "0.8714614771306515 0.8763547465205193 current epoch 270: saving best model...\n",
      "0.8627124726772308 0.8714614771306515 current epoch 271: saving best model...\n",
      "0.86168197914958 0.8627124726772308 current epoch 274: saving best model...\n",
      "0.8558716140687466 0.86168197914958 current epoch 276: saving best model...\n",
      "0.8520116060972214 0.8558716140687466 current epoch 280: saving best model...\n",
      "0.8471452705562115 0.8520116060972214 current epoch 281: saving best model...\n",
      "0.8360655717551708 0.8471452705562115 current epoch 282: saving best model...\n",
      "0.822468712925911 0.8360655717551708 current epoch 283: saving best model...\n",
      "0.8137595392763615 0.822468712925911 current epoch 285: saving best model...\n",
      "0.8124907501041889 0.8137595392763615 current epoch 299: saving best model...\n",
      "Epoch: 300, loss after minibatch     1: 0.07481\n",
      "Epoch: 300, loss after minibatch     2: 0.09075\n",
      "Epoch: 300, loss after minibatch     3: 0.07969\n",
      "Epoch: 300, loss after minibatch     4: 0.08954\n",
      "Epoch: 300, loss after minibatch     5: 0.08254\n",
      "Epoch: 300, loss after minibatch     6: 0.07462\n",
      "Epoch: 300, loss after minibatch     7: 0.08691\n",
      "Epoch: 300, loss after minibatch     8: 0.07962\n",
      "Epoch: 300, loss after minibatch     9: 0.08568\n",
      "Epoch: 300, loss after minibatch    10: 0.04283\n",
      "+-------------+----------------------+--------------------+---------------------+\n",
      "|     Tag     |      Precision       |       Recall       |        FBeta        |\n",
      "+-------------+----------------------+--------------------+---------------------+\n",
      "|  publisher  |         0.96         |        0.96        |         0.96        |\n",
      "|    punct    |         1.0          |        1.0         |         1.0         |\n",
      "|    editor   |         1.0          |        1.0         |         1.0         |\n",
      "|    pages    | 0.008627866495118444 |        1.0         | 0.01710812636002101 |\n",
      "|     note    |         1.0          |        1.0         |         1.0         |\n",
      "|   journal   |         1.0          |        1.0         |         1.0         |\n",
      "|   location  |         1.0          |        1.0         |         1.0         |\n",
      "|    other    |  0.9913793103448276  | 0.9583333333333334 |  0.9745762711864409 |\n",
      "|    author   |  0.9956331877729258  |        1.0         |  0.9978118161925601 |\n",
      "|     date    |  0.9830508474576272  | 0.9666666666666667 |  0.9747899159663865 |\n",
      "|    volume   |  0.9841269841269841  |        1.0         |  0.9919999999999999 |\n",
      "|  booktitle  |         1.0          |        1.0         |         1.0         |\n",
      "| institution |  0.9523809523809523  | 0.9523809523809523 |  0.9523809523809523 |\n",
      "|     tech    |         1.0          | 0.9666666666666667 |  0.983050847457627  |\n",
      "|    title    |  0.990990990990991   | 0.9977324263038548 |  0.9943502824858756 |\n",
      "+-------------+----------------------+--------------------+---------------------+\n",
      "0.7870065346360207 0.8124907501041889 current epoch 300: saving best model...\n",
      "0.7687301635742188 0.7870065346360207 current epoch 301: saving best model...\n",
      "0.7500303834676743 0.7687301635742188 current epoch 302: saving best model...\n",
      "0.7339847609400749 0.7500303834676743 current epoch 303: saving best model...\n",
      "0.7260073646903038 0.7339847609400749 current epoch 304: saving best model...\n",
      "0.7203076109290123 0.7260073646903038 current epoch 305: saving best model...\n",
      "0.7151243016123772 0.7203076109290123 current epoch 306: saving best model...\n",
      "0.706568069756031 0.7151243016123772 current epoch 307: saving best model...\n",
      "0.6945602558553219 0.706568069756031 current epoch 308: saving best model...\n",
      "0.6878197379410267 0.6945602558553219 current epoch 309: saving best model...\n",
      "0.681665126234293 0.6878197379410267 current epoch 310: saving best model...\n",
      "0.6762024872004986 0.681665126234293 current epoch 311: saving best model...\n",
      "0.6719708517193794 0.6762024872004986 current epoch 312: saving best model...\n",
      "0.6680689975619316 0.6719708517193794 current epoch 313: saving best model...\n",
      "0.6641566790640354 0.6680689975619316 current epoch 314: saving best model...\n",
      "0.6597782745957375 0.6641566790640354 current epoch 315: saving best model...\n",
      "0.6539294607937336 0.6597782745957375 current epoch 316: saving best model...\n",
      "0.6497556120157242 0.6539294607937336 current epoch 317: saving best model...\n",
      "0.6469080895185471 0.6497556120157242 current epoch 318: saving best model...\n",
      "0.6440352760255337 0.6469080895185471 current epoch 319: saving best model...\n",
      "0.6409893855452538 0.6440352760255337 current epoch 320: saving best model...\n",
      "0.6355362609028816 0.6409893855452538 current epoch 321: saving best model...\n",
      "0.631536204367876 0.6355362609028816 current epoch 322: saving best model...\n",
      "0.6270688995718956 0.631536204367876 current epoch 323: saving best model...\n",
      "0.6233516670763493 0.6270688995718956 current epoch 324: saving best model...\n",
      "0.6204036474227905 0.6233516670763493 current epoch 325: saving best model...\n",
      "0.6159340813755989 0.6204036474227905 current epoch 326: saving best model...\n",
      "0.6135506182909012 0.6159340813755989 current epoch 327: saving best model...\n",
      "0.6080803796648979 0.6135506182909012 current epoch 328: saving best model...\n",
      "0.6040364764630795 0.6080803796648979 current epoch 329: saving best model...\n",
      "0.6011606603860855 0.6040364764630795 current epoch 330: saving best model...\n",
      "0.5988218672573566 0.6011606603860855 current epoch 331: saving best model...\n",
      "0.5921612605452538 0.5988218672573566 current epoch 332: saving best model...\n",
      "0.5876622013747692 0.5921612605452538 current epoch 333: saving best model...\n",
      "0.5841831248253584 0.5876622013747692 current epoch 334: saving best model...\n",
      "0.580421581864357 0.5841831248253584 current epoch 335: saving best model...\n",
      "0.5780995693057775 0.580421581864357 current epoch 336: saving best model...\n",
      "0.5728687159717083 0.5780995693057775 current epoch 337: saving best model...\n",
      "0.5698584541678429 0.5728687159717083 current epoch 338: saving best model...\n",
      "0.5679245106875896 0.5698584541678429 current epoch 339: saving best model...\n",
      "0.5642725173383951 0.5679245106875896 current epoch 340: saving best model...\n",
      "0.5607161112129688 0.5642725173383951 current epoch 341: saving best model...\n",
      "0.556427925825119 0.5607161112129688 current epoch 342: saving best model...\n",
      "0.5533822812139988 0.556427925825119 current epoch 343: saving best model...\n",
      "0.5528914034366608 0.5533822812139988 current epoch 345: saving best model...\n",
      "0.548870638012886 0.5528914034366608 current epoch 346: saving best model...\n",
      "0.5214357692748308 0.548870638012886 current epoch 367: saving best model...\n",
      "0.49129511788487434 0.5214357692748308 current epoch 368: saving best model...\n",
      "0.47997904010117054 0.49129511788487434 current epoch 369: saving best model...\n",
      "0.47376731038093567 0.47997904010117054 current epoch 370: saving best model...\n",
      "0.46766211837530136 0.47376731038093567 current epoch 371: saving best model...\n",
      "0.4630333725363016 0.46766211837530136 current epoch 372: saving best model...\n",
      "0.4594634845852852 0.4630333725363016 current epoch 373: saving best model...\n",
      "0.45687612146139145 0.4594634845852852 current epoch 374: saving best model...\n",
      "0.4539913721382618 0.45687612146139145 current epoch 375: saving best model...\n",
      "0.4513102900236845 0.4539913721382618 current epoch 376: saving best model...\n",
      "0.44921712577342987 0.4513102900236845 current epoch 377: saving best model...\n",
      "0.4470443483442068 0.44921712577342987 current epoch 378: saving best model...\n",
      "0.4448129665106535 0.4470443483442068 current epoch 379: saving best model...\n",
      "0.44318313151597977 0.4448129665106535 current epoch 380: saving best model...\n",
      "0.44202301278710365 0.44318313151597977 current epoch 381: saving best model...\n",
      "0.4409126415848732 0.44202301278710365 current epoch 382: saving best model...\n",
      "0.44021841511130333 0.4409126415848732 current epoch 383: saving best model...\n",
      "0.4382830634713173 0.44021841511130333 current epoch 384: saving best model...\n",
      "0.4382514003664255 0.4382830634713173 current epoch 385: saving best model...\n",
      "0.43715968914330006 0.4382514003664255 current epoch 386: saving best model...\n",
      "0.4317430406808853 0.43715968914330006 current epoch 395: saving best model...\n",
      "Epoch: 400, loss after minibatch     1: 0.04982\n",
      "Epoch: 400, loss after minibatch     2: 0.04716\n",
      "Epoch: 400, loss after minibatch     3: 0.04262\n",
      "Epoch: 400, loss after minibatch     4: 0.05386\n",
      "Epoch: 400, loss after minibatch     5: 0.05656\n",
      "Epoch: 400, loss after minibatch     6: 0.05141\n",
      "Epoch: 400, loss after minibatch     7: 0.04799\n",
      "Epoch: 400, loss after minibatch     8: 0.04259\n",
      "Epoch: 400, loss after minibatch     9: 0.05223\n",
      "Epoch: 400, loss after minibatch    10: 0.02904\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "|     Tag     |      Precision      |       Recall       |        FBeta        |\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "|  publisher  |         1.0         |        1.0         |         1.0         |\n",
      "|    punct    |         1.0         |        1.0         |         1.0         |\n",
      "|    editor   |         1.0         |        1.0         |         1.0         |\n",
      "|    pages    | 0.00862851952770209 |        1.0         | 0.01710941017559658 |\n",
      "|     note    |         1.0         |        1.0         |         1.0         |\n",
      "|   journal   |         1.0         | 0.9928571428571429 |  0.996415770609319  |\n",
      "|   location  |         1.0         |        1.0         |         1.0         |\n",
      "|    other    |  0.967479674796748  | 0.9916666666666667 |  0.9794238683127573 |\n",
      "|    author   |         1.0         | 0.9956140350877193 |  0.9978021978021978 |\n",
      "|     date    |  0.9833333333333333 | 0.9833333333333333 |  0.9833333333333333 |\n",
      "|    volume   |         1.0         |        1.0         |         1.0         |\n",
      "|  booktitle  |         1.0         |        1.0         |         1.0         |\n",
      "| institution |         1.0         |        1.0         |         1.0         |\n",
      "|     tech    |         1.0         |        1.0         |         1.0         |\n",
      "|    title    |  0.9977324263038548 | 0.9977324263038548 |  0.9977324263038548 |\n",
      "+-------------+---------------------+--------------------+---------------------+\n",
      "0.41339488327503204 0.4317430406808853 current epoch 418: saving best model...\n",
      "0.3895915076136589 0.41339488327503204 current epoch 419: saving best model...\n",
      "0.37305805273354053 0.3895915076136589 current epoch 420: saving best model...\n",
      "0.36073736287653446 0.37305805273354053 current epoch 421: saving best model...\n",
      "0.35440324805676937 0.36073736287653446 current epoch 422: saving best model...\n",
      "0.3496405463665724 0.35440324805676937 current epoch 423: saving best model...\n",
      "0.34657721780240536 0.3496405463665724 current epoch 424: saving best model...\n",
      "0.343107707798481 0.34657721780240536 current epoch 425: saving best model...\n",
      "0.33983185328543186 0.343107707798481 current epoch 426: saving best model...\n",
      "0.33720036037266254 0.33983185328543186 current epoch 427: saving best model...\n",
      "0.33485535345971584 0.33720036037266254 current epoch 428: saving best model...\n",
      "0.33264428563416004 0.33485535345971584 current epoch 429: saving best model...\n",
      "0.3302118852734566 0.33264428563416004 current epoch 430: saving best model...\n",
      "0.3281432492658496 0.3302118852734566 current epoch 431: saving best model...\n",
      "0.3259875113144517 0.3281432492658496 current epoch 432: saving best model...\n",
      "0.3238054234534502 0.3259875113144517 current epoch 433: saving best model...\n",
      "0.3217406179755926 0.3238054234534502 current epoch 434: saving best model...\n",
      "0.3196982555091381 0.3217406179755926 current epoch 435: saving best model...\n",
      "0.31771348789334297 0.3196982555091381 current epoch 436: saving best model...\n",
      "0.31574092619121075 0.31771348789334297 current epoch 437: saving best model...\n",
      "0.3137259082868695 0.31574092619121075 current epoch 438: saving best model...\n",
      "0.3116833567619324 0.3137259082868695 current epoch 439: saving best model...\n",
      "0.30976018123328686 0.3116833567619324 current epoch 440: saving best model...\n",
      "0.3078990215435624 0.30976018123328686 current epoch 441: saving best model...\n",
      "0.3058781372383237 0.3078990215435624 current epoch 442: saving best model...\n",
      "0.3040347406640649 0.3058781372383237 current epoch 443: saving best model...\n",
      "0.30226410925388336 0.3040347406640649 current epoch 444: saving best model...\n",
      "0.300557479262352 0.30226410925388336 current epoch 445: saving best model...\n",
      "0.2992671709507704 0.300557479262352 current epoch 446: saving best model...\n",
      "0.2970090080052614 0.2992671709507704 current epoch 447: saving best model...\n",
      "0.2957375729456544 0.2970090080052614 current epoch 448: saving best model...\n",
      "0.2933129779994488 0.2957375729456544 current epoch 449: saving best model...\n",
      "0.2913044625893235 0.2933129779994488 current epoch 450: saving best model...\n",
      "0.28947601467370987 0.2913044625893235 current epoch 451: saving best model...\n",
      "0.28779306821525097 0.28947601467370987 current epoch 452: saving best model...\n",
      "0.2858639834448695 0.28779306821525097 current epoch 453: saving best model...\n",
      "0.28424421418458223 0.2858639834448695 current epoch 454: saving best model...\n",
      "0.28306980803608894 0.28424421418458223 current epoch 455: saving best model...\n",
      "0.28131834510713816 0.28306980803608894 current epoch 456: saving best model...\n",
      "0.2793821068480611 0.28131834510713816 current epoch 457: saving best model...\n",
      "0.27789254672825336 0.2793821068480611 current epoch 458: saving best model...\n",
      "0.2758989203721285 0.27789254672825336 current epoch 459: saving best model...\n",
      "0.2741548912599683 0.2758989203721285 current epoch 460: saving best model...\n",
      "0.2724738596007228 0.2741548912599683 current epoch 461: saving best model...\n",
      "0.2709199143573642 0.2724738596007228 current epoch 462: saving best model...\n",
      "0.26899618469178677 0.2709199143573642 current epoch 463: saving best model...\n",
      "0.26738962158560753 0.26899618469178677 current epoch 464: saving best model...\n",
      "0.26576567348092794 0.26738962158560753 current epoch 465: saving best model...\n",
      "0.264477820135653 0.26576567348092794 current epoch 466: saving best model...\n",
      "0.2632201658561826 0.264477820135653 current epoch 467: saving best model...\n",
      "0.2613987661898136 0.2632201658561826 current epoch 468: saving best model...\n",
      "0.26039807591587305 0.2613987661898136 current epoch 469: saving best model...\n",
      "0.2581039136275649 0.26039807591587305 current epoch 470: saving best model...\n",
      "0.25720470398664474 0.2581039136275649 current epoch 471: saving best model...\n",
      "0.25522798020392656 0.25720470398664474 current epoch 472: saving best model...\n",
      "0.2539998013526201 0.25522798020392656 current epoch 473: saving best model...\n",
      "0.25190326757729053 0.2539998013526201 current epoch 474: saving best model...\n",
      "0.2503649489954114 0.25190326757729053 current epoch 475: saving best model...\n",
      "0.2492474615573883 0.2503649489954114 current epoch 476: saving best model...\n",
      "0.24788815900683403 0.2492474615573883 current epoch 477: saving best model...\n",
      "0.2459653066471219 0.24788815900683403 current epoch 478: saving best model...\n",
      "0.2448397921398282 0.2459653066471219 current epoch 479: saving best model...\n",
      "0.24342626798897982 0.2448397921398282 current epoch 480: saving best model...\n",
      "0.24229844566434622 0.24342626798897982 current epoch 481: saving best model...\n",
      "0.24071020353585482 0.24229844566434622 current epoch 482: saving best model...\n",
      "0.23892290890216827 0.24071020353585482 current epoch 483: saving best model...\n",
      "0.23869681637734175 0.23892290890216827 current epoch 484: saving best model...\n",
      "0.23791596945375204 0.23869681637734175 current epoch 485: saving best model...\n",
      "0.23768616747111082 0.23791596945375204 current epoch 492: saving best model...\n",
      "0.2376201692968607 0.23768616747111082 current epoch 494: saving best model...\n",
      "Epoch: 500, loss after minibatch     1: 0.05028\n",
      "Epoch: 500, loss after minibatch     2: 0.04998\n",
      "Epoch: 500, loss after minibatch     3: 0.04650\n",
      "Epoch: 500, loss after minibatch     4: 0.03854\n",
      "Epoch: 500, loss after minibatch     5: 0.04474\n",
      "Epoch: 500, loss after minibatch     6: 0.04557\n",
      "Epoch: 500, loss after minibatch     7: 0.05067\n",
      "Epoch: 500, loss after minibatch     8: 0.05422\n",
      "Epoch: 500, loss after minibatch     9: 0.04299\n",
      "Epoch: 500, loss after minibatch    10: 0.01656\n",
      "+-------------+----------------------+--------------------+----------------------+\n",
      "|     Tag     |      Precision       |       Recall       |        FBeta         |\n",
      "+-------------+----------------------+--------------------+----------------------+\n",
      "|  publisher  |         1.0          |        1.0         |         1.0          |\n",
      "|    punct    |         1.0          |        1.0         |         1.0          |\n",
      "|    editor   |         1.0          |        1.0         |         1.0          |\n",
      "|    pages    | 0.011177566428081185 |        1.0         | 0.02210801900513915  |\n",
      "|     note    |         1.0          |        1.0         |         1.0          |\n",
      "|   journal   |  0.9929078014184397  |        1.0         |  0.9964412811387899  |\n",
      "|   location  |         1.0          |        1.0         |         1.0          |\n",
      "|    other    |  0.9916666666666667  | 0.9916666666666667 |  0.9916666666666667  |\n",
      "|    author   |  0.9978070175438597  | 0.9978070175438597 |  0.9978070175438597  |\n",
      "|     date    |  0.9836065573770492  |        1.0         |  0.9917355371900827  |\n",
      "|    volume   |         1.0          |        1.0         |         1.0          |\n",
      "|  booktitle  |         1.0          |        1.0         |         1.0          |\n",
      "| institution | 0.006594131223211342 | 0.9523809523809523 | 0.013097576948264572 |\n",
      "|     tech    |         1.0          |        1.0         |         1.0          |\n",
      "|    title    |  0.9977220956719818  | 0.9931972789115646 |  0.9954545454545456  |\n",
      "+-------------+----------------------+--------------------+----------------------+\n",
      "0.2329263398423791 0.2376201692968607 current epoch 510: saving best model...\n",
      "0.22915643453598022 0.2329263398423791 current epoch 511: saving best model...\n",
      "0.22605772502720356 0.22915643453598022 current epoch 512: saving best model...\n",
      "0.22470203321427107 0.22605772502720356 current epoch 513: saving best model...\n",
      "0.22343430668115616 0.22470203321427107 current epoch 514: saving best model...\n",
      "0.22201089840382338 0.22343430668115616 current epoch 515: saving best model...\n",
      "0.22036843933165073 0.22201089840382338 current epoch 516: saving best model...\n",
      "0.21857493463903666 0.22036843933165073 current epoch 517: saving best model...\n",
      "0.2166148889809847 0.21857493463903666 current epoch 518: saving best model...\n",
      "0.21444388013333082 0.2166148889809847 current epoch 519: saving best model...\n",
      "0.2122198659926653 0.21444388013333082 current epoch 520: saving best model...\n",
      "0.2098894463852048 0.2122198659926653 current epoch 521: saving best model...\n",
      "0.20779398921877146 0.2098894463852048 current epoch 522: saving best model...\n",
      "0.20606631226837635 0.20779398921877146 current epoch 523: saving best model...\n",
      "0.20481618866324425 0.20606631226837635 current epoch 524: saving best model...\n",
      "0.20378850493580103 0.20481618866324425 current epoch 525: saving best model...\n",
      "0.20290065184235573 0.20378850493580103 current epoch 526: saving best model...\n",
      "0.20180316641926765 0.20290065184235573 current epoch 527: saving best model...\n",
      "0.20045061968266964 0.20180316641926765 current epoch 528: saving best model...\n",
      "0.19883702136576176 0.20045061968266964 current epoch 529: saving best model...\n",
      "0.19724673498421907 0.19883702136576176 current epoch 530: saving best model...\n",
      "0.19570119678974152 0.19724673498421907 current epoch 531: saving best model...\n",
      "0.1941836979240179 0.19570119678974152 current epoch 532: saving best model...\n",
      "0.1932395761832595 0.1941836979240179 current epoch 533: saving best model...\n",
      "0.19300245121121407 0.1932395761832595 current epoch 534: saving best model...\n",
      "0.1926647499203682 0.19300245121121407 current epoch 540: saving best model...\n",
      "0.19193564308807254 0.1926647499203682 current epoch 541: saving best model...\n",
      "0.1909312792122364 0.19193564308807254 current epoch 542: saving best model...\n",
      "0.18984265718609095 0.1909312792122364 current epoch 543: saving best model...\n",
      "0.18857339583337307 0.18984265718609095 current epoch 544: saving best model...\n",
      "0.18737373501062393 0.18857339583337307 current epoch 545: saving best model...\n",
      "0.18619860522449017 0.18737373501062393 current epoch 546: saving best model...\n",
      "0.18588491063565016 0.18619860522449017 current epoch 547: saving best model...\n",
      "0.17257364699617028 0.18588491063565016 current epoch 576: saving best model...\n",
      "0.166920127812773 0.17257364699617028 current epoch 577: saving best model...\n",
      "0.15805265493690968 0.166920127812773 current epoch 578: saving best model...\n",
      "0.1549052488990128 0.15805265493690968 current epoch 579: saving best model...\n",
      "0.15357504040002823 0.1549052488990128 current epoch 580: saving best model...\n",
      "0.15172755485400558 0.15357504040002823 current epoch 581: saving best model...\n",
      "0.15026622312143445 0.15172755485400558 current epoch 582: saving best model...\n",
      "0.1490918113850057 0.15026622312143445 current epoch 583: saving best model...\n",
      "0.1481023789383471 0.1490918113850057 current epoch 584: saving best model...\n",
      "0.14723714301362634 0.1481023789383471 current epoch 585: saving best model...\n",
      "0.14618786610662937 0.14723714301362634 current epoch 586: saving best model...\n",
      "0.14522114908322692 0.14618786610662937 current epoch 587: saving best model...\n",
      "0.14434133423492312 0.14522114908322692 current epoch 588: saving best model...\n",
      "0.14341311622411013 0.14434133423492312 current epoch 589: saving best model...\n",
      "0.14246523147448897 0.14341311622411013 current epoch 590: saving best model...\n",
      "0.14169364562258124 0.14246523147448897 current epoch 591: saving best model...\n",
      "0.1407285463064909 0.14169364562258124 current epoch 592: saving best model...\n",
      "0.13984180055558681 0.1407285463064909 current epoch 593: saving best model...\n",
      "0.1390359322540462 0.13984180055558681 current epoch 594: saving best model...\n",
      "0.13820716552436352 0.1390359322540462 current epoch 595: saving best model...\n",
      "0.13739853957667947 0.13820716552436352 current epoch 596: saving best model...\n",
      "0.13672991935163736 0.13739853957667947 current epoch 597: saving best model...\n",
      "0.1358837252482772 0.13672991935163736 current epoch 598: saving best model...\n",
      "0.13529916992411017 0.1358837252482772 current epoch 599: saving best model...\n",
      "Epoch: 600, loss after minibatch     1: 0.01220\n",
      "Epoch: 600, loss after minibatch     2: 0.01550\n",
      "Epoch: 600, loss after minibatch     3: 0.01476\n",
      "Epoch: 600, loss after minibatch     4: 0.01512\n",
      "Epoch: 600, loss after minibatch     5: 0.01442\n",
      "Epoch: 600, loss after minibatch     6: 0.01263\n",
      "Epoch: 600, loss after minibatch     7: 0.01501\n",
      "Epoch: 600, loss after minibatch     8: 0.01437\n",
      "Epoch: 600, loss after minibatch     9: 0.01473\n",
      "Epoch: 600, loss after minibatch    10: 0.00566\n",
      "+-------------+----------------------+--------------------+---------------------+\n",
      "|     Tag     |      Precision       |       Recall       |        FBeta        |\n",
      "+-------------+----------------------+--------------------+---------------------+\n",
      "|  publisher  |         1.0          |        1.0         |         1.0         |\n",
      "|    punct    |         1.0          |        1.0         |         1.0         |\n",
      "|    editor   |         1.0          |        1.0         |         1.0         |\n",
      "|    pages    |         1.0          |        1.0         |         1.0         |\n",
      "|     note    |         1.0          |        1.0         |         1.0         |\n",
      "|   journal   |         1.0          |        1.0         |         1.0         |\n",
      "|   location  |         1.0          |        1.0         |         1.0         |\n",
      "|    other    |         1.0          | 0.9916666666666667 |   0.99581589958159  |\n",
      "|    author   |         1.0          |        1.0         |         1.0         |\n",
      "|     date    |  0.9917355371900827  |        1.0         |  0.995850622406639  |\n",
      "|    volume   |         1.0          |        1.0         |         1.0         |\n",
      "|  booktitle  |         1.0          |        1.0         |         1.0         |\n",
      "| institution | 0.001600731763091699 |        1.0         | 0.00319634703196347 |\n",
      "|     tech    |         1.0          |        1.0         |         1.0         |\n",
      "|    title    |         1.0          |        1.0         |         1.0         |\n",
      "+-------------+----------------------+--------------------+---------------------+\n",
      "0.13440672913566232 0.13529916992411017 current epoch 600: saving best model...\n",
      "0.1336059425957501 0.13440672913566232 current epoch 601: saving best model...\n",
      "0.13292947225272655 0.1336059425957501 current epoch 602: saving best model...\n",
      "0.1321098729968071 0.13292947225272655 current epoch 603: saving best model...\n",
      "0.13133332692086697 0.1321098729968071 current epoch 604: saving best model...\n",
      "0.13078695628792048 0.13133332692086697 current epoch 605: saving best model...\n",
      "0.12990041449666023 0.13078695628792048 current epoch 606: saving best model...\n",
      "0.12917611142620444 0.12990041449666023 current epoch 607: saving best model...\n",
      "0.12867491133511066 0.12917611142620444 current epoch 608: saving best model...\n",
      "0.1278046891093254 0.12867491133511066 current epoch 609: saving best model...\n",
      "0.12699691439047456 0.1278046891093254 current epoch 610: saving best model...\n",
      "0.12631400348618627 0.12699691439047456 current epoch 611: saving best model...\n",
      "0.12559855030849576 0.12631400348618627 current epoch 612: saving best model...\n",
      "0.12494515208527446 0.12559855030849576 current epoch 613: saving best model...\n",
      "0.12469456112012267 0.12494515208527446 current epoch 614: saving best model...\n",
      "0.12377750081941485 0.12469456112012267 current epoch 615: saving best model...\n",
      "0.12294974830001593 0.12377750081941485 current epoch 616: saving best model...\n",
      "0.12228079698979855 0.12294974830001593 current epoch 617: saving best model...\n",
      "0.12148123187944293 0.12228079698979855 current epoch 618: saving best model...\n",
      "0.12075892137363553 0.12148123187944293 current epoch 619: saving best model...\n",
      "0.12009010137990117 0.12075892137363553 current epoch 620: saving best model...\n",
      "0.11936195334419608 0.12009010137990117 current epoch 621: saving best model...\n",
      "0.11870813835412264 0.11936195334419608 current epoch 622: saving best model...\n",
      "0.11817901069298387 0.11870813835412264 current epoch 623: saving best model...\n",
      "0.1173635795712471 0.11817901069298387 current epoch 624: saving best model...\n",
      "0.11670873267576098 0.1173635795712471 current epoch 625: saving best model...\n",
      "0.11613955721259117 0.11670873267576098 current epoch 626: saving best model...\n",
      "0.11545735877007246 0.11613955721259117 current epoch 627: saving best model...\n",
      "0.11492978408932686 0.11545735877007246 current epoch 628: saving best model...\n",
      "0.11450706887990236 0.11492978408932686 current epoch 629: saving best model...\n",
      "0.11409169901162386 0.11450706887990236 current epoch 630: saving best model...\n",
      "0.11313939560204744 0.11409169901162386 current epoch 631: saving best model...\n",
      "0.11257967678830028 0.11313939560204744 current epoch 632: saving best model...\n",
      "0.11234159674495459 0.11257967678830028 current epoch 633: saving best model...\n",
      "0.11176449107006192 0.11234159674495459 current epoch 634: saving best model...\n",
      "0.11069413274526596 0.11176449107006192 current epoch 635: saving best model...\n",
      "0.11007434176281095 0.11069413274526596 current epoch 636: saving best model...\n",
      "0.10929952329024673 0.11007434176281095 current epoch 637: saving best model...\n",
      "0.10870791133493185 0.10929952329024673 current epoch 638: saving best model...\n",
      "0.10799559485167265 0.10870791133493185 current epoch 640: saving best model...\n",
      "0.10729740699753165 0.10799559485167265 current epoch 641: saving best model...\n",
      "0.10688268393278122 0.10729740699753165 current epoch 642: saving best model...\n",
      "0.10616293922066689 0.10688268393278122 current epoch 643: saving best model...\n",
      "0.10606442578136921 0.10616293922066689 current epoch 644: saving best model...\n",
      "0.10533202765509486 0.10606442578136921 current epoch 645: saving best model...\n",
      "0.10461844597011805 0.10533202765509486 current epoch 646: saving best model...\n",
      "0.1041263253428042 0.10461844597011805 current epoch 647: saving best model...\n",
      "0.10384568199515343 0.1041263253428042 current epoch 648: saving best model...\n",
      "0.10315512539818883 0.10384568199515343 current epoch 649: saving best model...\n",
      "0.10255131730809808 0.10315512539818883 current epoch 650: saving best model...\n",
      "0.10184969287365675 0.10255131730809808 current epoch 651: saving best model...\n",
      "0.10133511992171407 0.10184969287365675 current epoch 652: saving best model...\n",
      "0.10057915328070521 0.10133511992171407 current epoch 653: saving best model...\n",
      "0.1001356360502541 0.10057915328070521 current epoch 654: saving best model...\n",
      "0.10000968957319856 0.1001356360502541 current epoch 655: saving best model...\n",
      "0.09946447657421231 0.10000968957319856 current epoch 656: saving best model...\n",
      "0.09860520204529166 0.09946447657421231 current epoch 657: saving best model...\n",
      "0.09831570647656918 0.09860520204529166 current epoch 658: saving best model...\n",
      "0.09781170729547739 0.09831570647656918 current epoch 659: saving best model...\n",
      "0.09714047331362963 0.09781170729547739 current epoch 660: saving best model...\n",
      "0.09638806246221066 0.09714047331362963 current epoch 661: saving best model...\n",
      "0.09596782177686691 0.09638806246221066 current epoch 662: saving best model...\n",
      "0.0957749835215509 0.09596782177686691 current epoch 663: saving best model...\n",
      "0.09574402775615454 0.0957749835215509 current epoch 664: saving best model...\n",
      "0.09555748384445906 0.09574402775615454 current epoch 665: saving best model...\n",
      "0.09528542775660753 0.09555748384445906 current epoch 666: saving best model...\n",
      "0.09521821117959917 0.09528542775660753 current epoch 673: saving best model...\n",
      "0.09418235323391855 0.09521821117959917 current epoch 674: saving best model...\n",
      "0.09321070788428187 0.09418235323391855 current epoch 675: saving best model...\n",
      "0.0929191904142499 0.09321070788428187 current epoch 676: saving best model...\n",
      "Epoch: 700, loss after minibatch     1: 0.01902\n",
      "Epoch: 700, loss after minibatch     2: 0.02989\n",
      "Epoch: 700, loss after minibatch     3: 0.02568\n",
      "Epoch: 700, loss after minibatch     4: 0.02395\n",
      "Epoch: 700, loss after minibatch     5: 0.02780\n",
      "Epoch: 700, loss after minibatch     6: 0.02241\n",
      "Epoch: 700, loss after minibatch     7: 0.03568\n",
      "Epoch: 700, loss after minibatch     8: 0.02968\n",
      "Epoch: 700, loss after minibatch     9: 0.02707\n",
      "Epoch: 700, loss after minibatch    10: 0.01438\n",
      "+-------------+----------------------+--------------------+---------------------+\n",
      "|     Tag     |      Precision       |       Recall       |        FBeta        |\n",
      "+-------------+----------------------+--------------------+---------------------+\n",
      "|  publisher  |         1.0          |        1.0         |         1.0         |\n",
      "|    punct    |         1.0          |        1.0         |         1.0         |\n",
      "|    editor   |         0.95         |        1.0         |  0.9743589743589743 |\n",
      "|    pages    |         1.0          |        1.0         |         1.0         |\n",
      "|     note    |         1.0          |        1.0         |         1.0         |\n",
      "|   journal   |  0.9929078014184397  |        1.0         |  0.9964412811387899 |\n",
      "|   location  |         1.0          | 0.9444444444444444 |  0.9714285714285714 |\n",
      "|    other    |         1.0          |       0.975        |  0.9873417721518987 |\n",
      "|    author   |         1.0          | 0.9956140350877193 |  0.9978021978021978 |\n",
      "|     date    |  0.9916666666666667  | 0.9916666666666667 |  0.9916666666666667 |\n",
      "|    volume   |         1.0          |        1.0         |         1.0         |\n",
      "|  booktitle  |         1.0          |        1.0         |         1.0         |\n",
      "| institution | 0.001600731763091699 |        1.0         | 0.00319634703196347 |\n",
      "|     tech    |         1.0          |        1.0         |         1.0         |\n",
      "|    title    |  0.9910112359550561  |        1.0         |  0.9954853273137697 |\n",
      "+-------------+----------------------+--------------------+---------------------+\n",
      "0.09242604440078139 0.0929191904142499 current epoch 721: saving best model...\n",
      "0.09156436985358596 0.09242604440078139 current epoch 722: saving best model...\n",
      "0.09016033518128097 0.09156436985358596 current epoch 723: saving best model...\n",
      "0.08908573514781892 0.09016033518128097 current epoch 724: saving best model...\n",
      "0.08822879707440734 0.08908573514781892 current epoch 725: saving best model...\n",
      "0.0873093067202717 0.08822879707440734 current epoch 726: saving best model...\n",
      "0.08646609773859382 0.0873093067202717 current epoch 727: saving best model...\n",
      "0.08572426903992891 0.08646609773859382 current epoch 728: saving best model...\n",
      "0.0849539595656097 0.08572426903992891 current epoch 729: saving best model...\n",
      "0.08423418854363263 0.0849539595656097 current epoch 730: saving best model...\n",
      "0.08355543296784163 0.08423418854363263 current epoch 731: saving best model...\n",
      "0.08289897185750306 0.08355543296784163 current epoch 732: saving best model...\n",
      "0.082255226559937 0.08289897185750306 current epoch 733: saving best model...\n",
      "0.08166077313944697 0.082255226559937 current epoch 734: saving best model...\n",
      "0.08104753168299794 0.08166077313944697 current epoch 735: saving best model...\n",
      "0.08047775248996913 0.08104753168299794 current epoch 736: saving best model...\n",
      "0.07990870042704046 0.08047775248996913 current epoch 737: saving best model...\n",
      "0.0793871721252799 0.07990870042704046 current epoch 738: saving best model...\n",
      "0.07882536365650594 0.0793871721252799 current epoch 739: saving best model...\n",
      "0.07829780876636505 0.07882536365650594 current epoch 740: saving best model...\n",
      "0.07777493353933096 0.07829780876636505 current epoch 741: saving best model...\n",
      "0.07729313732124865 0.07777493353933096 current epoch 742: saving best model...\n",
      "0.07678196486085653 0.07729313732124865 current epoch 743: saving best model...\n",
      "0.07631478272378445 0.07678196486085653 current epoch 744: saving best model...\n",
      "0.07579811220057309 0.07631478272378445 current epoch 745: saving best model...\n",
      "0.07530554896220565 0.07579811220057309 current epoch 746: saving best model...\n",
      "0.07478569378145039 0.07530554896220565 current epoch 747: saving best model...\n",
      "0.07428417797200382 0.07478569378145039 current epoch 748: saving best model...\n",
      "0.07374784001149237 0.07428417797200382 current epoch 749: saving best model...\n",
      "0.07327112182974815 0.07374784001149237 current epoch 750: saving best model...\n",
      "0.07281801197677851 0.07327112182974815 current epoch 751: saving best model...\n",
      "0.07239357265643775 0.07281801197677851 current epoch 752: saving best model...\n",
      "0.07196032768115401 0.07239357265643775 current epoch 753: saving best model...\n",
      "0.07153539266437292 0.07196032768115401 current epoch 754: saving best model...\n",
      "0.07109795324504375 0.07153539266437292 current epoch 755: saving best model...\n",
      "0.0706926491111517 0.07109795324504375 current epoch 756: saving best model...\n",
      "0.0702778052072972 0.0706926491111517 current epoch 757: saving best model...\n",
      "0.06989647774025798 0.0702778052072972 current epoch 758: saving best model...\n",
      "0.06949522136710584 0.06989647774025798 current epoch 759: saving best model...\n",
      "0.06910968269221485 0.06949522136710584 current epoch 760: saving best model...\n",
      "0.06869761529378593 0.06910968269221485 current epoch 761: saving best model...\n",
      "0.0683266140986234 0.06869761529378593 current epoch 762: saving best model...\n",
      "0.06791508104652166 0.0683266140986234 current epoch 763: saving best model...\n",
      "0.06753711472265422 0.06791508104652166 current epoch 764: saving best model...\n",
      "0.06712799449451268 0.06753711472265422 current epoch 765: saving best model...\n",
      "0.06675686128437519 0.06712799449451268 current epoch 766: saving best model...\n",
      "0.0663392546121031 0.06675686128437519 current epoch 767: saving best model...\n",
      "0.06597219640389085 0.0663392546121031 current epoch 768: saving best model...\n",
      "0.06557699851691723 0.06597219640389085 current epoch 769: saving best model...\n",
      "0.06523430719971657 0.06557699851691723 current epoch 770: saving best model...\n",
      "0.06484518479555845 0.06523430719971657 current epoch 771: saving best model...\n",
      "0.06451191613450646 0.06484518479555845 current epoch 772: saving best model...\n",
      "0.06414589821361005 0.06451191613450646 current epoch 773: saving best model...\n",
      "0.06383112608455122 0.06414589821361005 current epoch 774: saving best model...\n",
      "0.06348616140894592 0.06383112608455122 current epoch 775: saving best model...\n",
      "0.06316445861011744 0.06348616140894592 current epoch 776: saving best model...\n",
      "0.06280202744528651 0.06316445861011744 current epoch 777: saving best model...\n",
      "0.06249869684688747 0.06280202744528651 current epoch 778: saving best model...\n",
      "0.06214303965680301 0.06249869684688747 current epoch 779: saving best model...\n",
      "0.061828883830457926 0.06214303965680301 current epoch 780: saving best model...\n",
      "0.06148232473060489 0.061828883830457926 current epoch 781: saving best model...\n",
      "0.06117632379755378 0.06148232473060489 current epoch 782: saving best model...\n",
      "0.06082428665831685 0.06117632379755378 current epoch 783: saving best model...\n",
      "0.06051318068057299 0.06082428665831685 current epoch 784: saving best model...\n",
      "0.06016835896298289 0.06051318068057299 current epoch 785: saving best model...\n",
      "0.059868728276342154 0.06016835896298289 current epoch 786: saving best model...\n",
      "0.05951897311024368 0.059868728276342154 current epoch 787: saving best model...\n",
      "0.05921562248840928 0.05951897311024368 current epoch 788: saving best model...\n",
      "0.05887749302200973 0.05921562248840928 current epoch 789: saving best model...\n",
      "0.0585859683342278 0.05887749302200973 current epoch 790: saving best model...\n",
      "0.05824857112020254 0.0585859683342278 current epoch 791: saving best model...\n",
      "0.05795775959268212 0.05824857112020254 current epoch 792: saving best model...\n",
      "0.05763428704813123 0.05795775959268212 current epoch 793: saving best model...\n",
      "0.057357451412826777 0.05763428704813123 current epoch 794: saving best model...\n",
      "0.05703752231784165 0.057357451412826777 current epoch 795: saving best model...\n",
      "0.056757304118946195 0.05703752231784165 current epoch 796: saving best model...\n",
      "0.05644653527997434 0.056757304118946195 current epoch 797: saving best model...\n",
      "0.05617540515959263 0.05644653527997434 current epoch 798: saving best model...\n",
      "0.05586846056394279 0.05617540515959263 current epoch 799: saving best model...\n",
      "\n",
      "+-------------+---------------------+---------------------+---------------------+\n",
      "|     Tag     |      Precision      |        Recall       |        FBeta        |\n",
      "+-------------+---------------------+---------------------+---------------------+\n",
      "|  publisher  |  0.6688741721854304 |  0.6047904191616766 |  0.6352201257861635 |\n",
      "|    punct    |  0.998172453895996  |  0.998504237992355  |  0.9983383183781988 |\n",
      "|    editor   |  0.743801652892562  |  0.5232558139534884 |  0.6143344709897611 |\n",
      "|    pages    |  0.9051833122629582 |  0.9335071707953064 |  0.9191270860077022 |\n",
      "|     note    |  0.4166666666666667 | 0.13513513513513514 | 0.20408163265306126 |\n",
      "|   journal   |  0.8377880184331797 |  0.8362465501379944 |  0.8370165745856354 |\n",
      "|   location  |  0.7402597402597403 |         0.76        |         0.75        |\n",
      "|    other    |  0.6825028968713789 |  0.6817129629629629 |  0.6821077012159815 |\n",
      "|    author   |  0.9447129909365559 |  0.9513233951931853 |  0.9480066696983479 |\n",
      "|     date    |  0.9208523592085236 |  0.9365325077399381 |  0.9286262471220262 |\n",
      "|    volume   |  0.8814317673378076 |  0.8565217391304348 |  0.868798235942668  |\n",
      "|  booktitle  |  0.7692307692307693 |  0.7768361581920904 |  0.7730147575544625 |\n",
      "| institution | 0.42857142857142855 |         0.52        | 0.46987951807228917 |\n",
      "|     tech    |  0.6111111111111112 |  0.6470588235294118 |  0.6285714285714287 |\n",
      "|    title    |  0.9390669628689305 |  0.9441608168474792 |  0.9416070007955449 |\n",
      "+-------------+---------------------+---------------------+---------------------+\n",
      "Test F1 macro score: 0.7465819844915514\n",
      "Test F1 micro score: 0.9212540393446339\n",
      "[['0.60479', '0.00000', '0.01198', '0.00599', '0.00000', '0.13174', '0.05988', '0.06587', '0.01198', '0.01198', '0.00000', '0.04192', '0.01198', '0.00599', '0.03593'], ['0.00017', '0.99850', '0.00000', '0.00000', '0.00017', '0.00000', '0.00000', '0.00033', '0.00000', '0.00000', '0.00000', '0.00033', '0.00000', '0.00000', '0.00050'], ['0.00000', '0.00000', '0.52326', '0.00000', '0.00000', '0.01744', '0.00581', '0.07558', '0.27907', '0.00000', '0.00000', '0.06395', '0.00000', '0.00000', '0.03488'], ['0.00391', '0.00000', '0.00000', '0.93351', '0.00000', '0.00130', '0.00261', '0.01956', '0.00000', '0.01043', '0.02347', '0.00261', '0.00000', '0.00000', '0.00261'], ['0.02703', '0.02703', '0.02703', '0.00000', '0.13514', '0.08108', '0.00000', '0.37838', '0.00000', '0.02703', '0.00000', '0.05405', '0.10811', '0.05405', '0.08108'], ['0.01012', '0.00092', '0.00368', '0.00184', '0.00276', '0.83625', '0.00184', '0.03220', '0.01840', '0.00460', '0.00276', '0.03772', '0.00368', '0.00552', '0.03772'], ['0.08000', '0.00000', '0.00000', '0.01333', '0.00000', '0.02000', '0.76000', '0.04000', '0.00000', '0.00000', '0.00667', '0.03333', '0.04667', '0.00000', '0.00000'], ['0.01042', '0.00347', '0.00810', '0.03125', '0.00116', '0.02546', '0.00926', '0.68171', '0.06829', '0.01042', '0.01968', '0.06366', '0.01157', '0.00463', '0.05093'], ['0.00030', '0.00030', '0.00152', '0.00183', '0.00000', '0.00913', '0.00091', '0.01612', '0.95132', '0.00274', '0.00183', '0.00091', '0.00000', '0.00000', '0.01308'], ['0.00310', '0.00155', '0.00310', '0.00774', '0.00000', '0.00619', '0.00774', '0.01238', '0.00155', '0.93653', '0.00310', '0.00155', '0.00000', '0.00000', '0.01548'], ['0.00000', '0.00000', '0.00000', '0.05000', '0.00000', '0.01304', '0.00217', '0.05870', '0.00435', '0.00652', '0.85652', '0.00435', '0.00000', '0.00000', '0.00435'], ['0.00989', '0.00000', '0.01271', '0.01271', '0.00141', '0.04096', '0.00989', '0.06215', '0.00141', '0.01412', '0.00282', '0.77684', '0.01554', '0.00282', '0.03672'], ['0.01333', '0.00000', '0.00000', '0.00000', '0.01333', '0.06667', '0.01333', '0.08000', '0.00000', '0.00000', '0.00000', '0.09333', '0.52000', '0.12000', '0.08000'], ['0.00000', '0.00000', '0.01471', '0.00000', '0.00000', '0.10294', '0.00000', '0.11765', '0.01471', '0.01471', '0.00000', '0.04412', '0.04412', '0.64706', '0.00000'], ['0.00064', '0.00128', '0.00000', '0.00000', '0.00000', '0.01308', '0.00000', '0.01021', '0.01563', '0.00128', '0.00128', '0.00766', '0.00351', '0.00128', '0.94416']]\n",
      "[[, punct, punct] [5, other, other] [], punct, punct] [P., author, author] [J., author, author] [Steinhardt, author, author] [and, author, author] [F., author, author] [S., author, author] [Accetta, author, author] [,, punct, punct] [Phys, journal, journal] [., punct, punct] [Rev, journal, journal] [., punct, punct] [Lett, journal, journal] [., punct, punct] [64, volume, volume] [,, punct, punct] [2740, pages, pages] [(, punct, punct] [1990, date, date] [), punct, punct] [., punct, punct] \n",
      "[[, punct, punct] [29, other, other] [], punct, punct] [S., author, author] [M., author, author] [Salamon, author, author] [,, punct, punct] [Quaternionic, title, title] [structures, title, title] [and, title, title] [twistor, title, title] [spaces, title, title] [,, punct, punct] [in, booktitle, other] [Global, booktitle, other] [Riemannian, booktitle, other] [geometry, booktitle, other] \n",
      "[[, punct, punct] [AxKo, other, other] [], punct, punct] [J, author, author] [., punct, punct] [Ax, author, author] [and, author, author] [S., author, author] [Kochen, author, author] [,, punct, punct] [``, title, title] [Diophantine, title, title] [problems, title, title] [over, title, title] [local, title, title] [rings, title, title] [I, title, title] [,, punct, punct] [``, title, title] [Amer, journal, journal] [., punct, punct] [J, journal, journal] [., punct, punct] [Math, journal, journal] [., punct, punct] [87, volume, volume] [(, punct, punct] [1965, date, date] [), punct, punct] [,, punct, punct] [605-630, pages, pages] [., punct, punct] \n",
      "[Danvy, author, author] [,, punct, punct] [O., author, author] [and, author, author] [A., author, author] [Filinski, author, author] [., punct, punct] [Representing, title, title] [control, title, title] [:, punct, punct] [A, title, title] [study, title, title] [of, title, title] [the, title, title] [CPS, title, title] [transformation, title, title] [., punct, punct] [Tech, tech, tech] [., punct, punct] [Rpt, tech, tech] [., punct, punct] [CIS-91-2, tech, other] [., punct, punct] [Kansas, institution, other] [State, institution, other] [University, institution, institution] [,, punct, punct] [1991, date, date] [., punct, punct] \n",
      "[[, punct, punct] [11, other, other] [], punct, punct] [Keller, author, author] [G., other, author] [Stochastic, title, author] [stability, title, title] [in, title, title] [some, title, title] [chaotic, title, title] [dynamical, title, title] [systems, title, title] [,, punct, punct] [Mh, journal, journal] [., punct, punct] [Math, journal, journal] [., punct, punct] [,, punct, punct] [94, volume, volume] [(, punct, punct] [1982, date, date] [), punct, punct] [,, punct, punct] [313-333, pages, pages] [., punct, punct] \n",
      "[[, punct, punct] [6, other, other] [], punct, punct] [Chen, author, author] [,, punct, punct] [J, author, author] [., punct, punct] [(, punct, punct] [1992, date, date] [), punct, punct] [., punct, punct] [Some, title, title] [results, title, title] [on, title, title] [2, title, title] [n-, title, title] [k, title, title] [fractional, title, title] [factorial, title, title] [designs, title, title] [and, title, title] [search, title, title] [for, title, title] [minimum, title, title] [aberration, title, title] [designs, title, title] [., punct, punct] [Ann, journal, journal] [., punct, punct] [Statist, journal, journal] [., punct, punct] [20, volume, volume] [2124, pages, pages] [--, pages, pages] [2141, pages, pages] [., punct, punct] [MR1193330, other, other] \n",
      "[Yao, author, author] [,, punct, punct] [A, author, title] [., punct, punct] [(, punct, punct] [1982, date, date] [), punct, punct] [,, punct, punct] [Protocols, title, title] [for, title, title] [secure, title, title] [computations, title, title] [,, punct, punct] [in, other, other] [`, punct, punct] [FOCS, booktitle, booktitle] [1982, booktitle, date] [:, punct, punct] [Proceedings, booktitle, booktitle] [of, booktitle, booktitle] [23rd, booktitle, booktitle] [Annual, booktitle, booktitle] [Symposium, booktitle, booktitle] [on, booktitle, booktitle] [Foundations, booktitle, booktitle] [of, booktitle, booktitle] [Computer, booktitle, booktitle] [Science, booktitle, booktitle] [', punct, punct] [,, punct, punct] [IEEE, other, booktitle] [Computer, other, booktitle] [Society, other, location] [,, punct, punct] [pp, pages, pages] [., punct, punct] [160, pages, volume] [--, pages, pages] [164, pages, pages] [., punct, punct] [doi:10.1109/SFCS.1982.38, other, other] \n",
      "[Barabasi, author, author] [,, punct, punct] [A., author, author] [L., author, author] [,, punct, punct] [&, punct, punct] [Albert, author, author] [,, punct, punct] [R., other, author] [(, punct, punct] [1999, date, date] [), punct, punct] [., punct, punct] [Emergence, title, title] [of, title, title] [scaling, title, title] [in, title, title] [random, title, title] [networks, title, title] [., punct, punct] [Science, journal, journal] [,, punct, punct] [286, volume, volume] [,, punct, punct] [509, pages, pages] [--, pages, pages] [512, pages, pages] [., punct, punct] \n",
      "[32, other, other] [., punct, punct] [S., author, author] [Capozziello, author, author] [,, punct, punct] [G., author, author] [Iovane, author, author] [,, punct, punct] [G., author, author] [Lambiase, author, author] [,, punct, punct] [and, author, author] [C., author, author] [Stornaiolo, author, author] [,, punct, punct] [Europhys, journal, journal] [., punct, punct] [Lett, journal, journal] [., punct, punct] [46, volume, volume] [(, punct, punct] [1999, date, date] [), punct, punct] [710, pages, pages] [., punct, punct] \n",
      "[[, punct, punct] [14, other, other] [], punct, punct] [,, punct, punct] [Formal, title, title] [languages, title, title] [and, title, title] [global, title, title] [cellular, title, title] [automaton, title, title] [behavior, title, title] [,, punct, punct] [In, other, booktitle] [Gutowitz, other, publisher] [[, punct, punct] [6, other, other] [], punct, punct] [., punct, punct] \n",
      "[[, punct, punct] [12, other, other] [], punct, punct] [K., author, author] [Bernl¨ohr, author, author] [et, author, author] [al, author, author] [., punct, punct] [,, punct, punct] [Nucl, journal, journal] [., punct, punct] [Instr, journal, journal] [., punct, punct] [and, journal, journal] [Meth, journal, journal] [., punct, punct] [A, journal, journal] [369, volume, volume] [(, punct, punct] [1996, date, date] [), punct, punct] [293, pages, pages] [., punct, punct] \n",
      "[G., author, author] [Moore, author, author] [and, author, author] [N., author, author] [Seiberg, author, author] [,, punct, punct] [Phys.Lett, journal, journal] [., punct, punct] [B, journal, journal] [220, volume, volume] [(, punct, punct] [1989, date, date] [), punct, punct] [422, pages, pages] [;, punct, punct] \n",
      "[[, punct, punct] [10, other, other] [], punct, punct] [Conlon, author, author] [,, punct, punct] [T., author, author] [,, punct, punct] [Crane, author, author] [,, punct, punct] [M., author, author] [,, punct, punct] [Ruskin, author, author] [,, punct, punct] [H., author, other] [J., author, author] [,, punct, punct] [Physica, journal, journal] [A, journal, journal] [387, volume, volume] [(, punct, punct] [21, other, other] [), punct, punct] [(, punct, punct] [2008, date, date] [), punct, punct] [5197-5204, pages, pages] [., punct, punct] \n",
      "[[, punct, punct] [G-N-N, other, other] [], punct, punct] [B., author, author] [Gidas, author, author] [,, punct, punct] [W., author, author] [-M, author, author] [., punct, punct] [Ni, author, author] [,, punct, punct] [and, author, author] [L., author, author] [Nirenberg, author, author] [,, punct, punct] [Symmetry, title, title] [of, title, title] [positive, title, title] [solutions, title, title] [of, title, title] [nonlinear, title, title] [elliptic, title, title] [equations, title, title] [in, title, title] [R, title, title] [n, title, title] [,, punct, punct] [Adv, journal, journal] [., punct, punct] [in, journal, other] [Math, journal, other] [., punct, punct] [Supplementary, journal, journal] [Stud, journal, journal] [., punct, punct] [7A, volume, other] [(, punct, punct] [1981, date, date] [), punct, punct] [pp, pages, pages] [369, pages, pages] [--, pages, pages] [402, pages, pages] \n",
      "[L., author, author] [G., author, author] [Valiant, author, author] [., punct, punct] [A, title, title] [Bridging, title, title] [Model, title, title] [for, title, title] [Parallel, title, title] [Computation, title, title] [., punct, punct] [Communications, journal, journal] [of, journal, journal] [the, journal, title] [ACM, journal, journal] [,, punct, punct] [33, volume, volume] [(, punct, punct] [8, volume, pages] [), punct, punct] [,, punct, punct] [103-111, pages, pages] [,, punct, punct] [1990, date, date] [., punct, punct] \n",
      "[Neistein, author, author] [E., author, author] [,, punct, punct] [Khochfar, author, author] [S., author, author] [,, punct, punct] [Dalla, author, author] [Vecchia, author, author] [C., author, author] [,, punct, punct] [Schaye, author, author] [S., author, author] [,, punct, punct] [2011, date, date] [b, date, date] [,, punct, punct] [preprint, tech, tech] [,, punct, punct] [arXiv:1109.4635, other, other] \n",
      "[[, punct, punct] [14, other, other] [], punct, punct] [Anteneodo, author, author] [C., author, author] [,, punct, punct] [Tsallis, author, author] [C., author, author] [,, punct, punct] [J, journal, author] [., punct, punct] [Math, journal, journal] [., punct, punct] [Phys, journal, journal] [., punct, punct] [,, punct, punct] [44, volume, volume] [(, punct, punct] [2003, date, date] [), punct, punct] [5194, pages, pages] [;, punct, punct] \n",
      "[[, punct, punct] [30, other, other] [], punct, punct] [Siciak, author, author] [J., author, author] [,, punct, punct] [On, title, title] [some, title, title] [extremal, title, title] [functions, title, title] [and, title, title] [their, title, title] [applications, title, title] [in, title, title] [the, title, title] [theory, title, title] [of, title, title] [analytic, title, title] [functions, title, title] [of, title, title] [several, title, title] [complex, title, title] [variables, title, title] [,, punct, punct] [Trans, journal, journal] [., punct, punct] [Amer, journal, journal] [., punct, punct] [Math, journal, journal] [., punct, punct] [Soc, journal, journal] [., punct, punct] [,, punct, punct] [105, volume, volume] [(, punct, punct] [1962, date, date] [), punct, punct] [,, punct, punct] [322-357, pages, pages] [., punct, punct] \n",
      "[[, punct, punct] [7, other, other] [], punct, punct] [S., author, author] [Maslov, author, author] [,, punct, punct] [Y, author, author] [., punct, punct] [-C, author, author] [., punct, punct] [Zhang, author, author] [,, punct, punct] [International, journal, journal] [Journal, journal, journal] [of, journal, journal] [Theoretical, journal, journal] [and, journal, journal] [Applied, journal, journal] [Finance, journal, journal] [1, volume, volume] [,, punct, punct] [1998, date, date] [,, punct, punct] [377, pages, pages] [--, pages, pages] [387, pages, pages] \n",
      "[Vander, author, author] [Linden, author, author] [,, punct, punct] [K., author, author] [;, punct, punct] [Cumming, author, author] [,, punct, punct] [S., author, author] [;, punct, punct] [and, author, author] [Martin, author, author] [,, punct, punct] [J, author, author] [., punct, punct] [1992, date, date] [., punct, punct] [Expressing, title, journal] [local, title, title] [rhetorical, title, title] [relations, title, title] [in, title, title] [instructional, title, title] [text, title, title] [., punct, punct] [Technical, tech, tech] [Report, tech, tech] [92-43, tech, tech] [,, punct, punct] [University, institution, institution] [of, institution, institution] [Colorado, institution, publisher] [., punct, punct] [To, note, institution] [appear, note, other] [in, note, other] [Computational, note, booktitle] [Linguistics, note, other] [., punct, punct] \n",
      "[Lyne, author, author] [A., author, author] [,, punct, punct] [Ritchings, author, author] [R., author, author] [,, punct, punct] [Smith, author, author] [F., author, author] [,, punct, punct] [1975, date, date] [,, punct, punct] [MNRAS, journal, journal] [,, punct, punct] [171, volume, pages] [,, punct, punct] [579, pages, pages] \n",
      "[[, punct, punct] [KK, other, other] [], punct, punct] [S-J, author, author] [., punct, punct] [Kang, author, author] [and, author, author] [M., author, author] [Kashiwara, author, author] [,, punct, punct] [Quantized, title, title] [affine, title, title] [algebras, title, title] [and, title, title] [crystals, title, title] [with, title, title] [core, title, title] [,, punct, punct] [Commun, journal, journal] [., punct, punct] [Math, journal, journal] [., punct, punct] [Phys, journal, journal] [., punct, punct] [195, volume, pages] [(, punct, punct] [1998, date, date] [), punct, punct] [725-740, pages, pages] [., punct, punct] \n",
      "[Reichman, author, author] [,, punct, punct] [R., other, author] [(, punct, punct] [1985, date, date] [), punct, punct] [., punct, punct] [Getting, title, title] [Computers, title, title] [to, title, title] [Talk, title, title] [Like, title, title] [You, title, title] [and, title, title] [Me, title, title] [:, punct, punct] [Discourse, title, title] [Context, title, title] [,, punct, punct] [Focus, title, title] [,, punct, punct] [and, title, title] [Semantics, title, booktitle] [., punct, punct] [Cambridge, location, location] [,, punct, punct] [MA, location, location] [:, punct, punct] [MIT, publisher, publisher] [Press, publisher, publisher] [., punct, punct] \n",
      "[[, punct, punct] [16, other, other] [], punct, punct] [S., author, author] [Browne, author, author] [,, punct, punct] [W., author, author] [Whitt, author, author] [,, punct, punct] [Adv, journal, journal] [., punct, punct] [Appl, journal, journal] [., punct, punct] [Prob, journal, journal] [., punct, punct] [28, volume, volume] [,, punct, punct] [1996, date, date] [,, punct, punct] [1145, pages, pages] [--, pages, pages] [1176, pages, pages] \n",
      "[[, punct, punct] [34, other, other] [], punct, punct] [C., author, author] [J., author, author] [Skinner, author, author] [and, author, author] [M., author, author] [J., author, author] [Elliot, author, author] [,, punct, punct] [A, title, title] [measure, title, title] [of, title, title] [disclosure, title, title] [risk, title, title] [for, title, title] [microdata, title, title] [,, punct, punct] [J., journal, journal] [Roy, journal, journal] [., punct, punct] [Statist, journal, journal] [., punct, punct] [Soc, journal, journal] [., punct, punct] [B, journal, journal] [64, volume, volume] [:, punct, punct] [855, pages, pages] [--, pages, pages] [867, pages, pages] [,, punct, punct] [2002, date, date] [., punct, punct] [MR1979391, other, other] \n",
      "[Rivest, author, author] [,, punct, punct] [R., author, author] [L., other, author] [(, punct, punct] [1987, date, date] [), punct, punct] [., punct, punct] [Learning, title, title] [decision, title, title] [lists, title, title] [., punct, punct] [Machine, booktitle, journal] [Learning, booktitle, author] [,, punct, punct] [2, volume, volume] [(, punct, punct] [3, volume, volume] [), punct, punct] [,, punct, punct] [229-246, pages, pages] [., punct, punct] \n",
      "[Sekine, author, author] [,, punct, punct] [T., author, author] [,, punct, punct] [(, punct, punct] [2001, date, date] [), punct, punct] [., punct, punct] [Modeling, title, title] [and, title, title] [Forecasting, title, title] [Inflation, title, title] [in, title, title] [Japan, title, title] [,, punct, punct] [IMF, tech, other] [Working, tech, tech] [Paper, tech, tech] [,, punct, punct] [WP/01/82, other, other] \n",
      "[Kirkpatrick, author, author] [,, punct, punct] [S., author, author] [,, punct, punct] [Gelatt, author, author] [,, punct, punct] [C., author, author] [,, punct, punct] [&, punct, punct] [Vecchi, author, author] [,, punct, punct] [M., other, other] [(, punct, punct] [1983, date, date] [), punct, punct] [., punct, punct] [Optimization, title, title] [by, title, title] [simulated, title, title] [annealing, title, title] [., punct, punct] [Science, journal, title] [,, punct, punct] [220, volume, volume] [(, punct, punct] [4598, volume, volume] [), punct, punct] [,, punct, punct] [671-680, pages, pages] [., punct, punct] \n",
      "[[, punct, punct] [Vo1, other, other] [], punct, punct] [V., author, author] [E., author, author] [Voskresenskii, author, author] [,, punct, punct] [R-equivalence, title, title] [questions, title, title] [on, title, title] [semisimple, title, title] [groups, title, title] [(, punct, punct] [in, other, title] [Rus-, other, booktitle] [sian, other, journal] [), punct, punct] [,, punct, punct] [Zap, booktitle, title] [., punct, punct] [Nauc, booktitle, journal] [., punct, punct] [Sem, booktitle, journal] [., punct, punct] [Leningrad, publisher, author] [Otdel, publisher, journal] [Mat, publisher, journal] [., punct, punct] [Inst, publisher, journal] [., punct, punct] [Steklov, publisher, journal] [(, punct, punct] [LOMI, publisher, journal] [), punct, punct] [86, volume, pages] [(, punct, punct] [1979, date, date] [), punct, punct] [,, punct, punct] [49, pages, pages] [--, pages, pages] [65, pages, pages] [and, pages, other] [189, pages, pages] [--, pages, pages] [190, pages, date] [., punct, punct] \n",
      "[[, punct, punct] [14, other, other] [], punct, punct] [O'Brien, author, author] [W., author, author] [A., author, author] [,, punct, punct] [Grovit-Ferbas, author, author] [K., author, author] [,, punct, punct] [Namazi, author, author] [A., author, author] [,, punct, punct] [et, author, author] [al, author, author] [., punct, punct] [:, punct, punct] [Human, title, title] [immunodefi-, title, title] [ciency, title, title] [virus, title, title] [type, title, title] [1, title, title] [replication, title, title] [can, title, title] [be, title, title] [increased, title, title] [in, title, title] [peripheral, title, title] [blood, title, title] [of, title, title] [seropositive, title, title] [patients, title, title] [after, title, title] [influenza, title, title] [vaccination, title, title] [., punct, punct] [Blood, journal, journal] [1995, date, journal] [,, punct, punct] [86, volume, volume] [:, punct, punct] [1082, pages, pages] [--, pages, pages] [1089, pages, pages] [., punct, punct] \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from sklearn_hierarchical_classification.classifier import HierarchicalClassifier\n",
    "from sklearn_hierarchical_classification.constants import ROOT\n",
    "from sklearn_hierarchical_classification.metrics import h_fbeta_score, multi_labeled\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix as cm\n",
    "from sklearn.model_selection import KFold\n",
    "from prettytable import PrettyTable\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "\n",
    "X_train, y_train = torch.tensor(X_train), torch.tensor(y_train)\n",
    "X_test, y_test = torch.tensor(X_test), torch.tensor(y_test)\n",
    "\n",
    "\n",
    "def categorical_accuracy(outputs, y, pad_index):\n",
    "    max_outputs = outputs.argmax(dim = 1, keepdim=True)\n",
    "    non_padded_elements = (y != pad_index).nonzero()\n",
    "    correct = max_outputs[non_padded_elements].squeeze(1).eq(y[non_padded_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_padded_elements].shape[0]])\n",
    "\n",
    "def get_max_outputs(outputs):\n",
    "    max_outputs = outputs.argmax(dim = -1)\n",
    "    return max_outputs\n",
    "\n",
    "def print_report(report):\n",
    "    table = PrettyTable(float_format=\"1.5f\")\n",
    "    table.field_names = [\"Tag\", \"Precision\", \"Recall\", \"FBeta\"]\n",
    "    for i in range(len(tag_arr)):\n",
    "      tag, scores = [tag_arr[i]], list(map(lambda metric: metric[i], report))[:-1] # exclude support metric\n",
    "      tag.extend(scores)\n",
    "      table.add_row(tag)\n",
    "    print(table)\n",
    "\n",
    "def print_statistics(X_test, y_test, y_pred, model):\n",
    "    macro_score = f1_score(y_test, y_pred, average='macro')\n",
    "    micro_score = f1_score(y_test, y_pred, average='micro')\n",
    "    cMtx = cm(y_test, y_pred)\n",
    "    normalized_cMtx = []\n",
    "    for row in cMtx:\n",
    "        total = sum(row)\n",
    "        if total != 0:\n",
    "            row = list(map(lambda value: \"{:.5f}\".format(value / total), row))\n",
    "        normalized_cMtx.append(row)\n",
    "    print('Test F1 macro score: {}'.format(macro_score))\n",
    "    print('Test F1 micro score: {}'.format(micro_score))\n",
    "    print(normalized_cMtx)\n",
    "\n",
    "def print_tags(y_pred, limit = 30):\n",
    "    for i, ref_pred in enumerate(y_pred):\n",
    "        if i == limit:\n",
    "            return\n",
    "        ref = ref_test[i]\n",
    "        output = \"\"\n",
    "        for index, token_tag in enumerate(ref):\n",
    "            token, tag = token_tag[0], token_tag[1]\n",
    "            output +=  (\"[\" + \", \".join([token, tag, tag_arr[ref_pred[index]]]) + \"] \")\n",
    "        print(output)\n",
    "\n",
    "\n",
    "# model_filename is used to save the model\n",
    "def train(train_dataset, model_filename=None):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256)\n",
    "    min_loss = 10.0\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            X_train, y_train = data\n",
    "            if i == len(train_loader)-1:\n",
    "                X_train_style = X_style[i*256:]\n",
    "            else:\n",
    "                X_train_style = X_style[i*256:(i+1)*256]\n",
    "            \n",
    "            outputs = model.forward(X_train, X_train_style)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = outputs.view(-1, outputs.shape[-1]) # [batch_size * seq_len, output_dim]\n",
    "            y_train = y_train.view(-1) # [batch_size * seq_len]\n",
    "            \n",
    "            # Get the loss function\n",
    "            loss = criterion(outputs, y_train.long())\n",
    "\n",
    "            # Calculate loss\n",
    "            loss.backward()\n",
    "            total_train_loss += loss.item()\n",
    "            # Backpropagation\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print loss at every 100th epoch\n",
    "            if epoch % 100 == 0:\n",
    "                print(\"Epoch: %d, loss after minibatch %5d: %1.5f\" % (epoch, i+1, loss.item()))\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            report = precision_recall_fscore_support(y_train.long(), \\\n",
    "                                                    get_max_outputs(outputs.detach()), \\\n",
    "                                                    average=None, \\\n",
    "                                                    zero_division=0, \\\n",
    "                                                    labels = [i for i in range(len(all_tags))])\n",
    "            print_report(report)\n",
    "        if total_train_loss < min_loss:\n",
    "            print(total_train_loss, min_loss, 'current epoch {}: saving best model...'.format(epoch))\n",
    "            min_loss = total_train_loss\n",
    "            torch.save(model.state_dict(), './models/checkpoint.pt')\n",
    "\n",
    "        \n",
    "    model.load_state_dict(torch.load('./models/checkpoint.pt'))\n",
    "\n",
    "def test(model, X_test, y_test):\n",
    "    with torch.no_grad():\n",
    "        print()\n",
    "        outputs = model.forward(X_test, X_test_style)\n",
    "\n",
    "        outputs_squeezed = outputs.view(-1, outputs.shape[-1]) # [batch_size * seq_len, output_dim]\n",
    "        y_test_squeezed = y_test.view(-1) # [batch_size * seq_len]\n",
    "\n",
    "        # Get the loss function\n",
    "        loss = criterion(outputs_squeezed, y_test_squeezed.long())\n",
    "        \n",
    "        int_y_test = y_test.int().tolist()\n",
    "        output_probs = outputs\n",
    "        y_true, y_pred = [], []\n",
    "        for idx, row in enumerate(int_y_test):\n",
    "            padding_idx = row.index(len(all_tags))\n",
    "            y_true.extend(row[:padding_idx])\n",
    "            test_row = output_probs[idx][:padding_idx]\n",
    "            y_pred.extend(get_max_outputs(test_row))\n",
    "\n",
    "        report = precision_recall_fscore_support(y_true, \\\n",
    "                                                y_pred, \\\n",
    "                                                average=None, \\\n",
    "                                                zero_division=0, \\\n",
    "                                                labels = [i for i in range(len(all_tags))])\n",
    "        print_report(report)  \n",
    "        print_statistics(X_test, y_true, y_pred, model)\n",
    "        print_tags(get_max_outputs(outputs.detach()))\n",
    "training_set = TensorDataset(X_train, y_train)\n",
    "train(training_set)\n",
    "model.load_state_dict(torch.load('./models/checkpoint.pt'))\n",
    "model_filename = \"./models/swe_we_cstyle_ner_4gramwords.pt\" # change accordingly based on features being run\n",
    "\n",
    "torch.save(model.state_dict(), str(model_filename))\n",
    "test(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "6e61ca267e818e5d9a430b77a02997073c0a797ef0ae9dc5b9fbd2a4d1cc9a76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
